id,url,body,title,description
7,https://cloud.google.com/tpu/docs/reference/rest/v1/projects.locations/list,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Query parameters](#body.QUERY_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [Try it!](#try-it)
Lists information about the supported locations for this service.
HTTP request
GET https://tpu.googleapis.com/v1/{name=projects/*}/locations
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
The resource that owns the locations collection, if applicable.
Query parameters
|Parameters
|
filter
|
A filter to narrow down results to a preferred subset. The filtering language accepts strings like
|
pageSize
|
The maximum number of results to return. If not set, the service selects a default.
|
pageToken
|
A page token received from the
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[ListLocationsResponse](/tpu/docs/reference/rest/Shared.Types/ListLocationsResponse)
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).",Method: projects.locations.list | Cloud TPU | Google Cloud,
id,url,body,title,description
1,https://cloud.google.com/tpu/docs/maintenance-events,"Manage maintenance events with Cloud TPU Pods
Overview
TPU Nodes and TPU VMs are instances of Compute Engine VMs with
attached TPU hardware. Compute Engine VMs are subject to
[Compute Engine VM maintenance events](/compute/docs/instances/setting-instance-scheduling-options#maintenanceevents).
Each TPU is connected to a Compute Engine VM, so using more TPUs (for example,
in a TPU Pod) increases the likelihood of one of your VMs encountering a
maintenance event.
This document discusses various approaches to handle maintenance events for long-running training jobs on Cloud TPUs.
Using checkpoints for fast recovery from maintenance events
Checkpoints are key to short recoveries from maintenance events and should be saved frequently: a good rule of thumb is saving checkpoints approximately every hour. Not checkpointing often enough risks losing a lot of training progress due to maintenance events or other training interruptions.
Checkpoints generally refer to all of the saved parameters used in training (such as model weights). The time it takes to save a checkpoint can range from the order of seconds to the order of minutes.
Although most maintenance events are automatically recovered and
training jobs continue without manual intervention, there might be
edge cases where the job does not restart and automatically continue.
When this happens, you need to delete and recreate the TPU resources,
and restart the training job from a saved checkpoint.
For information about how to detect and recover from automatic
recovery failures, see
[Detect and recover from TPU failures](#detect-failures).
The mechanisms used to save and load checkpoints are different for each ML
framework. Supported Cloud TPU models generally have checkpointing built-in.
For more information on checkpointing, see :
[TensorFlow 2.x](https://www.tensorflow.org/guide/checkpoint), [PyTorch](https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html), or [JAX/flax](https://flax.readthedocs.io/en/latest/flax.training.html).
Detecting Maintenance Events
You can detect if and when a maintenance event occurs on your TPU using
the following
[ command: gcloud describe](/sdk/gcloud/reference/alpha/compute/tpus/tpu-vm/describe)
TPU VMs
$ gcloud compute tpus tpu-vm describe tpu-name --zone=zone | grep 'health'
TPU Nodes
$ gcloud compute tpus describe tpu-name --zone=zone | grep 'health'
The output from this command displays the current state of the TPU and a description of the most recent maintenance event. The output should look similar to the following:
health: HEALTHY healthDescription: The TPU had a maintenance event at 2022-01-26T03:44:36.265703305Z
Maintenance event logs
You can view historical logs of maintenance events on your TPU in
[system event audit logs](/tpu/docs/audit-logs#audited_operations).
In the Google Cloud console navigation menu, click Compute Engine > VM instances and search, for example:
""tpu.nodes.terminate"" OR ""tpu.nodes.restart""
Within your search timeframe, any interruptions and repairs of your TPU workers
are displayed. The logs will show the date and time of the event, the type of
event, and for ""terminate"" events, the reason for the termination in
protoPayload.metadata.terminateReason.
Handle maintenance events
There are several ways you can mitigate maintenance event disruptions.
Periodically save checkpoints
In the ideal scenario, when an ""interruption event"" happens, training resumes from the latest checkpoint.
Training script retries
The training script might stop as a result of an ""interruption event"". You can use a
bashscript to continuously retry the training script until training is complete. Each retry should continue from the latest checkpoint, so retry scripts should always be used in conjunction with checkpoints.
Production-ready training pipelines should use a resource management system such as Google Kubernetes Engine (GKE). For more information on using Google Kubernetes Engine with the TPU VM architecture, see
[Deploy TPU workloads](/kubernetes-engine/docs/how-to/tpus). For more information about using Google Kubernetes Engine with the TPU Node architecture, see [Run TPU applications on Google Kubernetes Engine](/tpu/docs/kubernetes-engine-setup). Otherwise, you can implement a
bashscript to continuously retry the training script until completion. For example:
With TPU Node:
(From your VM)
bash while ! python3 [training command]; do sleep 1; done
With TPU VM:
while ! gcloud compute tpus tpu-vm ssh ${TPU_NAME} --command ""python3 [training command]""; do sleep 1; done
(Note that you need to run the TPU VM command from a Cloud Shell or from a terminal, not from the TPU VM).
Detect and recover from TPU failures
When a TPU does not recover from a maintenance event, you can use a recovery script to detect the TPU state and delete and re-create the TPU. An example of this script can be found
[here.](https://github.com/tensorflow/tpu/blob/master/tools/retry/retry.sh)See [Managing TPUs](/tpu/docs/creating-deleting-tpus)for details on manually deleting and re-creating TPUs.
When creating or re-creating a TPU VM, you can specify a startup script with the
--metadata startup-scriptparameter. A startup script runs whenever a TPU VM is created. Refer to
[Run standard installation scripts](/tpu/docs/managing-tpus-tpu-vm#startup-scripts)for more information.",Manage maintenance events with Cloud TPU Pods | Google Cloud,
id,url,body,title,description
6,https://cloud.google.com/tpu/docs/v5e-inference,"Cloud TPU v5e Inference introduction
Overview and Benefits
Cloud TPU v5e is Google Cloud's latest generation AI accelerator. With a smaller 256-chip footprint per Pod, v5e Pods are optimized for transformer-based, text-to-image and CNN-based training, fine-tuning, and serving.
Concepts
If you are new to Cloud TPUs, check out the
[TPU documentation home](/tpu/docs/tpus).
Chips
There are 256 chips in a single v5e with 8 chips per host.
See
[System architecture](/tpu/docs/system-architecture-tpu-vm) for more details.
Cores
TPU chips have one or two TensorCores to run matrix multiplication.
Similar to v2 and v3 Pods, v5e has one TensorCore per chip. By contrast,
v4 Pods have 2 TensorCores per chip. See
[System architecture](/tpu/docs/system-architecture-tpu-vm)
for more details on v5e TensorCores. Additional information about TensorCores
can be found in this [ACM article](https://dl.acm.org/doi/pdf/10.1145/3360307).
Host
A host is a physical computer (CPU) that runs VMs. A host can run multiple VMs at once.
Batch Inference
Batch or offline inference refers to doing inference outside of production pipelines typically on a bulk of inputs. Batch inference is used for offline tasks such as data labeling and also for evaluating the trained model. Latency SLOs are not a priority for batch inference.
Serving
Serving refers to the process of deploying a trained machine learning model to a production environment, where it can be used to make predictions or decisions. Latency SLOs are a priority for serving.
Single Host versus Multi Host
Slices using fewer than 8 chips use at most 1 host. Slices greater than 8 chips, have access to more than a single host and can run distributed training using multiple hosts.
Queued Resource
A representation of TPU resources, used to enqueue and manage a
request for a single-slice or multi-slice TPU environment. See the
[Queued Resources user guide](/tpu/docs/queued-resources) for more information.
TPU VM
A virtual machine running Linux that has access to the underlying TPU's. For v5e TPUs, each TPU VM has direct access to 1, 4, or 8 chips depending on the user-specified accelerator type. A TPU VM is also known as a worker.
Get started
Securing capacity
Contact
[Cloud Sales](https://cloud.google.com/contact)
to start using Cloud TPU v5e for your AI workloads.
Prepare a Google Cloud Project
[Sign in](https://accounts.google.com/Login)to your Google Account. If you haven't already, [sign up for a new account](https://accounts.google.com/SignUp).
In the
[Cloud Console](https://console.cloud.google.com/), [select](/resource-manager/docs/creating-managing-projects#get_an_existing_project)or [create](/resource-manager/docs/creating-managing-projects#creating_a_project)a Cloud project from the project selector page. [Billing setup](/billing/docs)is required for all Google Cloud usage so make sure billing is enabled for your project.
Install
[gcloud alpha components](https://cloud.google.com/sdk/gcloud/reference/components/install).
Enable the TPU API using the following
gcloudcommand in Cloud Shell. (You may also enable it
[from the Google Cloud Console](https://console.cloud.google.com/apis/library/tpu.googleapis.com).)
gcloud services enable tpu.googleapis.com
Enable the TPU service account.
Service accounts allow the Cloud TPU service to access other Google Cloud services. A user-managed service account is a
[recommended](/run/docs/securing/service-identity)Google Cloud practice. Follow these guides to [create](/iam/docs/service-accounts-create)and [grant](/iam/docs/granting-changing-revoking-access)the following roles to your service account. The following roles are necessary:
- TPU Admin
- Storage Admin
- Logs Writer
- Monitoring Metric Writer
Configure the project and zone.
Your project ID is the name of your project
[shown on the Cloud console](/resource-manager/docs/creating-managing-projects#identifying_projects). The default zone for Cloud TPU v5e is
us-west4-a.
export PROJECT_ID=project-ID export ZONE=us-west4-a gcloud alpha compute tpus tpu-vm service-identity create --zone=${ZONE} gcloud auth login gcloud config set project ${PROJECT} gcloud config set compute/zone ${ZONE}
Provision the Cloud TPU v5e environment.
A v5e is managed as a
[Queued Resource](/tpu/docs/queued-resources). Capacity can be provisioned using the
queued-resource createcommand.
Create environment variables for project ID, accelerator type, zone, runtime version, and TPU name.
export PROJECT_ID=project_ID export ACCELERATOR_TYPE=v5litepod-1 export ZONE=us-west4-a export RUNTIME_VERSION=v2-alpha-tpuv5-lite export SERVICE_ACCOUNT=service_account export TPU_NAME=tpu-name export QUEUED_RESOURCE_ID=queued_resource_id
Variable descriptions
- Project Name. Use your Google project name.
- See the
[Accelerator Types](/tpu/docs/supported-tpu-configurations#tpu-v5e-config)section for supported accelerator types.
- All inference capacity is in us-west4-a.
- v2-alpha-tpuv5-lite
- This is the email address of your service account that you can find in Google Cloud Console -> IAM -> Service Accounts For example: tpu-service-account@myprojectID.iam.gserviceaccount.com.
- The user-assigned ID of the TPU which is created when the queued resource request is allocated.
- The user-assigned ID of the queued resource request.
PROJECT_ID
ACCELERATOR_TYPE
ZONE
RUNTIME_VERSION
SERVICE_ACCOUNT
TPU_NAME
QUEUED_RESOURCE_ID
Create a TPU resource.
gcloud alpha compute tpus queued-resources create ${QUEUED_RESOURCE_ID} \ --node-id ${TPU_NAME} \ --project ${PROJECT_ID} \ --zone ${ZONE} \ --accelerator-type ${ACCELERATOR_TYPE} \ --runtime-version ${RUNTIME_VERSION} \ --service-account ${SERVICE_ACCOUNT} \ --${QUOTA_TYPE}
If you would like to delete the resource you have reserved, you need to delete the resource TPU_NAME first and then also delete the queued resource request.
gcloud alpha compute tpus delete $TPU_NAME --zone ${ZONE} --project ${PROJECT_ID} gcloud alpha compute tpus queued-resources delete ${QUEUED_RESOURCE_ID} \ --project ${PROJECT_ID} \ --zone ${ZONE}
Connect to your TPU vM using SSH
To run code on your TPU VMs, you need to
SSHinto each TPU VM. In this example, with a v5litepod-1, there is only one TPU VM.
gcloud compute config-ssh gcloud compute tpus tpu-vm ssh $TPU_NAME --zone $ZONE --project $PROJECT_ID
Manage your TPU VMs
For all TPU management options for your TPU VMs, see
[Managing TPUs](/tpu/docs/managing-tpus-tpu-vm).
Develop and Run
This section describes the general setup process for custom model inference using JAX or PyTorch on Cloud TPU v5e. TensorFlow support will be enabled soon.
For v5e training instructions, refer to the
[v5e training guide](/tpu/docs/v5e-training).
Running Inference on v5e
Inference Software Stack
Details of the Inference SW stack are covered in the following sections. This document focuses on single host serving for models trained with JAX, TensorFlow (TF), and PyTorch.
This section assumes that you have already set up your Google Cloud
project according to the instructions in
[Prepare a Google Cloud project](#prepare-a-project).
JAX Model Inference and Serving
The following section walks through the workflow for JAX model Inference.
There are two paths for JAX inference as shown in the diagram.
This section will cover the production path for JAX models through
jax2tf
and Cloud TPU Serving.
- Use
jax2tfto convert the model to Cloud TPU 2 and save the model
- Use the Inference Converter to convert the saved model
- Use Cloud TPU Serving to serve the model
Use
jax2tf to convert the model and save it
Refer to
[JAX and Cloud TPU interoperation](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md)
to convert and save your JAX model to Cloud TPU.
# Inference function def model_jax(params, inputs): return params[0] + params[1] * inputs # Wrap the parameter constants as tf.Variables; this will signal to the model # saving code to save those constants as variables, separate from the # computation graph. params_vars = tf.nest.map_structure(tf.Variable, params) # Build the prediction function by closing over the `params_vars`. If you # instead were to close over `params` your SavedModel would have no variables # and the parameters will be included in the function graph. prediction_tf = lambda inputs: jax2tf.convert(model_jax)(params_vars, inputs) my_model = tf.Module() # Tell the model saver what the variables are. my_model._variables = tf.nest.flatten(params_vars) my_model.f = tf.function(prediction_tf, jit_compile=True, autograph=False) tf.saved_model.save(my_model)
Use the Inference Converter to convert the saved model
The steps for Inference Converter are described in the
[Inference converter guide.](/tpu/docs/v5e-inference-converter)
Use Cloud TPU Serving
The steps for Cloud TPU Serving are described in
[Cloud TPU serving](#tensorflow-serving).
E2E JAX Model Serving Example
Prerequisite
You need to set up your Docker credentials and pull the Inference Converter and Cloud TPU Serving Docker image. If you have not already done so, run the following commands:
sudo usermod -a -G docker ${USER} newgrp docker gcloud auth configure-docker \ us-docker.pkg.dev docker pull us-docker.pkg.dev/cloud-tpu-images/inference/tpu-inference-converter-cli:2.13.0 docker pull us-docker.pkg.dev/cloud-tpu-images/inference/tf-serving-tpu:2.13.0
Download the Demo code: SSH to your TPU VM and install the inference Demo code.
gsutil -m cp -r \ ""gs://cloud-tpu-inference-public/demo"" \ .
Install the JAX demo dependencies
On your TPU VM, install
requirements.txt.
pip install -r ./demo/jax/requirements.txt
Run JAX BERT E2E Serving demo
The
[pretrained BERT model](https://huggingface.co/bert-base-uncased) is
from Hugging Face.
Export a TPU-compatible TF2 saved model from a Flax BERT model:
cd demo/jax/bert
python3 export_bert_model.py
Launch the Cloud TPU model server container for the model:
docker run -t --rm --privileged -d \ -p 8500:8500 -p 8501:8501 \ --mount type=bind,source=/tmp/jax/bert_tpu,target=/models/bert \ -e MODEL_NAME=bert \ us-docker.pkg.dev/cloud-tpu-images/inference/tf-serving-tpu:2.13.0
Check the model server container log and make sure the gRPC and Http Server is up:
CONTAINER_ID=$(docker ps | grep ""tf-serving-tpu"" | awk '{print $1}') docker logs ${CONTAINER_ID}
If you see the log ending with the following information, it means the server is ready to serve requests. It takes around 30 seconds.
2023-04-08 00:43:10.481682: I tensorflow_serving/model_servers/server.cc:409] Running gRPC ModelServer at 0.0.0.0:8500 ... [warn] getaddrinfo: address family for nodename not supported 2023-04-08 00:43:10.520578: I tensorflow_serving/model_servers/server.cc:430] Exporting HTTP/REST API at:localhost:8501 ... [evhttp_server.cc : 245] NET_LOG: Entering the event loop ...
Send the request to the model server.
python3 bert_request.py
The output will be similar to the following:
For input ""The capital of France is [MASK]."", the result is "". the capital of france is paris.."" For input ""Hello my name [MASK] Jhon, how can I [MASK] you?"", the result is "". hello my name is jhon, how can i help you?.""
Clean up.
Make sure to clean up the Docker container before running other demos.
CONTAINER_ID=$(docker ps | grep ""tf-serving-tpu"" | awk '{print $1}') docker stop ${CONTAINER_ID}
Clean up the model artifacts:
sudo rm -rf /tmp/jax/
Run JAX Stable Diffusion E2E Serving demo
The
[pretrained Stable Diffusion model](https://huggingface.co/CompVis/stable-diffusion-v1-4/tree/bf16) is from Hugging Face.
Export TPU-compatible TF2 saved model from Flax Stable Diffusion model:
cd demo/jax/stable_diffusion
python3 export_stable_diffusion_model.py
Launch the Cloud TPU model server container for the model:
docker run -t --rm --privileged -d \ -p 8500:8500 -p 8501:8501 \ --mount type=bind,source=/tmp/jax/stable_diffusion_tpu,target=/models/stable_diffusion \ -e MODEL_NAME=stable_diffusion \ us-docker.pkg.dev/cloud-tpu-images/inference/tf-serving-tpu:2.13.0
Check the model server container log and make sure the gRPC and Http Server is up:
CONTAINER_ID=$(docker ps | grep ""tf-serving-tpu"" | awk '{print $1}') docker logs ${CONTAINER_ID}
If you see the log ending with the following information, it means the server is ready to serve requests. It takes around 2 minutes.
2023-04-08 00:43:10.481682: I tensorflow_serving/model_servers/server.cc:409] Running gRPC ModelServer at 0.0.0.0:8500 ... [warn] getaddrinfo: address family for nodename not supported 2023-04-08 00:43:10.520578: I tensorflow_serving/model_servers/server.cc:430] Exporting HTTP/REST API at:localhost:8501 ... [evhttp_server.cc : 245] NET_LOG: Entering the event loop ...
Send the request to the model server.
python3 stable_diffusion_request.py
The prompt is ""Painting of a squirrel skating in New York"" and the output image will be saved as
stable_diffusion_images.jpgin your current directory.
Clean up.
Make sure to clean up the Docker container before running other demos.
CONTAINER_ID=$(docker ps | grep ""tf-serving-tpu"" | awk '{print $1}') docker stop ${CONTAINER_ID}
Clean up the model artifacts
sudo rm -rf /tmp/jax/
Cloud TPU Model Inference and Serving
The following sections walk through the workflow for Cloud TPU Model Inference.
- Use the Inference Converter to convert the model
- Use Cloud TPU Serving to serve the model
Inference Converter
Cloud TPU Inference Converter prepares and optimizes a model exported from
TensorFlow or JAX for TPU inference. The converter runs in a local shell
or in the TPU VM shell. The TPU VM shell is recommended because it comes
preinstalled with the command line tools needed for the converter.
For more details on the Inference Converter refer to the
[Inference Converter User Guide](/tpu/docs/v5e-inference-converter).
Prerequisites
The model must be exported from TensorFlow or JAX in the
[SavedModel](https://www.tensorflow.org/guide/saved_model)format.
The model must have a function alias for the TPU function. See the code examples in
[Inference Converter User Guide](/tpu/docs/v5e-inference-converter)for instructions on how to do this. The following examples use
tpu_funcas the TPU function alias.
Make sure your machine CPU supports Advanced Vector eXtensions (AVX) instructions, as the TensorFlow library (the dependency of the Cloud TPU Inference Converter) is compiled to use AVX instructions.
[Most CPUs](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions#CPUs_with_AVX)have the AVX support.
- You can run
lscpu | grep avxto check whether the AVX instruction set is supported.
- You can run
Getting Started
Set up TPU VM Environment Set up the environment using the following steps depending on the shell you are using:
TPU VM Shell
- In your TPU VM shell, run the following commands to allow non-root docker usage:
sudo usermod -a -G docker ${USER} newgrp docker
- Initialize your Docker Credential helpers:
gcloud auth configure-docker \ us-docker.pkg.dev
Local Shell
In your local shell, set up the environment using the following steps:
Install the
[Cloud SDK](https://cloud.google.com/sdk/install), which includes the
gcloudcommand-line tool.
Install
[Docker](https://docs.docker.com/engine/install/):
Allow non-root Docker usage:
sudo usermod -a -G docker ${USER} newgrp docker
Login in to your environment:
gcloud auth login
Initialize your Docker Credential helpers:
gcloud auth configure-docker \ us-docker.pkg.dev
Pull the Inference Converter Docker image:
CONVERTER_IMAGE=us-docker.pkg.dev/cloud-tpu-images/inference/tpu-inference-converter-cli:2.13.0 docker pull ${CONVERTER_IMAGE}
Converter Image
The Image is for doing one-time model conversions. Set the model
paths and adjust the
[converter options](/tpu/docs/v5e-inference-converter#converter-options)
to fit your needs. The Usage Examples section in the
[Inference Converter User Guide](/tpu/docs/v5e-inference-converter)
provides several common use cases.
docker run \ --mount type=bind,source=${MODEL_PATH},target=/tmp/input,readonly \ --mount type=bind,source=${CONVERTED_MODEL_PATH},target=/tmp/output \ ${CONVERTER_IMAGE} \ --input_model_dir=/tmp/input \ --output_model_dir=/tmp/output \ --converter_options_string=' tpu_functions { function_alias: ""tpu_func"" } batch_options { num_batch_threads: 2 max_batch_size: 8 batch_timeout_micros: 5000 allowed_batch_sizes: 2 allowed_batch_sizes: 4 allowed_batch_sizes: 8 max_enqueued_batches: 10 } '
The following section shows how to run this model with a TensorFlow model Server.
TensorFlow Serving
The following instructions demonstrate how you can serve your TensorFlow model on TPU VMs.
Prerequisites:
Set up your
[Docker credentials](#getting-started-serving), if you have
not already done so:
Download the TensorFlow Serving Docker Image for your TPU VM.
Set sample environment variables
export YOUR_LOCAL_MODEL_PATH=model-path export MODEL_NAME=model-name # Note: this image name may change later. export IMAGE_NAME=us-docker.pkg.dev/cloud-tpu-images/inference/tf-serving-tpu:2.13.0
Download the Docker image
docker pull ${IMAGE_NAME}
Serve your TensorFlow model using the TensorFlow Serving Docker Image on your TPU VM.
# PORT 8500 is for gRPC model server and 8501 is for HTTP/REST model server. docker run -t --rm --privileged -d \ -p 8500:8500 -p 8501:8501 \ --mount type=bind,source=${YOUR_LOCAL_MODEL_PATH},target=/models/${MODEL_NAME} \ -e MODEL_NAME=${MODEL_NAME} \ ${IMAGE_NAME}
Follow the Serving Client API to query your model.
E2E TensorFlow Model Serving Example:
Prerequisite: Make sure you already set up the Docker credentials and pulled the Inference Converter and TensorFlow Serving Docker image. If not, run the following commands:
sudo usermod -a -G docker ${USER} newgrp docker gcloud auth configure-docker \ us-docker.pkg.dev docker pull us-docker.pkg.dev/cloud-tpu-images/inference/tpu-inference-converter-cli:2.13.0 docker pull us-docker.pkg.dev/cloud-tpu-images/inference/tf-serving-tpu:2.13.0
Download the Demo code:
gsutil -m cp -r \ ""gs://cloud-tpu-inference-public/demo"" \ .
Install the TensorFlow demo dependencies:
pip install -r ./demo/tf/requirements.txt
Run TensorFlow ResNet-50 E2E Serving demo
Export a TPU-compatible TF2 saved model from the Keras ResNet-50 model.
cd demo/tf/resnet-50
python3 export_resnet_model.py
Launch the TensorFlow model server container for the model.
docker run -t --rm --privileged -d \ -p 8500:8500 -p 8501:8501 \ --mount type=bind,source=/tmp/tf/resnet_tpu,target=/models/resnet \ -e MODEL_NAME=resnet \ us-docker.pkg.dev/cloud-tpu-images/inference/tf-serving-tpu:2.13.0
Check the model server container log and make sure the gRPC and Http Server is up:
CONTAINER_ID=$(docker ps | grep ""tf-serving-tpu"" | awk '{print $1}') docker logs ${CONTAINER_ID}
If you see the log ending with the following information, it means the server is ready to serve requests. It takes around 30 seconds.
2023-04-08 00:43:10.481682: I tensorflow_serving/model_servers/server.cc:409] Running gRPC ModelServer at 0.0.0.0:8500 ... [warn] getaddrinfo: address family for nodename not supported 2023-04-08 00:43:10.520578: I tensorflow_serving/model_servers/server.cc:430] Exporting HTTP/REST API at:localhost:8501 ... [evhttp_server.cc : 245] NET_LOG: Entering the event loop ...
Send the request to the model server.
The request image is a banana from https://i.imgur.com/j9xCCzn.jpeg .
python3 resnet_request.py
The output will be similar to the following:
Predict result: [[('n07753592', 'banana', 0.94921875), ('n03532672', 'hook', 0.022338867), ('n07749582', 'lemon', 0.005126953)]]
Clean up.
Make sure to clean up the Docker container before running other demos.
CONTAINER_ID=$(docker ps | grep ""tf-serving-tpu"" | awk '{print $1}') docker stop ${CONTAINER_ID}
Clean up the model artifacts:
sudo rm -rf /tmp/tf/
PyTorch Model Inference and Serving
The following sections walk through the workflow for PyTorch Model Inference:
- Write a Python model handler for loading and inferencing using TorchDynamo and PyTorch/XLA
- Use TorchModelArchiver to create a model archive
- Use TorchServe to serve the model
TorchDynamo and PyTorch/XLA
[TorchDynamo](https://github.com/pytorch/torchdynamo) (Dynamo) is a
Python-level JIT compiler designed to make unmodified PyTorch programs
faster. It provides a clean API for compiler backends to hook into.
Its biggest feature is to dynamically modify Python bytecode just before
execution. In the PyTorch/XLA 2.0 release, an experimental backend for
Dynamo is provided for both inference and training.
Dynamo provides a
[Torch FX](https://pytorch.org/docs/stable/fx.html) (FX)
graph when it recognizes a model pattern and PyTorch/XLA uses a Lazy Tensor
approach to compile the FX graph and return the compiled function.
To get more insight regarding the technical details about PyTorch/XLA's
dynamo implementation, see the
[Pytorch Dev Discussions post](https://dev-discuss.pytorch.org/t/torchdynamo-update-10-integrating-with-pytorch-xla-for-inference-and-training/935)
dev-discuss post and the [TorchDynamo documentation](https://github.com/pytorch/xla/blob/r2.0/docs/dynamo.md).
See this [blog](https://pytorch.org/blog/pytorch-2.0-xla/) for more details.
Here is a small code example of running densenet161 inference with
torch.compile.
import torch import torchvision import torch_xla.core.xla_model as xm def eval_model(loader): device = xm.xla_device() xla_densenet161 = torchvision.models.densenet161().to(device) xla_densenet161.eval() dynamo_densenet161 = torch.compile( xla_densenet161, backend='torchxla_trace_once') for data, _ in loader: output = dynamo_densenet161(data)
TorchServe
The Cloud TPU TorchServe Docker Image lets you to serve the PyTorch eager mode model using TorchServe on a TPU VM.
You can use the provided
torchserve-tpu Docker image that is ready for
serving your archived pytorch model on a TPU VM.
Set up authentication for Docker:
sudo usermod -a -G docker ${USER} newgrp docker gcloud auth configure-docker \ us-docker.pkg.dev
Pull the Cloud TPU TorchServe Docker image to your TPU VM:
CLOUD_TPU_TORCHSERVE_IMAGE_URL=us-docker.pkg.dev/cloud-tpu-images/inference/torchserve-tpu:v0.9.0-2.1 docker pull ${CLOUD_TPU_TORCHSERVE_IMAGE_URL}
Collect Model Artifacts
To get started, you need to provide a model handler, which instructs the
TorchServe model server worker to load your model, process the input data
and run inference. You can use the
[TorchServe default inference handlers](https://pytorch.org/serve/default_handlers.html)
( [source](https://github.com/pytorch/serve/tree/master/ts/torch_handler)), or develop your own custom model handler
following the [base_handler.py](https://github.com/pytorch/serve/blob/master/ts/torch_handler/base_handler.py).
You may also need to provide the trained model, and the model definition file.
In the following Densenet 161 example, we use model artifacts and the default image classifier handler provided by TorchServe:
The work directory is shown below.
CWD=""$(pwd)"" WORKDIR=""${CWD}/densenet_161"" mkdir -p ${WORKDIR}/model-store mkdir -p ${WORKDIR}/logs
Download and copy model artifacts from the TorchServe image classifier example:
git clone https://github.com/pytorch/serve.git cp ${CWD}/serve/examples/image_classifier/densenet_161/model.py ${WORKDIR} cp ${CWD}/serve/examples/image_classifier/index_to_name.json ${WORKDIR}
Download the model weights:
wget https://download.pytorch.org/models/densenet161-8d451a50.pth -O densenet161-8d451a50.pth mv densenet161-8d451a50.pth ${WORKDIR}
Create a TorchServe model config file to use the Dynamo backend:
echo 'pt2: ""torchxla_trace_once""' >> ${WORKDIR}/model_config.yaml
You should see the files and directories shown below:
>> ls ${WORKDIR} model_config.yaml index_to_name.json logs model.py densenet161-8d451a50.pth model-store
Generate a model archive file
To serve your PyTorch model with Cloud TPU TorchServe, you need to
package your model handler and all your model artifacts into a model
archive file
(*.mar) using
[Torch Model Archiver](https://github.com/pytorch/serve/blob/master/model-archiver/README.md).
Generate a model archive file with torch-model-archiver:
MODEL_NAME=Densenet161 docker run \ --privileged \ --shm-size 16G \ --name torch-model-archiver \ -it \ -d \ --rm \ --mount type=bind,source=${WORKDIR},target=/home/model-server/ \ ${CLOUD_TPU_TORCHSERVE_IMAGE_URL} \ torch-model-archiver \ --model-name ${MODEL_NAME} \ --version 1.0 \ --model-file model.py \ --serialized-file densenet161-8d451a50.pth \ --handler image_classifier \ --export-path model-store \ --extra-files index_to_name.json \ --config-file model_config.yaml
You should see the model archive file generated in the model-store directory:
>> ls ${WORKDIR}/model-store Densenet161.mar
Serve inference requests
Now you have the model archive file, you can start the TorchServe model server and serve inference requests.
Start the TorchServe model server:
docker run \ --privileged \ --shm-size 16G \ --name torchserve-tpu \ -it \ -d \ --rm \ -p 7070:7070 \ -p 7071:7071 \ -p 8080:8080 \ -p 8081:8081 \ -p 8082:8082 \ -p 9001:9001 \ -p 9012:9012 \ --mount type=bind,source=${WORKDIR}/model-store,target=/home/model-server/model-store \ --mount type=bind,source=${WORKDIR}/logs,target=/home/model-server/logs \ ${CLOUD_TPU_TORCHSERVE_IMAGE_URL} \ torchserve \ --start \ --ncs \ --models ${MODEL_NAME}.mar \ --ts-config /home/model-server/config.properties
Query model server health:
curl http://localhost:8080/ping
If the model server is up and running, you will see:
{ ""status"": ""Healthy"" }
To query the default versions of the current registered model use:
curl http://localhost:8081/models
You should see the registered model:
{ ""models"": [ { ""modelName"": ""Densenet161"", ""modelUrl"": ""Densenet161.mar"" } ] }
To download an image for inference use:
curl -O https://raw.githubusercontent.com/pytorch/serve/master/docs/images/kitten_small.jpg mv kitten_small.jpg ${WORKDIR}
To send an inference request to the model server use:
curl http://localhost:8080/predictions/${MODEL_NAME} -T ${WORKDIR}/kitten_small.jpg
You should see a response similar to the following:
{ ""tabby"": 0.47878125309944153, ""lynx"": 0.20393909513950348, ""tiger_cat"": 0.16572578251361847, ""tiger"": 0.061157409101724625, ""Egyptian_cat"": 0.04997897148132324 }
Model server logs
Use the following commands to access the logs:
ls ${WORKDIR}/logs/ cat ${WORKDIR}/logs/model_log.log
You should see the following message in your log:
""Compiled model with backend torchxla\_trace\_once""
Clean Up
Stop the Docker container:
rm -rf serve rm -rf ${WORKDIR} docker stop torch-model-archiver docker stop torchserve-tpu
Large Language Model Serving
[SAX](https://github.com/google/saxml) is a serving framework to support serving
large models that may require TPUs on multiple hosts to run with
[GSPMD](https://arxiv.org/abs/2105.04663), such as PAX-based Large Language
Models. [PAX](https://github.com/google/paxml) is a framework on top of
JAX, and is for training large-scale models that allows for advanced and fully
configurable experimentation and parallelization.
The SAX cluster section describes the key elements to understand how SAX
works. The SAX model serving section walks through
[a single-host model serving example with a GPTJ6B model](#single-host-example).
SAX also provides multi-host serving on Cloud TPUs and users can run models on
larger TPU topologies for experimental multi-host serving.
[The following example with a 175B test model](#multi-host-preview) shows how to experiment with this setup.
SAX cluster (SAX cell)
SAX admin server and SAX model server are two essential components that run a SAX cluster.
SAX admin server
The SAX admin server monitors and coordinates all SAX model servers in a SAX cluster. In a SAX cluster, you can launch multiple SAX admin servers, where only one of the SAX admin server is active through leader election, the others are standby servers. When the active admin server fails, a standby admin server will become active. The active SAX admin server assigns model replicas and inference requests to available SAX model servers.
SAX admin storage bucket
Each SAX cluster requires a Cloud Storage bucket to store the configurations and locations of SAX admin servers and SAX model servers in the SAX cluster.
SAX model server
The SAX model server loads a model checkpoint and runs inference with
[GSPMD](https://arxiv.org/abs/2105.04663). A SAX model server runs on a single
TPU VM worker. Single-host TPU model serving requires a single SAX model
server on a single-host TPU VM. Multi-host TPU model serving requires a
group of SAX model servers on a multi-host TPU slice.
SAX model serving
The following section walks through the workflow for serving language models using SAX. It uses the GPT-J 6B model as an example for single-host model serving, and a 175B test model for multi-host model serving.
Before starting, install the Cloud TPU SAX Docker images on your TPU VM:
sudo usermod -a -G docker ${USER} newgrp docker gcloud auth configure-docker us-docker.pkg.dev SAX_ADMIN_SERVER_IMAGE_NAME=""us-docker.pkg.dev/cloud-tpu-images/inference/sax-admin-server"" SAX_MODEL_SERVER_IMAGE_NAME=""us-docker.pkg.dev/cloud-tpu-images/inference/sax-model-server"" SAX_UTIL_IMAGE_NAME=""us-docker.pkg.dev/cloud-tpu-images/inference/sax-util"" SAX_VERSION=v1.1.0 export SAX_ADMIN_SERVER_IMAGE_URL=${SAX_ADMIN_SERVER_IMAGE_NAME}:${SAX_VERSION} export SAX_MODEL_SERVER_IMAGE_URL=${SAX_MODEL_SERVER_IMAGE_NAME}:${SAX_VERSION} export SAX_UTIL_IMAGE_URL=""${SAX_UTIL_IMAGE_NAME}:${SAX_VERSION}"" docker pull ${SAX_ADMIN_SERVER_IMAGE_URL} docker pull ${SAX_MODEL_SERVER_IMAGE_URL} docker pull ${SAX_UTIL_IMAGE_URL}
Set some other variables you will use later:
export SAX_ADMIN_SERVER_DOCKER_NAME=""sax-admin-server"" export SAX_MODEL_SERVER_DOCKER_NAME=""sax-model-server"" export SAX_CELL=""/sax/test""
GPT-J 6B single-host model serving example
Single-host model serving is applicable to single-host TPU slice, that is, v5litepod-1, v5litepod-4 and v5litepod-8.
Create a SAX cluster
Create a Cloud Storage storage bucket for the SAX cluster:
SAX_ADMIN_STORAGE_BUCKET=${your_admin_storage_bucket} gcloud storage buckets create gs://${SAX_ADMIN_STORAGE_BUCKET} \ --project=${PROJECT_ID}
You might need another Cloud Storage storage bucket to store the checkpoint.
SAX_DATA_STORAGE_BUCKET=${your_data_storage_bucket}
Connect to your TPU VM using SSH in a terminal to launch the SAX admin server:
docker run \ --name ${SAX_ADMIN_SERVER_DOCKER_NAME} \ -it \ -d \ --rm \ --network host \ --env GSBUCKET=${SAX_ADMIN_STORAGE_BUCKET} \ ${SAX_ADMIN_SERVER_IMAGE_URL}
You can check the Docker log by:
docker logs -f ${SAX_ADMIN_SERVER_DOCKER_NAME}
The output in the log will look similar to the following:
I0829 01:22:31.184198 7 config.go:111] Creating config fs_root: ""gs://test_sax_admin/sax-fs-root"" I0829 01:22:31.347883 7 config.go:115] Created config fs_root: ""gs://test_sax_admin/sax-fs-root"" I0829 01:22:31.360837 24 admin_server.go:44] Starting the server I0829 01:22:31.361420 24 ipaddr.go:39] Skipping non-global IP address 127.0.0.1/8. I0829 01:22:31.361455 24 ipaddr.go:39] Skipping non-global IP address ::1/128. I0829 01:22:31.361462 24 ipaddr.go:39] Skipping non-global IP address fe80::4001:aff:fe8e:fc8/64. I0829 01:22:31.361469 24 ipaddr.go:39] Skipping non-global IP address fe80::42:bfff:fef9:1bd3/64. I0829 01:22:31.361474 24 ipaddr.go:39] Skipping non-global IP address fe80::20fb:c3ff:fe5b:baac/64. I0829 01:22:31.361482 24 ipaddr.go:56] IPNet address 10.142.15.200 I0829 01:22:31.361488 24 ipaddr.go:56] IPNet address 172.17.0.1 I0829 01:22:31.456952 24 admin.go:305] Loaded config: fs_root: ""gs://test_sax_admin/sax-fs-root"" I0829 01:22:31.609323 24 addr.go:105] SetAddr /gcs/test_sax_admin/sax-root/sax/test/location.proto ""10.142.15.200:10000"" I0829 01:22:31.656021 24 admin.go:325] Updated config: fs_root: ""gs://test_sax_admin/sax-fs-root"" I0829 01:22:31.773245 24 mgr.go:781] Loaded manager state I0829 01:22:31.773260 24 mgr.go:784] Refreshing manager state every 10s I0829 01:22:31.773285 24 admin.go:350] Starting the server on port 10000 I0829 01:22:31.773292 24 cloud.go:506] Starting the HTTP server on port 8080
-
Launch a single-host SAX model server into the SAX cluster:
At this point, the SAX cluster contains only the SAX admin server. You can connect to your TPU VM over SSH in a second terminal to launch a SAX model server in your SAX cluster:
docker run \ --privileged \ -it \ -d \ --rm \ --network host \ --name ${SAX_MODEL_SERVER_DOCKER_NAME} \ --env SAX_ROOT=gs://${SAX_ADMIN_STORAGE_BUCKET}/sax-root \ ${SAX_MODEL_SERVER_IMAGE_URL} \ --sax_cell=${SAX_CELL} \ --port=10001 \ --platform_chip=tpuv4 \ --platform_topology=1x1
You can check the Docker log by:
docker logs -f ${SAX_MODEL_SERVER_DOCKER_NAME}
Convert model checkpoint:
You need to install PyTorch and Transformers to download the GPT-J checkpoint from EleutherAI:
pip3 install accelerate pip3 install torch pip3 install transformers
To convert the checkpoint to SAX checkpoint, you need to install
paxml:
pip3 install paxml==1.1.0
Then, set the following variable:
>>PT_CHECKPOINT_PATH=./fine_tuned_pt_checkpoint
Download the fine tuned PyTorch checkpoint to
${PT_CHECKPOINT_PATH}, follow
https://github.com/mlcommons/inference/blob/master/language/gpt-j/README.md#download-gpt-j-model, and run the following commands:
ls ${PT_CHECKPOINT_PATH}
This should list the following:
added_tokens.json generation_config.json pytorch_model.bin.index.json pytorch_model-00001-of-00003.bin pytorch_model-00002-of-00003.bin pytorch_model-00003-of-00003.bin special_tokens_map.json trainer_state.json config.json merges.txt tokenizer_config.json vocab.json
The following
[script](https://github.com/google/saxml/blob/main/saxml/tools/convert_gptj_ckpt.py)converts the GPT-J checkpoint to SAX checkpoint, we use
${PT_CHECKPOINT_PATH}as the base model checkpoint, and after conversion, you will find the converted checkpoint in
${CONVERTED_CHECKPOINT_PATH}:
wget https://raw.githubusercontent.com/google/saxml/main/saxml/tools/convert_gptj_ckpt.py python3 -m convert_gptj_ckpt --base ${PT_CHECKPOINT_PATH} --pax ${CONVERTED_CHECKPOINT_PATH}
This should print output similar to the following:
transformer.wte.weight (50401, 4096) transformer.h.0.ln_1.weight (4096,) transformer.h.0.ln_1.bias (4096,) transformer.h.0.attn.k_proj.weight (4096, 4096) . . . transformer.ln_f.weight (4096,) transformer.ln_f.bias (4096,) lm_head.weight (50401, 4096) lm_head.bias (50401,) Saving the pax model to . done
After the conversion is done, enter the following command:
ls checkpoint_00000000/
This should list the following:
metadate state
You need to create a
commit_successfile and placed in the sub directories:
CHECKPOINT_PATH=gs://${SAX_DATA_STORAGE_BUCKET}/path/to/checkpoint_00000000 gsutil -m cp -r checkpoint_00000000 ${CHECKPOINT_PATH} touch commit_success.txt gsutil cp commit_success.txt ${CHECKPOINT_PATH}/ gsutil cp commit_success.txt ${CHECKPOINT_PATH}/metadata/ gsutil cp commit_success.txt ${CHECKPOINT_PATH}/state/
Publish the model to SAX cluster
You can now publish GPT-J with the checkpoint converted in the
[previous step](#convert-checkpoint).
MODEL_NAME=gptj4bf16bs32 MODEL_CONFIG_PATH=saxml.server.pax.lm.params.gptj.GPTJ4BF16BS32 REPLICA=1
To publish the GPT-J (and steps afterward), use SSH to connect to your TPU VM in a third terminal:
docker run \ ${SAX_UTIL_IMAGE_URL} \ --sax_root=gs://${SAX_ADMIN_STORAGE_BUCKET}/sax-root \ publish \ ${SAX_CELL}/${MODEL_NAME} \ ${MODEL_CONFIG_PATH} \ ${CHECKPOINT_PATH} \ ${REPLICA}
You will see a lot of activity from the model server Docker log until you see something like the following to indicate the model has loaded successfully:
I0829 01:33:49.287459 139865140229696 servable_model.py:697] loading completed.
To list published models:
docker run \ ${SAX_UTIL_IMAGE_URL} \ --sax_root=gs://${SAX_ADMIN_STORAGE_BUCKET}/sax-root \ list ${SAX_CELL}
You will see:
+---+---------------+ | # | MODEL ID | +---+---------------+ | 0 | gptj4bf16bs32 | +---+---------------+
If you want to use tokenized input and generate tokenized output, instead of using the above model config, you can publish using:
MODEL_NAME=gptj4tokenizedbf16bs32 MODEL_CONFIG_PATH=saxml.server.pax.lm.params.gptj.GPTJ4TokenizedBF16BS32 REPLICA=1
Generate inference results
To generate a summary for your article using
GPTJ4BF16BS32:
TEXT = (""Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction\:\nSummarize the following news article\:\n\n### Input\:\nMarch 10, 2015 . We're truly international in scope on Tuesday. We're visiting Italy, Russia, the United Arab Emirates, and the Himalayan Mountains. Find out who's attempting to circumnavigate the globe in a plane powered partially by the sun, and explore the mysterious appearance of craters in northern Asia. You'll also get a view of Mount Everest that was previously reserved for climbers. On this page you will find today's show Transcript and a place for you to request to be on the CNN Student News Roll Call. TRANSCRIPT . Click here to access the transcript of today's CNN Student News program. Please note that there may be a delay between the time when the video is available and when the transcript is published. CNN Student News is created by a team of journalists who consider the Common Core State Standards, national standards in different subject areas, and state standards when producing the show. ROLL CALL . For a chance to be mentioned on the next CNN Student News, comment on the bottom of this page with your school name, mascot, city and state. We will be selecting schools from the comments of the previous show. You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call! Thank you for using CNN Student News!\n\n### Response\:"")
docker run \ ${SAX_UTIL_IMAGE_URL} \ --sax_root=gs://${SAX_ADMIN_STORAGE_BUCKET}/sax-root \ lm.generate \ ${SAX_CELL}/${MODEL_NAME} \ ${TEXT}
You can expect something similar to:
+--------------------------------+------------+ | GENERATE | SCORE | +--------------------------------+------------+ | This page includes the | -1.0517541 | | show Transcript. The daily | | | transcript is a written | | | version of each day's CNN | | | Student News program. Use the | | | Transcript to help students | | | with reading comprehension and | | | vocabulary. At the bottom of | | | the page, comment for a chance | | | to be mentioned on CNN Student | | | News. You must be a teacher | | | or a student age 13 or older | | | to request a mention on the | | | CNN Student News Roll Call. | | | ... | | +--------------------------------+------------+
If you are using the
GPTJ4TokenizedBF16BS32, the input must be formatted as a comma separated token ID string. You will need to tokenize the text input.
TEXT=(""Below is an instruction that describes a task, paired with "" ""an input that provides further context. Write a response that "" ""appropriately completes the request.\n\n### Instruction\:\nSummarize the "" ""following news article\:\n\n### Input\:\nMarch 10, 2015 . We're truly "" ""international in scope on Tuesday. We're visiting Italy, Russia, the "" ""United Arab Emirates, and the Himalayan Mountains. Find out who's "" ""attempting to circumnavigate the globe in a plane powered partially by the "" ""sun, and explore the mysterious appearance of craters in northern Asia. "" ""You'll also get a view of Mount Everest that was previously reserved for "" ""climbers. On this page you will find today's show Transcript and a place "" ""for you to request to be on the CNN Student News Roll Call. TRANSCRIPT . "" ""Click here to access the transcript of today's CNN Student News program. "" ""Please note that there may be a delay between the time when the video is "" ""available and when the transcript is published. CNN Student News is "" ""created by a team of journalists who consider the Common Core State "" ""Standards, national standards in different subject areas, and state "" ""standards when producing the show. ROLL CALL . For a chance to be "" ""mentioned on the next CNN Student News, comment on the bottom of this page "" ""with your school name, mascot, city and state. We will be selecting "" ""schools from the comments of the previous show. You must be a teacher or a "" ""student age 13 or older to request a mention on the CNN Student News Roll "" ""Call! Thank you for using CNN Student News!\n\n### Response\:"")
You can obtain the token IDs string through the EleutherAI/gpt-j-6b tokenizer:
from transformers import GPT2Tokenizer tokenizer = GPT2Tokenizer.from_pretrained(${PT_CHECKPOINT_PATH})
Tokenize the input text:
encoded_example = tokenizer(TEXT) input_ids = encoded_example.input_ids INPUT_STR = "","".join([str(input_id) for input_id in input_ids])
You can expect a token ID string similar to the following:
>>> INPUT_STR '21106,318,281,12064,326,8477,257,4876,11,20312,351,281,5128,326,3769,2252,4732,13,19430,257,2882,326,20431,32543,262,2581,13,198,198,21017,46486,25,198,13065,3876,1096,262,1708,1705,2708,25,198,198,21017,23412,25,198,16192,838,11,1853,764,775,821,4988,3230,287,8354,319,3431,13,775,821,10013,8031,11,3284,11,262,1578,4498,24880,11,290,262,42438,22931,21124,13,9938,503,508,338,9361,284,2498,4182,615,10055,262,13342,287,257,6614,13232,12387,416,262,4252,11,290,7301,262,11428,5585,286,1067,8605,287,7840,7229,13,921,1183,635,651,257,1570,286,5628,41336,326,373,4271,10395,329,39311,13,1550,428,2443,345,481,1064,1909,338,905,42978,290,257,1295,329,345,284,2581,284,307,319,262,8100,13613,3000,8299,4889,13,48213,6173,46023,764,6914,994,284,1895,262,14687,286,1909,338,8100,13613,3000,1430,13,4222,3465,326,612,743,307,257,5711,1022,262,640,618,262,2008,318,1695,290,618,262,14687,318,3199,13,8100,13613,3000,318,2727,416,257,1074,286,9046,508,2074,262,8070,7231,1812,20130,11,2260,5423,287,1180,2426,3006,11,290,1181,5423,618,9194,262,905,13,15107,3069,42815,764,1114,257,2863,284,307,4750,319,262,1306,8100,13613,3000,11,2912,319,262,4220,286,428,2443,351,534,1524,1438,11,37358,11,1748,290,1181,13,775,481,307,17246,4266,422,262,3651,286,262,2180,905,13,921,1276,307,257,4701,393,257,3710,2479,1511,393,4697,284,2581,257,3068,319,262,8100,13613,3000,8299,4889,0,6952,345,329,1262,8100,13613,3000,0,198,198,21017,18261,25'
To generate a summary for your article:
docker run \ ${SAX_UTIL_IMAGE_URL} \ --sax_root=gs://${SAX_ADMIN_STORAGE_BUCKET}/sax-root \ lm.generate \ ${SAX_CELL}/${MODEL_NAME} \ ${INPUT_STR}
You can expect something similar to:
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------+ | GENERATE | SCORE | +----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------+ | 1212,2443,3407,262,905,42978,764,198,11041,262,42978,284,1037,2444,351,3555,35915,290,25818,764,198,2953,262,4220,286,262,2443,11,2912,329,257,2863,284,307,4750,319,8100,13613,3000,13,220,921,1276,307,257,4701,393,257,3710,2479,1511,393,4697,284,2581,257,3068,319,262,8100,13613,3000,8299,4889,13,50256 | -0.023136413 | | 1212,2443,3407,262,905,42978,764,198,11041,262,42978,284,1037,2444,351,3555,35915,290,25818,764,198,2953,262,4220,286,262,2443,11,2912,329,257,2863,284,307,4750,319,8100,13613,3000,13,220,921,1276,307,257,4701,393,257,3710,2479,1511,393,4697,284,2581,257,3068,319,262,8100,13613,3000,8299,4889,0,50256 | -0.91842502 | | 1212,2443,3407,262,905,42978,764,198,11041,262,42978,284,1037,2444,351,3555,35915,290,25818,764,198,2953,262,4220,286,262,2443,11,2912,329,257,2863,284,307,4750,319,8100,13613,3000,13,921,1276,307,257,4701,393,257,3710,2479,1511,393,4697,284,2581,257,3068,319,262,8100,13613,3000,8299,4889,13,50256 | -1.1726116 | | 1212,2443,3407,262,905,42978,764,198,11041,262,42978,284,1037,2444,351,3555,35915,290,25818,764,198,2953,262,4220,286,262,2443,11,2912,329,257,2863,284,307,4750,319,8100,13613,3000,13,220,921,1276,307,1511,393,4697,284,2581,257,3068,319,262,8100,13613,3000,8299,4889,13,50256 | -1.2472695 | +----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------+
To detokenize the output token IDs string:
output_token_ids = [int(token_id) for token_id in OUTPUT_STR.split(',')] OUTPUT_TEXT = tokenizer.decode(output_token_ids, skip_special_tokens=True)
You can expect the detokenized text as:
>>> OUTPUT_TEXT 'This page includes the show Transcript.\nUse the Transcript to help students with reading comprehension and vocabulary.\nAt the bottom of the page, comment for a chance to be mentioned on CNN Student News. You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call.'
[Clean up](#cleanup)your Docker containers and Cloud Storage storage buckets.
175B multi-host model serving
Some of the large language models will require a multi-host TPU slice, that is, v5litepod-16 and above. In those cases, all multi-host TPU hosts will need to have a copy of a SAX model server, and all model servers function as a SAX model server group to serve the large model on a multi-host TPU slice.
Create a new SAX cluster
You can follow the same step of Create a SAX cluster in the
[GPT-J walk through](#single-host-example)to create a new SAX cluster and a SAX admin server.
Or, if you already have an existing SAX cluster, you can launch a multi-host model server into your SAX cluster.
Launch a multi-host SAX model server into a SAX cluster
Use the same command to create a multi-host TPU slice as you use for a single-host TPU slice, just specify the appropriate multi-host accelerator type:
ACCELERATOR_TYPE=v5litepod-32 ZONE=us-east1-c gcloud alpha compute tpus queued-resources create ${QUEUED_RESOURCE_ID} \ --node-id ${TPU_NAME} \ --project ${PROJECT_ID} \ --zone ${ZONE} \ --accelerator-type ${ACCELERATOR_TYPE} \ --runtime-version ${RUNTIME_VERSION} \ --service-account ${SERVICE_ACCOUNT} \ --${QUOTA_TYPE}Note: The
QUOTA_TYPEflag can be either
reservedor
best-effort. See
[Quota Types](https://cloud.google.com/tpu/docs/quota#quota_types)for information on the different types of quotas supported by Cloud TPU. Use the following command to pull the SAX model server image to all TPU hosts/workers and launch them: To pull the SAX model server image to all TPU hosts/workers and launch them:
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \ --project ${PROJECT_ID} \ --zone ${ZONE} \ --worker=all \ --command="" gcloud auth configure-docker \ us-docker.pkg.dev # Pull SAX model server image docker pull ${SAX_MODEL_SERVER_IMAGE_URL} # Run model server docker run \ --privileged \ -it \ -d \ --rm \ --network host \ --name ${SAX_MODEL_SERVER_DOCKER_NAME} \ --env SAX_ROOT=gs://${SAX_ADMIN_STORAGE_BUCKET}/sax-root \ ${SAX_MODEL_SERVER_IMAGE_URL} \ --sax_cell=${SAX_CELL} \ --port=10001 \ --platform_chip=tpuv4 \ --platform_topology=1x1""
Publish the model to SAX cluster
This example uses a
[LmCloudSpmd175B32Test](https://github.com/google/saxml/blob/main/saxml/server/pax/lm/params/lm_cloud.py)model:
MODEL_NAME=lmcloudspmd175b32test MODEL_CONFIG_PATH=saxml.server.pax.lm.params.lm_cloud.LmCloudSpmd175B32Test CHECKPOINT_PATH=None REPLICA=1
To publish the test model:
docker run \ ${SAX_UTIL_IMAGE_URL} \ --sax_root=gs://${SAX_ADMIN_STORAGE_BUCKET}/sax-root \ publish \ ${SAX_CELL}/${MODEL_NAME} \ ${MODEL_CONFIG_PATH} \ ${CHECKPOINT_PATH} \ ${REPLICA}
Generate inference results
docker run \ ${SAX_UTIL_IMAGE_URL} \ --sax_root=gs://${SAX_ADMIN_STORAGE_BUCKET}/sax-root \ lm.generate \ ${SAX_CELL}/${MODEL_NAME} \ ""Q: Who is Harry Porter's mother? A\: ""
Note that since this example uses a test model with random weights, the output may not be meaningful.
Clean Up
Stop the docker containers:
docker stop ${SAX_ADMIN_SERVER_DOCKER_NAME} docker stop ${SAX_MODEL_SERVER_DOCKER_NAME}
Delete your Cloud Storage admin storage bucket and any data storage bucket using
gsutilas shown below.
gsutil rm -rf gs://${SAX_ADMIN_STORAGE_BUCKET} gsutil rm -rf gs://${SAX_DATA_STORAGE_BUCKET}
Profiling
After setting up the inference, profilers can be used to analyze the performance and TPU utilization. References to some profiling related documents are shown below:
Support and Feedback
We welcome all feedback! To share feedback or request support, reach out to
us
[here](https://forms.gle/pLFRKSdWZ97o2o867) or by
emailing [cloudtpu-support@google.com](mailto:cloudtpu-support@google.com)
Terms
All information Google has provided to you regarding this software release
is Google's confidential information and subject to the confidentiality
provisions in the
[Google Cloud Platform Terms of
Service](https://cloud.google.com/terms) (or other agreement governing your use of Google Cloud Platform).",Cloud TPU v5e Inference introduction | Google Cloud,
id,url,body,title,description
27,https://cloud.google.com/tpu/docs/troubleshooting/tpu-vm-monitoring,"Monitoring Cloud TPU VMs
This guide explains how to use
[Google Cloud Monitoring](https://cloud.google.com/monitoring) to monitor
your Cloud TPU VMs. Google Cloud Monitoring automatically collects [metrics](https://cloud.google.com/monitoring/api/metrics_gcp)
and [logs](https://cloud.google.com/logging) from your Cloud TPU
and its host Compute Engine. These data can be used to monitor the health
of your Cloud TPU and Compute Engine.
Metrics enable you to track a numerical quantity over time, for example, CPU
utilization, network usage, or TensorCore idle duration. Logs capture events at
a specific point in time. Log entries are written by your own code, Google Cloud
services, third-party applications, and the Google Cloud infrastructure. You can
also generate metrics from the data present in a log entry by creating a
[log-based metric](https://cloud.google.com/logging/docs/logs-based-metrics).
You can also set [alert policies](https://cloud.google.com/monitoring/alerts)
based on metric values or log entries.
This guide discusses Google Cloud Monitoring and shows you how to:
- View Cloud TPU metrics
- Set up Cloud TPU metrics alert policies
- Query Cloud TPU logs
- Create log-based metrics for setting up alerts and visualizing dashboards
Prerequisites
This document assumes some basic knowledge of
[Google Cloud Monitoring](https://cloud.google.com/monitoring).
You must have a Compute Engine VM and Cloud TPU
resources created before you can begin generating and working with [Google Cloud Monitoring](https://cloud.google.com/monitoring).
See the [Cloud Cloud TPU Quickstart](/tpu/docs/quickstart) for more details.
Metrics
Google Cloud metrics are automatically generated by Compute Engine VMs and the Cloud TPU runtime. The following metrics are generated by Cloud TPU VMs:
memory/usage
network/received_bytes_count
network/sent_bytes_count
cpu/utilization
tpu/tensorcore/idle_duration
Memory usage
The
memory/usage metric tracks the memory currently being used by the Cloud TPU VM in
bytes. This metric is sampled every 60 seconds. It may take up to 180 seconds
between the time a value is generated and when it's displayed.
Network received bytes count
The
network/received_bytes_count metric tracks the number of cumulative bytes of
data the Cloud TPU VM received over the network at a point in time. It may take up to
180 seconds between the time a value is generated and when it's displayed.
Network sent bytes count
The
network/sent_bytes_count metric tracks the number of cumulative bytes the
Cloud TPU VM sent over the network at a point in time. It may take up to 180 seconds
between the time a value is generated and when it's displayed.
CPU utilization
THe
cpu/utilization metric tracks the current CPU utilization on the Cloud TPU worker,
represented as a percentage. Values are typically between 0.0 and 100.0, but
might exceed 100.0. Sampled every 60 seconds. It may take up to 180 seconds
between the time a value is generated and when it's displayed.
TensorCore idle duration
The
tpu/tensorcore/idle_duration metric tracks the number of seconds each TPU
chip's TensorCore has been idle. This metric is available for each chip on all
Cloud TPU in use. If a TensorCore is in use, the idle duration value is reset to
zero. When the TensorCore is no longer in use, the idle duration value starts
to increase.
The following graph shows the
tpu/tensorcore/idle_duration metric for a v2-8
Cloud TPU VM which has one worker. Each worker has four chips. In this example, all four
chips have the same values for
tpu/tensorcore/idle_duration, so the graphs are
superimposed on each other.
For a complete list of metrics generated by Cloud TPU, see
[Google Cloud Cloud TPU metrics](https://cloud.google.com/monitoring/api/metrics_gcp#gcp-tpu).
Viewing metrics
You can view metrics using the
[Metrics Explorer](https://cloud.google.com/monitoring/charts/metrics-explorer)
in the Google Cloud console.
In the Metrics Explorer, click SELECT A METRIC
and search for
Cloud TPU Worker. If Show only active resources and metrics is
on, only metrics that are currently being generated will be displayed. Click
Cloud TPU Worker to display the available metrics.
You can also access metrics using curl HTTP calls:
Use the Try it! button in the
[projects.timeSeries.query documentation](https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.timeSeries/query)
to retrieve the value for a metric within the specified timeframe.
- Fill in the name in the following format: projects/{project-name}
- Add a query to the Request body section. The following is a sample query
for retrieving the idle duration metric for the specified zone for the last five
minutes
fetch tpu_worker | filter zone = 'us-central2-b' | metric tpu.googleapis.com/tpu/tensorcore/idle_duration | within 5m
- Click Execute to make the call and see the results of the HTTP POST message
The
[Monitoring Query Language reference](https://cloud.google.com/monitoring/mql/reference)
document has more information on how to customize this query.
You can create
[alert policies](https://cloud.google.com/monitoring/alerts)
that tell Google Cloud Monitoring to send an alert when a condition is met.
Creating alerts
The steps in this section show an example of how to add an alert policy for the TensorCore Idle Duration metric. Whenever this metric exceeds 24 hours, Cloud Monitoring sends an email to the registered email address.
[Go to the Monitoring console](https://console.cloud.google.com/monitoring)
- In the navigation pane click Alerting
- Click EDIT NOTIFICATION CHANNELS
- Under Email, click ADD NEW. Type an email address, a display name, and click SAVE
- Click CREATE POLICY
- Click SELECT A METRIC and then select Tensorcore Idle Duration and click APPLY
- Click NEXT and then Threshold
- For Alert trigger, select Any time series violates
- For Threshold Position, select Above threshold
- For Threshold Value, type
86400000
- Click NEXT
- Under Notification Channels select your email notification channel and click OK
- Type a name for the alert policy
- Click NEXT and then CREATE POLICY
When the TensorCore Idle Duration goes over 24 hours, an email is sent to the email address you specified.
Logging
Log entries are written by Google Cloud services, third party services, ML
frameworks or your code. You can view logs using the Logs Viewer or Logs API.
For more information about Google Cloud logging, see
[Google Cloud Logging](https://cloud.google.com/logging).
In the Logs Explorer, you can select a resource type:
- Cloud TPU Worker -> Zone -> Node ID
- Audited Resource -> Cloud TPU -> API (
google.cloud.tpu.v2alpha1.Tpu.CreateNode,
google.cloud.tpu.v2alpha1.Tpu.DeleteNode,
google.cloud.tpu.v2alpha1.Tpu.UpdateNode)
Cloud TPU Worker logs contain information about a specific Cloud TPU worker in a specific
zone, for example the amount of memory available on the Cloud TPU worker
(
system_available_memory_GiB).
Audited Resource logs contain information about when a specific Cloud TPU API
was called and who made the call. For example
CreateNode,
UpdateNode, and
DeleteNode.
ML frameworks can generate logs to stdout and stderr. These logs are controlled by environment variables and are read by your training script.
Your code can write logs to Google Cloud Logging. For more information, see
[Write standard logs](https://cloud.google.com/logging/docs/samples/logging-stdlogging) and
[Write structured logs](https://cloud.google.com/logging/docs/samples/logging-write-log-entry).
To view Cloud TPU logs:
- Go to the Google Cloud Logs Viewer
- Click the Resource drop-down
- Click Cloud TPU Worker
- Select a zone
- Select the Cloud TPU you're interested in
- Click Apply. Logs are displayed in the query results
To view Audited Resource logs:
- Go to the Google Cloud Logs Viewer
- Click the Resource drop-down
- Click Audited Resource and then Cloud TPU
- Choose the Cloud TPU API that you're interested in
- Click Apply. Logs are displayed in the query results
- Choose the APIs that begin with
google.cloud.tpu.v2alpha1.Tpu
Query Google Cloud Logs
When you view logs in the Google Cloud console, the page performs a default query.
You can view the query by selecting the
Show query toggle switch. You can
modify the default query or create a new one. For more information, see
[Build Queries in the Logs Explorer](/logging/docs/view/building-queries).
Understanding the log output for Audited Resource logs
Click any log entry to expand it, and you will find a field called
protoPayload.
Expand
protoPayload, and you will see a number of subfields:
- logName: the name of the log
- protoPayload -> @type: the type of the log
- resourceName: the name of your Cloud TPU
- methodName: the name of the method called (audit logs only)
- request -> @type: the request type
- request -> node: details about the Cloud TPU node
- request -> node_id: the name of the TPU
- severity: the severity of the log
Understanding the log output for Cloud TPU Worker logs
Click any log entry to expand it, and you will find a field called
jsonPayload.
Expand
jsonPayload and you will see a number of subfields:
- accelerator_type: the accelerator type
- consumer_project: the project where the Cloud TPU lives
- evententry_timestamp: the time when the log was generated
- system_available_memory_GiB: the available memory on the Cloud TPU worker (0 ~ 350 GiB)
Creating log-based metrics
This section describes how to create
[log-based metrics](/logging/docs/logs-based-metrics)
used for setting up monitoring dashboards and alerts. For information about
programmatically creating log-based metrics, see
[Creating log-based metrics programmatically using the Cloud Logging REST API](#rest_api).
The following example uses the system_available_memory_GiB subfield to demonstrate how to create a log-based metric for monitoring Cloud TPU worker available memory.
- Navigate to the Logs Explorer
In the query box, enter the following query to extract all log entries that have system_available_memory_GiB defined for the primary Cloud TPU worker:
resource.type=tpu_worker resource.labels.project_id=your-project resource.labels.zone=your-tpu-zone resource.labels.node_id=your-tpu-name resource.labels.worker_id=0 logName=projects/your-project/logs/tpu.googleapis.com%2Fruntime_monitor jsonPayload.system_available_memory_GiB:*
Click Create metric to display the Metric Editor
Under Metric Type, choose Distribution
Type a name, optional description, and unit of measurement for your metric. enter ""matrix_unit_utilization_percent"" and ""MXU utilization"" in the Name and Description fields, respectively
The filter is pre-populated with the script that you entered in the Logs Explorer
Click CREATE METRIC
Click Explore Metrics to view your new metric. It make take a few minutes before your metrics are displayed
Creating log-based metrics programmatically using the Cloud Logging REST API
You can also create log-based metrics through the
[Cloud Logging API](/logging/docs/reference/v2/rest).
For more information, see [Creating a distribution metric](/logging/docs/logs-based-metrics/distribution-metrics).
Creating dashboards and alerts using log-based metrics
Dashboards are useful for visualizing metrics (expect ~2 minute delay); alerts
are helpful for sending notifications when errors occur. For more information,
see
[Manage custom dashboards](https://cloud.google.com/monitoring/charts/dashboards)
and [Create metric-based alert policies](https://cloud.google.com/monitoring/alerts/using-alerting-ui).
Creating dashboards
To create a dashboard in Cloud Monitoring for the Tensorcore idle duration metric:
[Go to the Monitoring console](https://console.cloud.google.com/monitoring)
- In the navigation pane, click Dashboards
- Click CREATE DASHBOARD and then Add Chart
- Choose the chart type that you want to add. For this example, choose Line
- Type a title for the dashboard
- Click the button underneath Resource & Metric
- Scroll down the list of resources/metrics and select Cloud TPU Worker -> Tpu -> Tensorcore idle duration
- Click Apply
- To filter the dashboard contents, click CREATE DASHBOARD FILTERS
- In the Label field, set project_id to your project
- Click ADD and set zone to the zone where you created your TPU
- Add another filter for node_id and specify your Cloud TPU name",Monitoring Cloud TPU VMs | Google Cloud,
id,url,body,title,description
44,https://cloud.google.com/tpu/docs/reference/rest/v1alpha1/projects.locations,"Resource: Location
A resource that represents a Google Cloud location.
|JSON representation
|
{ ""name"": string, ""locationId"": string, ""displayName"": string, ""labels"": { string: string, ... }, ""metadata"": { ""@type"": string, field1: ..., ... } }
|Fields
|
name
|
Resource name for the location, which may vary between implementations. For example:
|
locationId
|
The canonical id for this location. For example:
|
displayName
|
The friendly name for this location, typically a nearby city name. For example, ""Tokyo"".
|
labels
|
Cross-service attributes for the location. For example
An object containing a list of
|
metadata
|
Service-specific metadata. For example the available capacity at the given location.
An object containing fields of an arbitrary type. An additional field
|
Methods
|
|Gets information about a location.
|
|Lists information about the supported locations for this service.",REST Resource: projects.locations | Cloud TPU | Google Cloud,
id,url,body,title,description
11,https://cloud.google.com/tpu/docs/ctpu-reference,"CTPU Reference
Overview
The open source
ctpu tool is used to create a flock of compute resources,
which consist of a Compute Engine VM and one or more
Cloud TPU devices. The tool is pre-installed in your Cloud
shell.
You can find
[documentation and code for ctpu](https://github.com/tensorflow/tpu/tree/master/tools/ctpu)
on GitHub.
The
ctpu tool uses the following syntax:
ctpu <subcommand> <flags> <subcommand> <subcommand args>
Following are the subcommands for
ctpu:
auth
- Description
- Set or display authorization(s) for Cloud TPUs.
- Usage
ctpu auth <flags> <subcommand> <subcommand args>
- Example
ctpu auth list --project=""my-project"" --zone=us-central1-a ctpu auth list --project my-project --zone us-central1-a
- Subcommands
The
ctpu authcommand supports the following subcommands:
- add-bigtable - ensure Cloud TPU is authorized for Cloud Bigtable
- add-gcs - ensure Cloud TPU is authorized for Cloud Storage
- list - display Cloud TPU service account authorizations
- commands - list all command names
- flags - describe all known top-level flags
- help - describe subcommands and their syntax
- Optional Flags
The following are optional commands for
ctpu auth.
name | project | zone
delete (rm)
- Description
- Delete your Compute Engine VM and Cloud TPU.
- Usage
ctpu rm <flags>
- Example
ctpu rm --zone=us-central1-b
help
- Description
- List all
ctpusubcommands and top level flags.
- Usage
ctpu help ctpu help <subcommand>
- Example
ctpu help // list all ctpu subcommands and top level flags ctpu help auth // list all flags that can be used with `ctpu auth` ctpu help up // list all flags that can be used with `ctpu up`
list (ls)
- Description
- List all Compute Engine VMs and Cloud TPU in the specified zone.
- Usage
ctpu ls <flags>
- Example
ctpu ls --zone=us-central1-b
pause (zz)
- Description
Stop the Compute Engine VM, and delete your Cloud TPU. Stop charging for Cloud TPU usage until you run
ctpu up.
To ensure that the Cloud TPU is stopped, you must specify the Cloud TPU name and the zone on the command line.
- Usage
ctpu pause <name, zone>
- Example
ctpu pause --name=my-tpu --zone=us-central1-a // pause the named TPU in the specified zone
print-config (cfg)
- Description
- Print onscreen the current configuration of the Cloud TPU name, project name, and zone.
- Example
ctpu print-config
quota
- Description
- Display a URL where you can see quotas.
- Usage
ctpu quota
- Example
ctpu quota Output: Quotas cannot currently be displayed within
ctpu. To view your quota, open <url> Request additional quota from <url>
restart
- Description
Restarts a Cloud TPU that is still in the RUNNING state (shown in
ctpu status), but has stopped running because of a hardware problem. Use
gcloud compute tpu startor the START button on the Compute Engine > TPUs page in the Cloud console if the TPU is in the STOPPED state.
restartdoes not restart a preempted Cloud TPU. You need to run
ctpu deleteand
ctpu upif your Cloud TPU has been preempted.
- Usage
ctpu restart <flags>
- Example
ctpu restart --zone=us-central1-a
status (st)
- Description
Query the GCP APIs (default zone only) to determine the current status of your Cloud TPU and Compute Engine VM.
- Usage
ctpu st
- Example
ctpu st --zone=us-central1-a Status message: Your cluster is running! Compute Engine VM: RUNNING Cloud TPU: RUNNING
tpu-locations
- Description
- List all
[zones](/tpu/docs/types-zones#types)where TPU types are available.
- Usage
- ctpu tpu-locations
- Output
Cloud TPU Locations: asia-east1-c europe-west4-a us-central1-a us-central1-b us-central1-c
tpu-sizes
- Description
- List all available TPU sizes in specified zone. Some sizes are available
only in
[certain zones](/tpu/docs/types-zones#types). (default = default zone)
- Usage
tpu-sizes <zone>
- Example
ctpu tpu-sizes --zone=us-central1-a
up
- Description
Bring up a
ctpuresource set. The first time you run
ctpu upon a project, it takes longer than it will in future runs because it is performing tasks such as SSH key propagation and API turn-up.
- Enables the Compute Engine and Cloud TPU services.
- Creates a Compute Engine VM with the latest stable TensorFlow version pre-installed.
- Assigns a default zone, such as
us-central1-bbased on your location.
- Passes the name of the Cloud TPU to the Compute Engine
VM as an environment variable (
TPU_NAME).
- Ensures your Cloud TPU has access to resources it needs from your Google Cloud project, by granting specific IAM roles to your Cloud TPU service account.
- Performs a number of other checks.
- Logs you into your new Compute Engine VM. Your shell prompt changes
from
username@projectto
username@tpuname.
You can run
ctpu upas often as you like. For example, if you lose the SSH connection to the Compute Engine VM, run
ctpu upto restore the connection. You must specify a zone if your Compute Engine is not in the default zone. For example:
$ ctpu up --zone=us-central1-a
- Usage
ctpu up <flags>
- Example
ctpu up --tpu-size=v2-8 --disk-size-gb=320 --preemptible
- Flags
--disk-size-gb
Configure the root volume size of your Compute Engine VM. Value must be an integer. (default = 250)
--dry-run
Do not make changes; print only what would have happened.
--forward-agent
Enable ssh agent forwarding when sshing into the Compute Engine VM. SSH Agent Forwarding enables access to shared repositories (such as GitHub) without having to place private keys on the Compute Engine VM. (default = true)
--forward-ports
Automatically forward useful ports from the Compute Engine VM to your local machine. Ports forwarded are: 6006 (tensorboard), 8888 (jupyter notebooks), 8470 (TPU port), 8466 (TPU profiler port). (default = true)
--gce-image
Override the automatically chosen Compute Engine Image. Use this flag when you are using your own custom images instead of the ones provided with the installed TensorFlow.
--gcp-network
Specify the network in which the Cloud TPU and associated VM should be created. Refer to
[Virtual Private Cloud (VPC) Network Overview](https://cloud.google.com/vpc/docs/vpc)for information on networks. (default = default network)
--log-http
Print the full content of http request-response pairs. To enable the printout, set this flag to true. Use this flag when you need log output to file a bug report against
ctpu. Refer to
[for details.](https://github.com/tensorflow/tpu/tree/master/tools/ctpu)
ctpuREADME
--machine-type
Configure the size of your Compute Engine VM. A full list of machine types is available on the Cloud
[Machine Types](https://cloud.google.com/compute/docs/machine-types)page. (default = n1-standard-2)
--name
Override the name to use for VMs and Cloud TPU. (default = your username)
--noconf
Skip confirmation.
--preemptible
Create a preemptible Cloud TPU node. A preemptible Cloud TPU costs less per hour than a non-preemptible one. Cloud TPU service can exit a preemptible device at any time. (default = non-preemptible)
--preemptible-vm
Create a preemptible Compute Engine VM. A preemptible VM costs less per hour than a non-preemptible VM. The Compute Engine service can exit the VM instance at any time. (default = non-preemptible)
--print-welcome
Always print the welcome message.
--project
Override the GCP project name to use when allocating VMs and TPUs. Specify a value from cloud config or Compute Engine metadata, usually your project name. If a good value cannot be found, you must to provide a value on the command line.
--tf-version
Set the version of TensorFlow to use when creating the Compute Engine VM and the Cloud TPU. (default = latest stable release)
--tpu-only
Allocate a Cloud TPU only; use this only if you already have a VM available.
--tpu-size
Configure the size and hardware version of a Cloud TPU.
--use-dl-images
Use Deep Learning VM Images (refer docs: https://cloud.google.com/deep-learning-vm/) instead of TPU machine images. (default = TPU machine images)
--vm-only
Allocate a VM only; use this when you are not ready to set up and pay for a TPU.
--zone
Override the Compute Engine zone to use when allocating VMs and Cloud TPU. On the command line, run
ctpu help upto view the list.
version
- Description
- Prints out the version of
ctpuinstalled.
- Usage
ctpu version
- Output
ctpu version Output: ctpu version: 1.9",CTPU Reference | Google Cloud,
id,url,body,title,description
24,https://cloud.google.com/tpu/docs/reference/rest/v2/projects.locations.operations,"Resource: Operation
This resource represents a long-running operation that is the result of a network API call.
|JSON representation
|
{ ""name"": string, ""metadata"": { ""@type"": string, field1: ..., ... }, ""done"": boolean, // Union field
|Fields
|
name
|
The server-assigned name, which is only unique within the same service that originally returns it. If you use the default HTTP mapping, the
|
metadata
|
Service-specific metadata associated with the operation. It typically contains progress information and common metadata such as create time. Some services might not provide such metadata. Any method that returns a long-running operation should document the metadata type, if any.
An object containing fields of an arbitrary type. An additional field
|
done
|
If the value is
|Union field
result. The operation result, which can be either an
error or a valid
response. If
done ==
false, neither
error nor
response is set. If
done ==
true, exactly one of
error or
response can be set. Some services might not provide the result.
result can be only one of the following:
|
error
|
The error result of the operation in case of failure or cancellation.
|
response
|
The normal, successful response of the operation. If the original method returns no data on success, such as
An object containing fields of an arbitrary type. An additional field
|
Methods
|
|Starts asynchronous cancellation on a long-running operation.
|
|Deletes a long-running operation.
|
|Gets the latest state of a long-running operation.
|
|Lists operations that match the specified filter in the request.",REST Resource: projects.locations.operations | Cloud TPU | Google Cloud,
id,url,body,title,description
5,https://cloud.google.com/tpu/docs/reference/rest/v1alpha1/projects.locations.tensorflowVersions/list,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Query parameters](#body.QUERY_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [Try it!](#try-it)
Lists TensorFlow versions supported by this API.
HTTP request
GET https://tpu.googleapis.com/v1alpha1/{parent=projects/*/locations/*}/tensorflowVersions
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
parent
|
Required. The parent resource name.
Query parameters
|Parameters
|
pageSize
|
The maximum number of items to return.
|
pageToken
|
The nextPageToken value returned from a previous List request, if any.
|
filter
|
List filter.
|
orderBy
|
Sort results.
Request body
The request body must be empty.
Response body
Response for
.
[tensorflowVersions.list](/tpu/docs/reference/rest/v1alpha1/projects.locations.tensorflowVersions/list#google.cloud.tpu.v1alpha1.Tpu.ListTensorFlowVersions)
If successful, the response body contains data with the following structure:
|JSON representation
|
{
""tensorflowVersions"": [
{
object (
|Fields
|
tensorflowVersions[]
|
The listed nodes.
|
nextPageToken
|
The next page token or empty if none.
|
unreachable[]
|
Locations that could not be reached.
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).",Method: projects.locations.tensorflowVersions.list | Cloud TPU | Google Cloud,
id,url,body,title,description
0,https://cloud.google.com/tpu/docs/reference/rest/v1/projects.locations.nodes/get,"Method: projects.locations.nodes.get
Stay organized with collections
Save and categorize content based on your preferences.
Gets the details of a node.
HTTP request
GET https://tpu.googleapis.com/v1/{name=projects/*/locations/*/nodes/*}
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
string
Required. The resource name.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[Node](/tpu/docs/reference/rest/v1/projects.locations.nodes#Node)
Authorization scopes
Requires one of the following OAuth scopes:
-
https://www.googleapis.com/auth/cloud.tpu
-
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Method: projects.locations.nodes.get | Cloud TPU | Google Cloud,
id,url,body,title,description
10,https://cloud.google.com/tpu/docs/reference/rest/v2alpha1/projects.locations.runtimeVersions/get,"Method: projects.locations.runtimeVersions.get
Stay organized with collections
Save and categorize content based on your preferences.
HTTP request
GET https://tpu.googleapis.com/v2alpha1/{name=projects/*/locations/*/runtimeVersions/*}
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
string
Required. The resource name.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[RuntimeVersion](/tpu/docs/reference/rest/v2alpha1/projects.locations.runtimeVersions#RuntimeVersion)
Authorization scopes
Requires one of the following OAuth scopes:
-
https://www.googleapis.com/auth/cloud.tpu
-
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Method: projects.locations.runtimeVersions.get | Cloud TPU | Google Cloud,
id,url,body,title,description
18,https://cloud.google.com/tpu/docs/preemptible,"Preemptible TPUs
Preemptible TPUs cost much less than non-preemptible TPUs. The Cloud TPU service might preempt (shut down) these TPUs at any time, if it requires additional TPU resources.
If you are creating a preemptible TPU VM, use the
[.
If you are creating a preemptible TPU Node, you can use the gcloud command](/tpu/docs/preemptible#create-tpuvm)
gcloud command
or the
[Console](https://console.cloud.google.com/compute/tpus). For information on the differences between TPU VMs and TPU Nodes, see [System Architecture](/tpu/docs/system-architecture-tpu-vm).
Creating a preemptible TPU VM
gcloud
$ gcloud compute tpus tpu-vm create demo-tpu \
--zone=europe-west4-a \
--accelerator-type=v3-8 \
--version=tpu-vm-tf-2.15.0-pjrt \
--preemptible
where:
demo-tpuis a name for the TPU.
--accelerator-typespecifies the type of TPU.
--versionspecifies the version of TPU VM software to install.
--preemptibleallows Cloud TPU to preempt the TPU.
Creating a preemptible TPU Node
Console
[Go to the TPUs page](https://console.cloud.google.com/compute/tpus)under Compute Engine on the main page.
- Click CREATE TPU NODE to open the TPU node creation page.
- Type a name for your TPU node.
- Select the zone in which to create the TPU node.
- Select a
[TPU type](/tpu/docs/supported-tpu-configurations)for your TPU node.
- Click Turn on preemtibility for this node to make your TPU node preemptible.
- Select the Tensorflow or PyTorch version to install on your VM.
gcloud
$ gcloud compute tpus execution-groups create \
--name=demo-tpu \
--zone=europe-west4-a \
--accelerator-type=v3-8 \
--tf-version=2.12.0 \
--preemptible
where:
demo-tpuis a name for the TPU.
- `--accelerator-type specifies the type of TPU.
--tf-versionspecifies the version of Tensorflow or PyTorch to install on your VM.
--preemptibleallows Cloud TPU to preempt the TPU.
With TPU Nodes, the preemptible status of a TPU is independent of the preemptible status of your VM instance.
Pricing and quota for preemptible TPUs
Pricing for preemptible TPUs is significantly lower than for normal TPUs.
For details, see the
[pricing page](/tpu/pricing). You are not
charged for TPUs if they are preempted in the first minute after you create
them.
Quota for preemptible TPUs is generally higher, and is separate from the quota
for normal TPUs. See the
[quota page](/tpu/docs/quota).
Detecting if a TPU has been preempted
You use the following
gcloud command to check whether the Cloud TPU service has preempted your TPU:
List your available TPUs:
TPU VM
gcloud compute tpus tpu-vm list --zone=us-central1-b
TPU Node
(vm)$ gcloud compute tpus list --zone=us-central1-b
The above command displays the details of the TPUs created in your project. If a
TPU has been preempted, the status changes from
READY to
PREEMPTED.
For example:
NAME ZONE ACCELERATOR_TYPE NETWORK_ENDPOINT NETWORK RANGE STATUS demo-tpu us-central1-b v2-8 10.240.1.2:8470 default 10.240.1.0/29 PREEMPTED
Preemptible VMs and TPUs (TPU Nodes only)
As described in the
[quickstart guide](/tpu/docs/quickstart) for your framework,
you need a
Compute Engine virtual machine (VM) in order to connect to a TPU.
Note that the preemptible status of the TPU is independent of the preemptible
status of the VM. You can define your TPU as preemptible and the VM as not
preemptible, or the other way round. You can also define them both as
preemptible.
The most likely combination is a preemptible TPU and a non-preemptible VM. Note the following points:
- The charges for the VM are likely to be low in relation to the charges for
the TPU. The VM charges depend on the machine type you use. See the
[pricing page](/tpu/pricing)for a simple example of the relative costs.
- Cloud TPU does not coordinate the preempting of the VM and the TPU. If you define them both as preemptible, the VM and the TPU can be preempted at different times.
- If Compute Engine preempts your VM, you are still charged for the TPU (unless the TPU is itself preempted). Note that the TPU is idle while the VM is preempted.
- Preemptible instances, both Compute Engine VM and Cloud TPU instances,
are always preempted after they run for 24 hours.
[Certain actions](https://cloud.google.com/compute/docs/instances/preemptible#preemption_selection)reset this 24-hour counter.
Detecting if a VM instance has been preempted (TPU Nodes only)
To check whether the VM instance has been preempted, use the
gcloud compute operations list command to get a list of recent system
operations. Add a
name filter to only display the instances you currently
have running or add the
operationType filter to only display resources
that have been preempted.
For example, use the following command to display only the instances with
the specified instance name:
$ gcloud compute operations list--filter=""name=( 'NAME' my-vm)""
The following example displays only the resources that have been preempted:
$ gcloud compute operations list --filter=""operationType=compute.instances.preempted""
For more details, see the
[Compute Engine guide](/compute/docs/instances/preemptible).
Designing your machine learning application to run on preemptible TPUs
Make sure your application is resilient to restarts of the VM and TPU, by saving model checkpoints regularly and by configuring your application to restore the most recent checkpoint on restart.",Preemptible TPUs | Google Cloud,
id,url,body,title,description
13,https://cloud.google.com/tpu/docs/troubleshooting/tpu-node-monitoring,"Monitoring Cloud TPU Nodes
This guide explains how to use
[Google Cloud Monitoring](https://cloud.google.com/monitoring) to monitor
your Cloud TPU Nodes. Google Cloud Monitoring automatically collects [metrics](https://cloud.google.com/monitoring/api/metrics_gcp)
and [logs](https://cloud.google.com/logging) from your Cloud TPU
and its host Compute Engine. These data can be used to monitor the health
of your Cloud TPU and Compute Engine.
Metrics enable you to track a numerical quantity over time, for example, CPU
utilization, network usage, or MXU utilization. Logs capture events at a specific
point in time. Log entries are written by your own code, Google Cloud services,
third-party applications, and the Google Cloud infrastructure. You can also
generate metrics from the data present in a log entry by creating a
[log-based metric](https://cloud.google.com/logging/docs/logs-based-metrics).
You can also set [alert policies](https://cloud.google.com/monitoring/alerts)
based on metric values or log entries.
This guide discusses Google Cloud Monitoring and shows you how to:
- View Cloud TPU metrics
- Set up Cloud TPU metrics alert policies
- Query Cloud TPU logs
- Create log-based metrics for setting up alerts and visualizing dashboards.
Prerequisites
This document assumes some basic knowledge of
[Google Cloud Monitoring](https://cloud.google.com/monitoring).
You must have a Compute Engine VM and Cloud TPU
resources created before you can begin generating and working with [Google Cloud Monitoring](https://cloud.google.com/monitoring).
See the [Cloud TPU Quickstart](/tpu/docs/quickstart) for more details.
Metrics
Google Cloud metrics are automatically generated by Compute Engine VMs and the Cloud Cloud TPU runtime. The following metrics are generated by Cloud TPU Nodes:
cpu/utilization
memory/usage
network/received_bytes_count
network/sent_bytes_count
tpu/mxu/utilization
tpu/tensorcore/idle_duration
CPU utilization
THe
cpu/utilization metric tracks the current CPU utilization on the Cloud TPU worker,
represented as a percentage. Values are typically between 0.0 and 100.0, but
might exceed 100.0. Sampled every 60 seconds. It may take up to 180 seconds
between the time a value is generated and when it's displayed.
Memory usage
The
memory/usage metric tracks the memory currently being used by the Cloud TPU VM in
bytes. This metric is sampled every 60 seconds. It may take up to 180 seconds
between the time a value is generated and when it's displayed.
Network received bytes count
The
network/received_bytes_count metric tracks the number of cumulative bytes of
data the Cloud TPU VM received over the network at a point in time. It may take up to
180 seconds between the time a value is generated and when it's displayed.
Network sent bytes count
The
network/sent_bytes_count metric tracks the number of cumulative bytes the
Cloud TPU VM sent over the network at a point in time. It may take up to 180 seconds
between the time a value is generated and when it's displayed.
TensorCore idle duration
The
tpu/tensorcore/idle_duration metric tracks the number of seconds each TPU
chip's TensorCore has been idle. This metric is available for each chip on all
TPUs in use. If a TensorCore is in use, the idle duration value is reset to
zero. When the TensorCore is no longer in use, the idle duration value starts
to increase.
The following graph shows the
tpu/tensorcore/idle_duration metric for a v2-8
Cloud TPU VM which has one worker. Each worker has four chips. In this example, all four
chips have the same values for
tpu/tensorcore/idle_duration, so the graphs are
superimposed on each other.
MXU utilization
The
tpu/mxu/utilization metric tracks the current MXU utilization on the TPU
worker, represented as a percentage. Values are typically numbers between 0.0
and 100.0. Sampled every 60 seconds. After sampling, data is not visible for up
to 180 seconds.
For a complete list of metrics generated by Cloud TPU, see
[Cloud TPU metrics](https://cloud.google.com/monitoring/api/metrics_gcp#gcp-tpu).
Viewing metrics
You can view metrics using the
[Metrics Explorer](https://cloud.google.com/monitoring/charts/metrics-explorer)
in the Google Cloud console.
In the Metrics Explorer, click SELECT A METRIC
and search for
Cloud TPU Worker. If Show only active resources and metrics is
on, only metrics that are currently being generated will be displayed. Click
Cloud TPU Worker to display the available metrics.
You can also access metrics using curl HTTP calls:
Use the Try it! button in the
[projects.timeSeries.query documentation](https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.timeSeries/query)
to retrieve the value for a metric within the specified timeframe.
- Fill in the name in the following format: projects/{project-name}
- Add a query to the Request body section. The following is a sample query
for retrieving the idle duration metric for the specified zone for the last 5
minutes
fetch tpu_worker | filter zone = 'us-central2-b' | metric tpu.googleapis.com/tpu/tensorcore/idle_duration | within 5m""
- Click Execute to retrieve the results of the HTTP POST message
The
[Monitoring Query Language reference](https://cloud.google.com/monitoring/mql/reference)
document has more information on how to customize this query.
You can create
[alert policies](https://cloud.google.com/monitoring/alerts)
that tell Google Cloud Monitoring to send an alert when a condition is met.
Creating alerts
The steps in this section show an example of how to add an alert policy for the TensorCore Idle Duration metric. Whenever this metric exceeds 24 hours, Cloud Monitoring sends an email to the registered email address.
[Go to the Monitoring console](https://console.cloud.google.com/monitoring)
- In the navigation pane click Alerting
- Click EDIT NOTIFICATION CHANNELS
- Under Email, click ADD NEW
- Type an email address, a display name, and click SAVE
- Click CREATE POLICY
- Click SELECT A METRIC, select Tensorcore Idle Duration and click APPLY
- Click NEXT and then Threshold
- For Alert trigger, select Any time series violates
- For Threshold Position, select Above threshold
- For Threshold Value, type
86400000
- Click NEXT
- Under Notification Channels select your email notification channel and click OK
- Type a name for the alert policy
- Click NEXT and then CREATE POLICY
When the TensorCore Idle Duration goes over 24 hours, an email is sent to the email address you specified.
Logging
Log entries are written by Google Cloud services, third party services, ML
frameworks or your code. You can view logs using the Logs Viewer or Logs API.
For more information about Google Cloud logging, see
[Google Cloud Logging](https://cloud.google.com/logging).
In the Logs Explorer, you can select a resource type:
- Cloud TPU Worker -> Zone -> Node ID
- Audited Resource -> Cloud TPU -> API (
google.cloud.tpu.v1.Tpu.CreateNode,
google.cloud.tpu.v1.Tpu.DeleteNode,
google.cloud.tpu.v1.Tpu.UpdateNode)
Cloud TPU Worker logs contain information about a specific Cloud TPU worker in a specific
zone, for example the amount of memory available on the Cloud TPU worker
(
system_available_memory_GiB).
Audited Resource logs contain information about when a specific Cloud TPU API
was called and who made the call. For example
CreateNode,
UpdateNode, and
DeleteNode.
ML frameworks can generate logs to
stdout and
stderr. These logs are
controlled by environment variables and are read by your training script.
Your code can write logs to Google Cloud Logging. For more information, see
[Write standard logs](https://cloud.google.com/logging/docs/samples/logging-stdlogging) and
[Write structured logs](https://cloud.google.com/logging/docs/samples/logging-write-log-entry).
Viewing Cloud TPU logs
- Go to the Google Cloud Logs Viewer
- Click the Resource drop-down
- Click Cloud TPU Worker
- Select a zone
- Select the Cloud TPU you're interested in
- Click Apply. Logs are displayed in the query results
To view Audited Resource logs:
- Go to the Google Cloud Logs Viewer
- Click the Resource drop-down
- Click Audited Resource and then Cloud TPU
- Choose the Cloud TPU API you're interested in
- Click Apply. Logs are displayed in the query results
- Choose the APIs that begin with
google.cloud.tpu.v1.Tpu
Query Google Cloud Logs
When you view logs in the Google Cloud console, the page performs a default query.
You can view the query by selecting the
Show query toggle switch. You can
modify the default query or create a new one. For more information, see
[Build Queries in the Logs Explorer](/logging/docs/view/building-queries).
Understanding the log output for Audited Resource logs
Click any log entry to expand it, and you will find a field called
protoPayload.
Expand
protoPayload and you will see a number of subfields:
- logName: the name of the log
- protoPayload -> @type: the type of the log
- resourceName: the name of your Cloud TPU
- methodName: the name of the method called (audit logs only)
- request -> @type: the request type
- request -> node: details about the Cloud TPU node
- request -> node_id: the name of the TPU
- severity: the severity of the log
Understanding the log output for Cloud TPU Worker logs
Click any log entry to expand it, and you will find a field called
jsonPayload.
Expand
jsonPayload and you will see a number of subfields:
- accelerator_type: the accelerator type
- consumer_project: the project where the Cloud TPU lives
- evententry_timestamp: the time when the log was generated
- system_available_memory_GiB: the available memory on the Cloud TPU worker (0~350 GB)
Creating log-based metrics
This section describes how to create
[log-based metrics](/logging/docs/logs-based-metrics)
used for setting up monitoring dashboards and alerts. For information about
programmatically creating log-based metrics, see
[Creating log-based metrics programmatically using the Cloud Logging REST API](#rest_api).
The following example uses the system_available_memory_GiB subfield to demonstrate how to create a log-based metric for monitoring Cloud TPU worker available memory.
- Navigate to the Logs Explorer
In the query box, enter the following query to extract all log entries that have system_available_memory_GiB defined for the primary Cloud TPU worker:
resource.type=tpu_worker resource.labels.project_id=your-project resource.labels.zone=your-tpu-zone resource.labels.node_id=your-tpu-name resource.labels.worker_id=0 logName=projects/your-project/logs/tpu.googleapis.com%2Fruntime_monitor jsonPayload.system_available_memory_GiB:*
Click Create metric to display the Metric Editor
Under Metric Type, choose Distribution
Type a name, optional description, and unit of measurement for your metric. enter ""matrix_unit_utilization_percent"" and ""MXU utilization"" in the Name and Description fields, respectively
The filter is pre-populated with the script that you entered in the Logs Explorer
Click CREATE METRIC
Click Explore Metrics to view your new metric. It may take a few minutes before your metrics are displayed
Creating log-based metrics programmatically using the Cloud Logging REST API
You can also create log-based metrics through the
[Cloud Logging API](/logging/docs/reference/v2/rest).
For more information, see [Creating a distribution metric](/logging/docs/logs-based-metrics/distribution-metrics).
Creating dashboards and alerts using log-based metrics
Dashboards are useful for visualizing metrics (expect ~2 minute delay); alerts
are helpful for sending notifications when errors occur. For more information,
see
[Manage custom dashboards](https://cloud.google.com/monitoring/charts/dashboards)
and [Create metric-based alert policies](https://cloud.google.com/monitoring/alerts/using-alerting-ui).
Creating dashboards
To create a dashboard in Cloud Monitoring for the Tensorcore idle duration metric:
[Go to the Monitoring console](https://console.cloud.google.com/monitoring)
- In the navigation pane, click Dashboards
- Click CREATE DASHBOARD and then Add Chart
- Choose the chart that type you want to add. For this example, choose Line
- Type a title for the dashboard
- Click the button underneath Resource & Metric
- Scroll down the list of resources/metrics and select Cloud TPU Worker -> Tpu -> Tensorcore idle duration
- Click Apply
- To filter the dashboard contents, click CREATE DASHBOARD FILTERS
- In the Label field, set project_id to your project
- Click ADD and set zone to the zone where you created your TPU
- Add another filter for node_id and specify your Cloud TPU name",Monitoring Cloud TPU Nodes | Google Cloud,
id,url,body,title,description
55,https://cloud.google.com/tpu/docs/reference/rest/v2/projects.locations.nodes/delete,"Method: projects.locations.nodes.delete
Stay organized with collections
Save and categorize content based on your preferences.
HTTP request
DELETE https://tpu.googleapis.com/v2/{name=projects/*/locations/*/nodes/*}
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
string
Required. The resource name.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[Operation](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse#Operation)
Authorization scopes
Requires one of the following OAuth scopes:
-
https://www.googleapis.com/auth/cloud.tpu
-
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Method: projects.locations.nodes.delete | Cloud TPU | Google Cloud,
id,url,body,title,description
39,https://cloud.google.com/tpu/docs/reference/rest/v2alpha1/projects.locations.nodes/create,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Query parameters](#body.QUERY_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [Try it!](#try-it)
Creates a node.
HTTP request
POST https://tpu.googleapis.com/v2alpha1/{parent=projects/*/locations/*}/nodes
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
parent
|
Required. The parent resource name.
Query parameters
|Parameters
|
nodeId
|
The unqualified resource name.
|
requestId
|
Idempotent request UUID.
Request body
The request body contains an instance of
.
[Node](/tpu/docs/reference/rest/v2alpha1/projects.locations.nodes#Node)
Response body
If successful, the response body contains a newly created instance of
.
[Operation](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse#Operation)
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).",Method: projects.locations.nodes.create | Cloud TPU | Google Cloud,
id,url,body,title,description
28,https://cloud.google.com/tpu/docs/version-switching,"Switching software versions on your Cloud TPU
Overview
The software version of the framework running on your TPU must match the version
running on your local VM. This software version can now be
switched on a running Cloud TPU, without deleting and
recreating the TPU. This also enables configuring the
Cloud TPU with specific nightly versions of software frameworks.
It is still recommended to select a
[supported](https://cloud.google.com/tpu/docs/supported-versions)
version of these frameworks.
Usage
The recommended way to switch versions is to use the
[cloud-tpu-client](https://pypi.org/project/cloud-tpu-client/) python library.
Example usage for TensorFlow.
from cloud_tpu_client import Client
import argparse
parser = argparse.ArgumentParser()
parser.add_argument('--tpu-name',
type=str,
required=True,
help='Name of the TPU Instance')
parser.add_argument('--target-version',
type=str,
required=True,
help='target TPU Runtime version')
args = parser.parse_args()
c = Client(args.tpu_name)
c.configure_tpu_version(args.target_version, restart_type='ifNeeded')
c.wait_for_healthy()
This configures the Cloud TPU to match the TensorFlow version running on your local VM, this includes official releases as well as dated nightly builds.
The
restart_type parameter of the
configure_tpu_version API defines
the TPU restart behavior when switching versions. Options are
'always' (the default) and 'ifNeeded'.
'always' can be used to fix a TPU with, for example, status UNHEALTHY_TENSORFLOW, or that is returning Out of Memory (OOM) errors due to leaked resources from a previous run. When this option is set, the TPU is restarted even when a new framework version is not installed.
'ifNeeded' can be useful because it does not restart the runtime if it is already on the right version, so it will not add any significant startup time to a training script. When this option is set, the TPU is only restarted if it does not have the correct framework version installed.
The library communicates directly with the Cloud TPU so this code needs to be run in a VM in the same network. It is recommended to run this within the code for the rest of your model.
Additional software options
TensorFlow includes a
tf.__version__ string which is the simplest way
to configure the correct version. Other software options include:
- PyTorch -
pytorch-1.13,
pytorch-nightly-dev20220930,
pytorch-nightly
- Jax -
tpu_driver,
tpu_driver0.1-dev20200320,
tpu_driver_nightly
For example to configure a TPU to run with the latest nightly build of PyTorch.
from cloud_tpu_client import Client
c = Client()
c.configure_tpu_version('pytorch-nightly', restart_type='ifNeeded')
c.wait_for_healthy()",Switching software versions on your Cloud TPU | Google Cloud,
id,url,body,title,description
8,https://cloud.google.com/tpu/docs/reference/rest/v1alpha1/projects.locations.acceleratorTypes/list,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Query parameters](#body.QUERY_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [Try it!](#try-it)
Lists accelerator types supported by this API.
HTTP request
GET https://tpu.googleapis.com/v1alpha1/{parent=projects/*/locations/*}/acceleratorTypes
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
parent
|
Required. The parent resource name.
Query parameters
|Parameters
|
pageSize
|
The maximum number of items to return.
|
pageToken
|
The nextPageToken value returned from a previous List request, if any.
|
filter
|
List filter.
|
orderBy
|
Sort results.
Request body
The request body must be empty.
Response body
Response for
.
[acceleratorTypes.list](/tpu/docs/reference/rest/v1alpha1/projects.locations.acceleratorTypes/list#google.cloud.tpu.v1alpha1.Tpu.ListAcceleratorTypes)
If successful, the response body contains data with the following structure:
|JSON representation
|
{
""acceleratorTypes"": [
{
object (
|Fields
|
acceleratorTypes[]
|
The listed nodes.
|
nextPageToken
|
The next page token or empty if none.
|
unreachable[]
|
Locations that could not be reached.
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).",Method: projects.locations.acceleratorTypes.list | Cloud TPU | Google Cloud,
id,url,body,title,description
43,https://cloud.google.com/tpu/docs/reference/rest/v1/projects.locations.nodes/delete,"Method: projects.locations.nodes.delete
Stay organized with collections
Save and categorize content based on your preferences.
HTTP request
DELETE https://tpu.googleapis.com/v1/{name=projects/*/locations/*/nodes/*}
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
string
Required. The resource name.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[Operation](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse#Operation)
Authorization scopes
Requires one of the following OAuth scopes:
-
https://www.googleapis.com/auth/cloud.tpu
-
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Method: projects.locations.nodes.delete | Cloud TPU | Google Cloud,
id,url,body,title,description
3,https://cloud.google.com/tpu/docs/reference/rest/v2/projects.locations.runtimeVersions/list,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Query parameters](#body.QUERY_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [Try it!](#try-it)
Lists runtime versions supported by this API.
HTTP request
GET https://tpu.googleapis.com/v2/{parent=projects/*/locations/*}/runtimeVersions
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
parent
|
Required. The parent resource name.
Query parameters
|Parameters
|
pageSize
|
The maximum number of items to return.
|
pageToken
|
The nextPageToken value returned from a previous List request, if any.
|
filter
|
List filter.
|
orderBy
|
Sort results.
Request body
The request body must be empty.
Response body
Response for
.
[runtimeVersions.list](/tpu/docs/reference/rest/v2/projects.locations.runtimeVersions/list#google.cloud.tpu.v2.Tpu.ListRuntimeVersions)
If successful, the response body contains data with the following structure:
|JSON representation
|
{
""runtimeVersions"": [
{
object (
|Fields
|
runtimeVersions[]
|
The listed nodes.
|
nextPageToken
|
The next page token or empty if none.
|
unreachable[]
|
Locations that could not be reached.
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).",Method: projects.locations.runtimeVersions.list | Cloud TPU | Google Cloud,
id,url,body,title,description
78,https://cloud.google.com/tpu/docs/reference/rest/v2alpha1/projects.locations.nodes/get,"Method: projects.locations.nodes.get
Stay organized with collections
Save and categorize content based on your preferences.
Gets the details of a node.
HTTP request
GET https://tpu.googleapis.com/v2alpha1/{name=projects/*/locations/*/nodes/*}
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
string
Required. The resource name.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[Node](/tpu/docs/reference/rest/v2alpha1/projects.locations.nodes#Node)
Authorization scopes
Requires one of the following OAuth scopes:
-
https://www.googleapis.com/auth/cloud.tpu
-
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Method: projects.locations.nodes.get | Cloud TPU | Google Cloud,
id,url,body,title,description
35,https://cloud.google.com/tpu/docs/reference/rest/v2alpha1/projects.locations.nodes,"[Resource: Node](#Node) [JSON representation](#Node.SCHEMA_REPRESENTATION) [State](#Node.State) [NetworkConfig](#Node.NetworkConfig) [ServiceAccount](#Node.ServiceAccount) [SchedulingConfig](#Node.SchedulingConfig) [NetworkEndpoint](#Node.NetworkEndpoint) [AccessConfig](#Node.AccessConfig) [Health](#Node.Health) [AttachedDisk](#Node.AttachedDisk) [DiskMode](#Node.DiskMode) [ApiVersion](#Node.ApiVersion) [Symptom](#Node.Symptom) [SymptomType](#Node.SymptomType) [ShieldedInstanceConfig](#Node.ShieldedInstanceConfig) [BootDiskConfig](#Node.BootDiskConfig) [CustomerEncryptionKey](#Node.CustomerEncryptionKey)
-
[Methods](#METHODS_SUMMARY)
Resource: Node
A TPU instance.
|JSON representation
|
{ ""name"": string, ""description"": string, ""acceleratorType"": string, ""state"": enum (
|Fields
|
name
|
Output only. Immutable. The name of the TPU.
|
description
|
The user-supplied description of the TPU. Maximum of 512 characters.
|
acceleratorType
|
The type of hardware accelerators associated with this node.
|
state
|
Output only. The current state for the TPU Node.
|
healthDescription
|
Output only. If this field is populated, it contains a description of why the TPU Node is unhealthy.
|
runtimeVersion
|
Required. The runtime version running in the Node.
|
networkConfig
|
Network configurations for the TPU node.
|
cidrBlock
|
The CIDR block that the TPU node will use when selecting an IP address. This CIDR block must be a /29 block; the Compute Engine networks API forbids a smaller block, and using a larger block would be wasteful (a node can only consume one IP address). Errors will occur if the CIDR block has already been used for a currently existing TPU node, the CIDR block conflicts with any subnetworks in the user's provided network, or the provided network is peered with another network that is using that CIDR block.
|
serviceAccount
|
The Google Cloud Platform Service Account to be used by the TPU node VMs. If None is specified, the default compute service account will be used.
|
createTime
|
Output only. The time when the node was created.
A timestamp in RFC3339 UTC ""Zulu"" format, with nanosecond resolution and up to nine fractional digits. Examples:
|
schedulingConfig
|
The scheduling options for this node.
|
networkEndpoints[]
|
Output only. The network endpoints where TPU workers can be accessed and sent work. It is recommended that runtime clients of the node reach out to the 0th entry in this map first.
|
health
|
The health status of the TPU node.
|
labels
|
Resource labels to represent user-provided metadata.
An object containing a list of
|
metadata
|
Custom metadata to apply to the TPU Node. Can set startup-script and shutdown-script
An object containing a list of
|
tags[]
|
Tags to apply to the TPU Node. Tags are used to identify valid sources or targets for network firewalls.
|
id
|
Output only. The unique identifier for the TPU Node.
|
dataDisks[]
|
The additional data disks for the Node.
|
apiVersion
|
Output only. The API version that created this Node.
|
symptoms[]
|
Output only. The Symptoms that have occurred to the TPU Node.
|
queuedResource
|
Output only. The qualified name of the QueuedResource that requested this Node.
|
acceleratorConfig
|
The AccleratorConfig for the TPU Node.
|
shieldedInstanceConfig
|
Shielded Instance options.
|
multisliceNode
|
Output only. Whether the Node belongs to a Multislice group.
|
autocheckpointEnabled
|
Optional. Whether Autocheckpoint is enabled.
|
bootDiskConfig
|
Optional. Boot disk configuration.
State
Represents the different states of a TPU node during its lifecycle.
|Enums
|
STATE_UNSPECIFIED
|TPU node state is not known/set.
|
CREATING
|TPU node is being created.
|
READY
|TPU node has been created.
|
RESTARTING
|TPU node is restarting.
|
REIMAGING
|TPU node is undergoing reimaging.
|
DELETING
|TPU node is being deleted.
|
REPAIRING
|TPU node is being repaired and may be unusable. Details can be found in the 'help_description' field.
|
STOPPED
|TPU node is stopped.
|
STOPPING
|TPU node is currently stopping.
|
STARTING
|TPU node is currently starting.
|
PREEMPTED
|TPU node has been preempted. Only applies to Preemptible TPU Nodes.
|
TERMINATED
|TPU node has been terminated due to maintenance or has reached the end of its life cycle (for preemptible nodes).
|
HIDING
|TPU node is currently hiding.
|
HIDDEN
|TPU node has been hidden.
|
UNHIDING
|TPU node is currently unhiding.
NetworkConfig
Network related configurations.
|JSON representation
|
{ ""network"": string, ""subnetwork"": string, ""enableExternalIps"": boolean, ""canIpForward"": boolean, ""queueCount"": integer }
|Fields
|
network
|
The name of the network for the TPU node. It must be a preexisting Google Compute Engine network. If none is provided, ""default"" will be used.
|
subnetwork
|
The name of the subnetwork for the TPU node. It must be a preexisting Google Compute Engine subnetwork. If none is provided, ""default"" will be used.
|
enableExternalIps
|
Indicates that external IP addresses would be associated with the TPU workers. If set to false, the specified subnetwork or network should have Private Google Access enabled.
|
canIpForward
|
Allows the TPU node to send and receive packets with non-matching destination or source IPs. This is required if you plan to use the TPU workers to forward routes.
|
queueCount
|
Optional. Specifies networking queue count for TPU VM instance's network interface.
ServiceAccount
A service account.
|JSON representation
|
{ ""email"": string, ""scope"": [ string ] }
|Fields
|
email
|
Email address of the service account. If empty, default Compute service account will be used.
|
scope[]
|
The list of scopes to be made available for this service account. If empty, access to all Cloud APIs will be allowed.
SchedulingConfig
Sets the scheduling options for this node.
|JSON representation
|
{ ""preemptible"": boolean, ""reserved"": boolean }
|Fields
|
preemptible
|
Defines whether the node is preemptible.
|
reserved
|
Whether the node is created under a reservation.
NetworkEndpoint
A network endpoint over which a TPU worker can be reached.
|JSON representation
|
{
""ipAddress"": string,
""port"": integer,
""accessConfig"": {
object (
|Fields
|
ipAddress
|
The internal IP address of this network endpoint.
|
port
|
The port of this network endpoint.
|
accessConfig
|
The access config for the TPU worker.
AccessConfig
An access config attached to the TPU worker.
|JSON representation
|
{ ""externalIp"": string }
|Fields
|
externalIp
|
Output only. An external IP address associated with the TPU worker.
Health
Health defines the status of a TPU node as reported by Health Monitor.
|Enums
|
HEALTH_UNSPECIFIED
|Health status is unknown: not initialized or failed to retrieve.
|
HEALTHY
|The resource is healthy.
|
TIMEOUT
|The resource is unresponsive.
|
UNHEALTHY_TENSORFLOW
|The in-guest ML stack is unhealthy.
|
UNHEALTHY_MAINTENANCE
|The node is under maintenance/priority boost caused rescheduling and will resume running once rescheduled.
AttachedDisk
A node-attached disk resource. Next ID: 8;
|JSON representation
|
{
""sourceDisk"": string,
""mode"": enum (
|Fields
|
sourceDisk
|
Specifies the full path to an existing disk. For example: ""projects/my-project/zones/us-central1-c/disks/my-disk"".
|
mode
|
The mode in which to attach this disk. If not specified, the default is READ_WRITE mode. Only applicable to dataDisks.
DiskMode
The different mode of the attached disk.
|Enums
|
DISK_MODE_UNSPECIFIED
|The disk mode is not known/set.
|
READ_WRITE
|Attaches the disk in read-write mode. Only one TPU node can attach a disk in read-write mode at a time.
|
READ_ONLY
|Attaches the disk in read-only mode. Multiple TPU nodes can attach a disk in read-only mode at a time.
ApiVersion
TPU API Version.
|Enums
|
API_VERSION_UNSPECIFIED
|API version is unknown.
|
V1_ALPHA1
|TPU API V1Alpha1 version.
|
V1
|TPU API V1 version.
|
V2_ALPHA1
|TPU API V2Alpha1 version.
Symptom
A Symptom instance.
|JSON representation
|
{
""createTime"": string,
""symptomType"": enum (
|Fields
|
createTime
|
Timestamp when the Symptom is created.
A timestamp in RFC3339 UTC ""Zulu"" format, with nanosecond resolution and up to nine fractional digits. Examples:
|
symptomType
|
Type of the Symptom.
|
details
|
Detailed information of the current Symptom.
|
workerId
|
A string used to uniquely distinguish a worker within a TPU node.
SymptomType
SymptomType represents the different types of Symptoms that a TPU can be at.
|Enums
|
SYMPTOM_TYPE_UNSPECIFIED
|Unspecified symptom.
|
LOW_MEMORY
|TPU VM memory is low.
|
OUT_OF_MEMORY
|TPU runtime is out of memory.
|
EXECUTE_TIMED_OUT
|TPU runtime execution has timed out.
|
MESH_BUILD_FAIL
|TPU runtime fails to construct a mesh that recognizes each TPU device's neighbors.
|
HBM_OUT_OF_MEMORY
|TPU HBM is out of memory.
|
PROJECT_ABUSE
|Abusive behaviors have been identified on the current project.
ShieldedInstanceConfig
A set of Shielded Instance options.
|JSON representation
|
{ ""enableSecureBoot"": boolean }
|Fields
|
enableSecureBoot
|
Defines whether the instance has Secure Boot enabled.
BootDiskConfig
Boot disk configurations.
|JSON representation
|
{
""customerEncryptionKey"": {
object (
|Fields
|
customerEncryptionKey
|
Optional. Customer encryption key for boot disk.
|
enableConfidentialCompute
|
Optional. Whether the boot disk will be created with confidential compute mode.
CustomerEncryptionKey
Customer's encryption key.
|JSON representation
|
{ // Union field
|Fields
|
Union field
|
kmsKeyName
|
The name of the encryption key that is stored in Google Cloud KMS. For example:
""kmsKeyName"": ""projects/ kms_project_id/locations/ region/keyRings/ key_region/cryptoKeys/key
The fully-qualifed key name may be returned for resource GET requests. For example:
""kmsKeyName"": ""projects/ kms_project_id/locations/ region/keyRings/ key_region/cryptoKeys/key /cryptoKeyVersions/1
|
Methods
|
|Creates a node.
|
|Deletes a node.
|
|Gets the details of a node.
|
|Retrieves the guest attributes for the node.
|
|Lists nodes.
|
|Updates the configurations of a node.
|
|Simulates a maintenance event.
|
|Starts a node.
|
|Stops a node.",REST Resource: projects.locations.nodes | Cloud TPU | Google Cloud,
id,url,body,title,description
37,https://cloud.google.com/tpu/docs/reference/rest/Shared.Types/ListOperationsRequest,"ListOperationsRequest
Stay organized with collections
Save and categorize content based on your preferences.
|JSON representation
|
{
""name"": string,
""filter"": string,
""pageSize"": integer,
""pageToken"": string
}
|Fields
|
name
|
string
The name of the operation's parent resource.
|
filter
|
string
The standard list filter.
|
pageSize
|
integer
The standard list page size.
|
pageToken
|
string
The standard list page token.
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2022-11-28 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",ListOperationsRequest | Cloud TPU | Google Cloud,
id,url,body,title,description
66,https://cloud.google.com/tpu/docs/troubleshooting/trouble-pytorch,"Troubleshooting PyTorch - TPU
This guide provides troubleshooting information to
help you identify and resolve problems you might encounter while training
PyTorch models on Cloud TPU. For a more general guide to
getting started with Cloud TPU, see the
[PyTorch quickstart](/tpu/docs/run-calculation-pytorch).
Troubleshooting slow training performance
If your model trains slowly,
[generate and review a metrics report.](https://github.com/pytorch/xla/blob/master/TROUBLESHOOTING.md#get-a-metrics-report)
To automatically analyze the metrics report and provide a summary, simply run your workload with PT_XLA_DEBUG=1.
For more information about issues that might cause your model to train slowly,
see
[Known performance caveats](https://github.com/pytorch/xla/blob/master/TROUBLESHOOTING.md#known-performance-caveats).
Performance profiling
To profile your workload in depth to discover bottlenecks, you can use the following resources:
[PyTorch/XLA performance profiling](https://cloud.google.com/tpu/docs/pytorch-xla-performance-profiling-tpu-vm) [PyTorch/XLA profiling Colab](https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/pytorch-xla-profiling-colab.ipynb) [Sample MNIST training script with profiling](https://github.com/pytorch/xla/blob/master/test/test_profile_mp_mnist.py)
More debugging tools
You can specify
[environment variables](https://github.com/pytorch/xla/blob/master/TROUBLESHOOTING.md#environment-variables)
to control the behavior of the PyTorch/XLA software stack.
If the PyTorch process stops responding, file a GitHub issue and include
[stack traces](https://github.com/pytorch/xla/blob/master/TROUBLESHOOTING.md#retrieving-stack-traces).
A
[debug_run.py utility](https://github.com/pytorch/xla/blob/master/TROUBLESHOOTING.md#using-debug_runpy-to-collect-debug-information)
is provided in scripts/debug_run.py which can be used to create a
tar.gz
archive with the information required to debug PyTorch/XLA executions.
Managing XLA tensors
[XLA tensor Quirks](https://github.com/pytorch/xla/blob/master/TROUBLESHOOTING.md#xla-tensor-quirks)
describes what you should and should not do when working with XLA tensors and
shared weights.",Troubleshooting PyTorch - TPU | Google Cloud,
id,url,body,title,description
88,https://cloud.google.com/tpu/docs/setup-gcp-account,"Set up the Cloud TPU environment
Before you can use Cloud TPU resources to train or run inference on models, you need to do the following set up steps:
[Set up your Google Cloud project](#set-up-project) [Set up your environment to use Cloud TPU](#set-up-env) [Prepare to request a Cloud TPU](#prepare-to-request)
Set up your Google Cloud project
You must have a Google Cloud account and project to use Cloud TPU.
In the
[Google Cloud console](https://console.cloud.google.com/), [sign in](https://accounts.google.com/Login)to your Google Account or [sign up for a new account](https://accounts.google.com/SignUp). [Install](/sdk/docs/install)the Google Cloud CLI. The Google Cloud CLI is an interface for accessing and managing Google Cloud resources and services.
Select or create a Google Cloud project:
In the
[Google Cloud console](https://console.cloud.google.com/), select or create a Cloud project from the project selector.
In the Cloud Shell, set your project ID using the gcloud CLI. The project ID is the name of your project shown in the Google Cloud console.
$ gcloud config set project PROJECT-ID
[Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled).
Billing setup is required for all Google Cloud usage. You will only be charged after you start using Google Cloud resources. For more information, see the
[Cloud Billing documentation](/billing/docs).
Billing for all Cloud TPU versions usage follows the standard regional pricing shown on the
[Cloud TPU pricing page](/tpu/pricing).
Set up your environment to use Cloud TPU
Before requesting a Cloud TPU, you must activate the Cloud TPU API and ensure that you have permissions to manage access in your project and to create a Cloud TPU. It is also recommended that you create a user-managed service account to attach to your TPU.
Activate the Cloud TPU API from the Google Cloud console or using the gcloud CLI in the Cloud Shell:
gcloud
$ gcloud services enable tpu.googleapis.com
Console
- In the Google Cloud console, go to the Cloud TPU API page.
- Click Enable.
Make sure you have the following roles on your project:
[Service Account Admin](/iam/docs/understanding-roles#iam.serviceAccountAdmin): Needed to create a service account [Project IAM Admin](/iam/docs/understanding-roles#resourcemanager.projectIamAdmin): Needed to grant a role in a project [TPU Admin](/iam/docs/understanding-roles#tpu.admin): Needed to create a TPU
Follow the instructions in
[View current access](/iam/docs/granting-changing-revoking-access#view-access)to view who has access to your project, folder, or organization. To view your own access, in the Principal column, find the row that has your email address. If your email address isn't in that column, then you don't have any roles. In the Role column for the row with your email address, check whether the list of roles includes the required roles.
If you don't have a required role,
[grant the role](/iam/docs/granting-changing-revoking-access#grant-single-role)or ask an administrator to do so.
-
Create a TPU service account:
Service accounts allow the Cloud TPU service to access other Google Cloud services. A user-managed service account is a recommended Google Cloud practice. Attach a service account to your TPU when you create it using the
--service-accountflag.
Follow the instructions in
[Create service accounts](/iam/docs/service-accounts-create)to create a service account.
Follow the instructions in
[Manage access to projects, folders, and organizations](/iam/docs/granting-changing-revoking-access)to grant your service account access to Google Cloud services that your TPU will access. The following roles are recommended so that your TPU to access commonly-used Google Cloud services. [TPU Admin](/iam/docs/understanding-roles#tpu.admin): Needed for full access to TPU resources [Storage Admin](/iam/docs/understanding-roles#storage.admin): Needed for accessing Cloud Storage [Logs Writer](/iam/docs/understanding-roles#logging.logWriter): Needed for writing logs with the Logging API [Monitoring Metric Writer](/iam/docs/understanding-roles#monitoring.metricWriter): Needed for writing metrics to Cloud Monitoring
-
-
Prepare to create a Cloud TPU
Before creating a Cloud TPU, you must request quota. You should also consider using queued resources, as well as which parameters you want to use to configure your TPU.
Request quota:
In order to create a Cloud TPU, your Google Cloud project must have quota for the version and size of TPU you want to create and the zone where you want to create it. For example, if you want to create a TPU v4-8 in
us-central2-b, you would request a quota of 8 TPU v4 cores in
us-central2-b. For more information about zones where Cloud TPU is available, see
[TPU regions and zones](/tpu/docs/regions-zones).
Quota is allocated differently depending on the TPU version. Different types of quota have different availability expectations. For more information about quota allocation, quota types, and how to request quota, see
[Quotas](/tpu/docs/quota).
Determine if you want to use
[queued resources](/tpu/docs/queued-resources).
Creating a Cloud TPU as a queued resource is a best practice. Queued resources allow you to receive capacity once it becomes available. You can specify an optional start and end time for when the request should be filled.
There are different gcloud CLI commands for working with queued resources. For more information, see
[Queued resources user guide](/tpu/docs/queued-resources).
Determine Cloud TPU creation parameters:
Zone: Set the
--zoneflag to the zone where you want to create a TPU. You must have quota allocated in this zone. For more information, see
[TPU regions and zones](/tpu/docs/regions-zones).
TPU configuration: If you don't need to specify a custom topology, or you are using TPU v2 or v3, set the
--accelerator-typeflag to
vVERSION-TENSORCORES. Replace VERSION with the TPU version number you want to use. Replace TENSORCORES with the number of TensorCores you want to use.
If you want to customize the physical topology of your TPU, use the
--versionand
--topologyflags. Set the
--versionflag to the TPU version you want to use. Set the
--topologyflag to the topology you want to use.
For more information about TPU configurations, including supported configurations and topology variants, see
[TPU configurations](/tpu/docs/supported-tpu-configurations).
Software version: If you are requesting a queued resource, set the
--runtime-versionflag to the name of the software version you want to use. Otherwise, use the
--versionflag. TPU software versions are available for TensorFlow, PyTorch, and JAX frameworks. For more information about supported software versions, see
[TPU VM software versions](/tpu/docs/supported-tpu-configurations#tpu-vm-versions).
Service account: Set
--service-accountto the email address of a service account, if you created one, to attach the service account to your TPU. If empty, the
[default Compute Engine service account](/compute/docs/access/service-accounts#default_service_account)will be used.
Quota type: If you want to create a TPU using reserved quota, add the
--reservedflag to your request.
If you want to create a TPU using preemptible quota, and you are requesting a queued resource, add the
--best-effortflag to your request.
If you want to create a TPU using preemptible quota, and you aren't requesting a queued resource, add the
--preemptibleflag to your request.
If you want to create a TPU using on-demand quota, you don't need to add any additional flags.
Advanced configuration: You can add additional flags to your request to configure your TPU. See the
[and the following sections in](/sdk/gcloud/reference/compute/tpus/tpu-vm/create)
gcloud compute tpus tpu-vm createdocumentation
[Manage TPUs](/tpu/docs/managing-tpus-tpu-vm)for more information:
-
For examples of how to create a Cloud TPU, see
[Get started](/tpu/docs/quick-starts).
What's next
- Learn how to create and manage
[VM and TPU resources](/tpu/docs/managing-tpus-tpu-vm)
- Run a
[Cloud TPU quickstart](/tpu/docs/quick-starts)",Set up the Cloud TPU environment | Google Cloud,
id,url,body,title,description
67,https://cloud.google.com/tpu/docs/reference/rest/v2alpha1/projects.locations.nodes/patch,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Query parameters](#body.QUERY_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [Try it!](#try-it)
Updates the configurations of a node.
HTTP request
PATCH https://tpu.googleapis.com/v2alpha1/{node.name=projects/*/locations/*/nodes/*}
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
node.name
|
Output only. Immutable. The name of the TPU.
Query parameters
|Parameters
|
updateMask
|
Required. Mask of fields from [Node][Tpu.Node] to update. Supported fields: [description, tags, labels, metadata, networkConfig.enable_external_ips].
This is a comma-separated list of fully qualified names of fields. Example:
Request body
The request body contains an instance of
.
[Node](/tpu/docs/reference/rest/v2alpha1/projects.locations.nodes#Node)
Response body
If successful, the response body contains an instance of
.
[Operation](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse#Operation)
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).",Method: projects.locations.nodes.patch | Cloud TPU | Google Cloud,
id,url,body,title,description
47,https://cloud.google.com/tpu/docs/reference/rest/v2/projects.locations.nodes/stop,"Method: projects.locations.nodes.stop
Stay organized with collections
Save and categorize content based on your preferences.
Stops a node. This operation is only available with single TPU nodes.
HTTP request
POST https://tpu.googleapis.com/v2/{name=projects/*/locations/*/nodes/*}:stop
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
string
Required. The resource name.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[Operation](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse#Operation)
Authorization scopes
Requires one of the following OAuth scopes:
-
https://www.googleapis.com/auth/cloud.tpu
-
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Method: projects.locations.nodes.stop | Cloud TPU | Google Cloud,
id,url,body,title,description
9,https://cloud.google.com/tpu/docs/reference/rest/v1alpha1/projects.locations.nodes/start,"Method: projects.locations.nodes.start
Stay organized with collections
Save and categorize content based on your preferences.
HTTP request
POST https://tpu.googleapis.com/v1alpha1/{name=projects/*/locations/*/nodes/*}:start
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
string
The resource name.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[Operation](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse#Operation)
Authorization scopes
Requires one of the following OAuth scopes:
-
https://www.googleapis.com/auth/cloud.tpu
-
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Method: projects.locations.nodes.start | Cloud TPU | Google Cloud,
id,url,body,title,description
31,https://cloud.google.com/tpu/docs/billing-questions,"Stay organized with collections
Save and categorize content based on your preferences.
Billing questions
Use the following resources to get help with billing questions:
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2024-01-31 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Billing questions | Cloud TPU | Google Cloud,
id,url,body,title,description
120,https://cloud.google.com/tpu/docs/reference/rest/v1/projects.locations.tensorflowVersions/list,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Query parameters](#body.QUERY_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [Try it!](#try-it)
List TensorFlow versions supported by this API.
HTTP request
GET https://tpu.googleapis.com/v1/{parent=projects/*/locations/*}/tensorflowVersions
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
parent
|
Required. The parent resource name.
Query parameters
|Parameters
|
pageSize
|
The maximum number of items to return.
|
pageToken
|
The nextPageToken value returned from a previous List request, if any.
|
filter
|
List filter.
|
orderBy
|
Sort results.
Request body
The request body must be empty.
Response body
Response for
.
[tensorflowVersions.list](/tpu/docs/reference/rest/v1/projects.locations.tensorflowVersions/list#google.cloud.tpu.v1.Tpu.ListTensorFlowVersions)
If successful, the response body contains data with the following structure:
|JSON representation
|
{
""tensorflowVersions"": [
{
object (
|Fields
|
tensorflowVersions[]
|
The listed nodes.
|
nextPageToken
|
The next page token or empty if none.
|
unreachable[]
|
Locations that could not be reached.
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).",Method: projects.locations.tensorflowVersions.list | Cloud TPU | Google Cloud,
id,url,body,title,description
63,https://cloud.google.com/tpu/docs/performance-guide,"Cloud TPU performance guide
Your first step when troubleshooting TPU performance is to profile your model.
For more information on capturing a performance profile, see
[Profiling your model on Cloud TPU](/tpu/docs/cloud-tpu-tools).
TPU model performance
This section describes general issues that can reduce model performance and how you can address them.
Model is input bound
TPUs perform calculations very fast. To ensure the TPU is not idle, it is important to make sure there is a steady stream of data being loaded onto the TPU. How this is done depends on how you load and preprocess your dataset. For example, you can read datafiles in parallel using
[tf.data.TFRecordset()](https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset)and the
num_parallel_readsparameter.
Batch size is too small because of sharding (splitting batches across cores)
The TPU runtime splits a batch across all 8 cores of a TPU device (for example v2-8 or v3-8). If you specify a global batch size of 128, each core receives a batch size of 16 (128 / 8).
For optimum memory usage, use the largest batch size that fits into TPU memory. Each TPU core uses two-dimensional 8 X 128 vector registers for processing matrix multiplications. In general, your batch size should be evenly divisible by 8 or 128.
XLA compiler optimizations
[XLA](https://www.tensorflow.org/performance/xla/) is a compiler for machine
learning that can produce binaries for TPUs, CPUs, GPUs and other platforms.
While XLA is part of the standard TensorFlow code base, it can also be used on
[PyTorch](/tpu/docs/run-calculation-pytorch) and [JAX](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html) models. Models
for Cloud TPU are translated to an XLA graph, which XLA then compiles to a TPU
executable. For more information about XLA, see [XLA: Optimizing Compiler for Machine Learning](https://www.tensorflow.org/xla).
Padding
To use TPU memory efficiently, structure your data so that it can be tiled into 128 x 8 chunks. When the data for a matrix computation does not fill an entire 128 x 8 chunk, the XLA compiler pads tensors. There are two drawbacks to padding:
- Padded tensors under-utilize the TPU core.
- Padding increases the amount of on-chip memory storage required for a tensor and can lead to an out-of-memory error.
While padding is automatically performed by the XLA compiler when necessary, you can determine the amount of padding performed using the memory viewer tool. You can avoid padding by picking tensor dimensions that are well suited for TPU.
Tensor dimensions
The XLA compiler rounds up the sizes of tensors stored in TPU HBM memory to perform computations more efficiently. This padding happens transparently at the hardware level and does not affect results. However, in certain cases the padding can result in significantly increased memory use and execution time.
The TPU runtime lays out tensors in memory to maximize computational efficiency and minimize padding. To minimize memory overhead and maximize computational efficiency, one of the following must be true:
The total batch size should be a multiple of 64 (8 per TPU core), and feature dimension sizes should be a multiple of 128.
The total batch size should be a multiple of 1024 (128 per TPU core), and feature dimension sizes should be a multiple of 8.
Using a batch size of 1024 and feature dimensions that are a multiple of 128 results in the best efficiency, although this may not be possible for all models.
Fusion
Fusion is a general technique the XLA compiler uses to optimize programs. A fused operation is the combination of multiple constituent operations that are to be executed in combination.
For example, consider the following series of operations:
tmp = tf.add(x, y)
result = tf.multiply(tmp, z)
This code is roughly equivalent to the following pseudo code:
for (i = 0; i < element_count; i++) {
tmp[i] = x[i] + y[i];
}
for (i = 0; i < element_count; i++) {
result = tmp[i] * z[i];
}
With fusion, the array accesses happen at the same time:
for (i = 0; i < element_count; i++) {
result = (x[i] + y[i]) * z[i];
}
In this example, the number of memory round trips is reduced and XLA does not need to allocate any space for 'tmp'.
Fusion is a critical optimization and benefits the Cloud TPU in several ways:
- It reduces memory transfers by removing the need to store intermediate results in main memory, which is slow.
- It allows greater utilization of hardware units which would otherwise be unutilized.
- It can reduce the memory utilization of a model as fewer buffers need to be live at the same time.
Broadcasting
Broadcasting implicitly occurs when two tensors with different, but compatible, shapes are combined.
For example,
tf.add(vector, matrix) requires the vector to be broadcasted to
the shape of the matrix. The result of the operation has the same shape as the
matrix. For more details, see the guide to
[broadcasting arrays](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html).
While broadcasts can often be fused with their consumers, forcing a broadcast may result in poor performance and increased memory usage.
In the following example, the broadcast implicit in the addition of a vector and matrix cannot be fused with the argmax resulting in a materialized broadcast:
`tf.argmax(tf.add(vector, zero_matrix), axis=0)`",Cloud TPU performance guide | Google Cloud,
id,url,body,title,description
42,https://cloud.google.com/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse,"The response message for
.
[Operations.ListOperations](/tpu/docs/reference/rest/v1alpha1/projects.locations.operations/list#google.longrunning.Operations.ListOperations)
|JSON representation
|
{
""operations"": [
{
object (
|Fields
|
operations[]
|
A list of operations that matches the specified filter in the request.
|
nextPageToken
|
The standard List next-page token.
Operation
This resource represents a long-running operation that is the result of a network API call.
|JSON representation
|
{ ""name"": string, ""metadata"": { ""@type"": string, field1: ..., ... }, ""done"": boolean, // Union field
|Fields
|
name
|
The server-assigned name, which is only unique within the same service that originally returns it. If you use the default HTTP mapping, the
|
metadata
|
Service-specific metadata associated with the operation. It typically contains progress information and common metadata such as create time. Some services might not provide such metadata. Any method that returns a long-running operation should document the metadata type, if any.
An object containing fields of an arbitrary type. An additional field
|
done
|
If the value is
|Union field
result. The operation result, which can be either an
error or a valid
response. If
done ==
false, neither
error nor
response is set. If
done ==
true, exactly one of
error or
response can be set. Some services might not provide the result.
result can be only one of the following:
|
error
|
The error result of the operation in case of failure or cancellation.
|
response
|
The normal, successful response of the operation. If the original method returns no data on success, such as
An object containing fields of an arbitrary type. An additional field
Status
The
Status type defines a logical error model that is suitable for different programming environments, including REST APIs and RPC APIs. It is used by
[gRPC](https://github.com/grpc). Each
Status message contains three pieces of data: error code, error message, and error details.
You can find out more about this error model and how to work with it in the
[API Design Guide](https://cloud.google.com/apis/design/errors).
|JSON representation
|
{ ""code"": integer, ""message"": string, ""details"": [ { ""@type"": string, field1: ..., ... } ] }
|Fields
|
code
|
The status code, which should be an enum value of
|
message
|
A developer-facing error message, which should be in English. Any user-facing error message should be localized and sent in the
|
details[]
|
A list of messages that carry the error details. There is a common set of message types for APIs to use.
An object containing fields of an arbitrary type. An additional field",ListOperationsResponse | Cloud TPU | Google Cloud,
id,url,body,title,description
71,https://cloud.google.com/tpu/docs/reference/rest/v2alpha1/projects.locations.queuedResources/list,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Query parameters](#body.QUERY_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [Try it!](#try-it)
Lists queued resources.
HTTP request
GET https://tpu.googleapis.com/v2alpha1/{parent=projects/*/locations/*}/queuedResources
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
parent
|
Required. The parent resource name.
Query parameters
|Parameters
|
pageSize
|
The maximum number of items to return.
|
pageToken
|
The nextPageToken value returned from a previous List request, if any.
Request body
The request body must be empty.
Response body
Response for
.
[queuedResources.list](/tpu/docs/reference/rest/v2alpha1/projects.locations.queuedResources/list#google.cloud.tpu.v2alpha1.Tpu.ListQueuedResources)
If successful, the response body contains data with the following structure:
|JSON representation
|
{
""queuedResources"": [
{
object (
|Fields
|
queuedResources[]
|
The listed queued resources.
|
nextPageToken
|
The next page token or empty if none.
|
unreachable[]
|
Locations that could not be reached.
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).",Method: projects.locations.queuedResources.list | Cloud TPU | Google Cloud,
id,url,body,title,description
32,https://cloud.google.com/tpu/docs/reference/rest/v2/projects.locations,"Resource: Location
A resource that represents a Google Cloud location.
|JSON representation
|
{ ""name"": string, ""locationId"": string, ""displayName"": string, ""labels"": { string: string, ... }, ""metadata"": { ""@type"": string, field1: ..., ... } }
|Fields
|
name
|
Resource name for the location, which may vary between implementations. For example:
|
locationId
|
The canonical id for this location. For example:
|
displayName
|
The friendly name for this location, typically a nearby city name. For example, ""Tokyo"".
|
labels
|
Cross-service attributes for the location. For example
An object containing a list of
|
metadata
|
Service-specific metadata. For example the available capacity at the given location.
An object containing fields of an arbitrary type. An additional field
|
Methods
|
|Generates the Cloud TPU service identity for the project.
|
|Gets information about a location.
|
|Lists information about the supported locations for this service.",REST Resource: projects.locations | Cloud TPU | Google Cloud,
id,url,body,title,description
46,https://cloud.google.com/tpu/docs/inception-v3-advanced,"Advanced Guide to Inception v3
This document discusses aspects of the
[Inception](https://github.com/tensorflow/tpu/tree/master/models/experimental/inception) model and how
they come together to make the model run efficiently on Cloud TPU.
It is an advanced view of the guide to running Inception v3 on
Cloud TPU. Specific changes to the model that led to significant
improvements are discussed in more detail. This document supplements the
[Inception v3 tutorial](/tpu/docs/tutorials/inception).
Inception v3 TPU training runs match accuracy curves produced by GPU jobs of similar configuration. The model has been successfully trained on v2-8, v2-128, and v2-512 configurations. The model has attained greater than 78.1% accuracy in about 170 epochs.
The code samples shown in this document are meant to be illustrative,
a high-level picture of what happens in the actual implementation.
Working code can be found on
[GitHub](https://github.com/tensorflow/tpu/tree/master/models/experimental/inception).
Introduction
Inception v3 is an image recognition model that has been shown to
attain greater than 78.1% accuracy on the ImageNet dataset. The model is the
culmination of many ideas developed by multiple researchers over the years. It
is based on the original paper:
[""Rethinking the Inception Architecture for Computer Vision""](https://arxiv.org/abs/1512.00567)
by Szegedy, et. al.
The model itself is made up of symmetric and asymmetric building blocks, including convolutions, average pooling, max pooling, concatenations, dropouts, and fully connected layers. Batch normalization is used extensively throughout the model and applied to activation inputs. Loss is computed using Softmax.
A high-level diagram of the model is shown in the following screenshot:
Estimator API
The TPU version of Inception v3 is written using
[TPUEstimator](https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/TPUEstimator),
an API designed to facilitate development, so that you can focus on the models
themselves rather than on the details of the underlying hardware. The API does
most of the low-level grunge work necessary for running models on TPUs behind
the scenes, while automating common functions, such as saving and restoring
checkpoints.
The Estimator API enforces separation of model and input portions of the code.
You define
model_fn and
input_fn functions, corresponding to the model
definition and input pipeline. The following code shows the declaration of these
functions:
def model_fn(features, labels, mode, params):

return tpu_estimator.TPUEstimatorSpec(mode=mode, loss=loss, train_op=train_op)
def input_fn(params):
def parser(serialized_example):

return image, label

images, labels = dataset.make_one_shot_iterator().get_next()
return images, labels
Two key functions provided by the API are
train() and
evaluate() used to
train and evaluate as shown in the following code:
def main(unused_argv):

run_config = tpu_config.RunConfig(
master=FLAGS.master,
model_dir=FLAGS.model_dir,
session_config=tf.ConfigProto(
allow_soft_placement=True, log_device_placement=True),
tpu_config=tpu_config.TPUConfig(FLAGS.iterations, FLAGS.num_shards),)
estimator = tpu_estimator.TPUEstimator(
model_fn=model_fn,
use_tpu=FLAGS.use_tpu,
train_batch_size=FLAGS.batch_size,
eval_batch_size=FLAGS.batch_size,
config=run_config)
estimator.train(input_fn=input_fn, max_steps=FLAGS.train_steps)
eval_results = inception_classifier.evaluate(
input_fn=imagenet_eval.input_fn, steps=eval_steps)
ImageNet dataset
Before the model can be used to recognize images, it must be trained using a
large set of labeled images.
[ImageNet](http://www.image-net.org) is a common
dataset to use.
ImageNet has over ten million URLs of labeled images. One million of the images also have bounding boxes specifying a more precise location for the labeled objects.
For this model, the ImageNet dataset is composed of 1,331,167 images which are split into training and evaluation datasets containing 1,281,167 and 50,000 images, respectively.
The training and evaluation datasets are kept separate intentionally. Only images from the training dataset are used to train the model and only images from the evaluation dataset are used to evaluate model accuracy.
The model expects images to be stored as TFRecords. For more information about
how to convert images from raw JPEG files into TFRecords, see
[. download_and_preprocess_imagenet.sh](/tpu/docs/imagenet-setup)
Input pipeline
Each Cloud TPU device has 8 cores and is connected to a host (CPU). Larger slices have multiple hosts. Other larger configurations interact with multiple hosts. For example a v2-256 communicates with 16 hosts.
Hosts retrieve data from the file system or local memory, do whatever data preprocessing is required, and then transfer preprocessed data to the TPU cores. We consider these three phases of data handling done by the host individually and refer to the phases as: 1) Storage, 2) Preprocessing, 3) Transfer. A high-level picture of the diagram is shown in the following figure:
To yield good performance, the system should be balanced. If the host CPU takes longer than the TPU to complete the three data handling phases, then execution will be host-bound. Both cases are shown in the following diagram:
The current implementation of Inception v3 is at the edge of being input-bound. Images are retrieved from the file system, decoded, and then preprocessed. Different types of preprocessing stages are available, ranging from moderate to complex. If we use the most complex of preprocessing stages, the training pipeline will be preprocessing bound. You can attain accuracy greater than 78.1% using a moderately complex preprocessing stage that keeps the model TPU-bound.
The model uses
[tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) to
handle input pipeline processing. For more information about how to optimize
input pipelines, see the [datasets performance guide](https://www.tensorflow.org/guide/data_performance).
Although you can define a function and pass it to the Estimator API, the class
InputPipeline encapsulates all required features.
The Estimator API makes it straightforward to use this class. You pass it to the
input_fn parameter of functions
train() and
evaluate(), as shown in the
following code snippet:
def main(unused_argv):

inception_classifier = tpu_estimator.TPUEstimator(
model_fn=inception_model_fn,
use_tpu=FLAGS.use_tpu,
config=run_config,
params=params,
train_batch_size=FLAGS.train_batch_size,
eval_batch_size=eval_batch_size,
batch_axis=(batch_axis, 0))

for cycle in range(FLAGS.train_steps // FLAGS.train_steps_per_eval):
tf.logging.info('Starting training cycle %d.' % cycle)
inception_classifier.train(
input_fn=InputPipeline(True), steps=FLAGS.train_steps_per_eval)
tf.logging.info('Starting evaluation cycle %d .' % cycle)
eval_results = inception_classifier.evaluate(
input_fn=InputPipeline(False), steps=eval_steps, hooks=eval_hooks)
tf.logging.info('Evaluation results: %s' % eval_results)
The main elements of
InputPipeline are shown in the following code snippet.
class InputPipeline(object):
def __init__(self, is_training):
self.is_training = is_training
def __call__(self, params):
# Storage
file_pattern = os.path.join(
FLAGS.data_dir, 'train-*' if self.is_training else 'validation-*')
dataset = tf.data.Dataset.list_files(file_pattern)
if self.is_training and FLAGS.initial_shuffle_buffer_size > 0:
dataset = dataset.shuffle(
buffer_size=FLAGS.initial_shuffle_buffer_size)
if self.is_training:
dataset = dataset.repeat()
def prefetch_dataset(filename):
dataset = tf.data.TFRecordDataset(
filename, buffer_size=FLAGS.prefetch_dataset_buffer_size)
return dataset
dataset = dataset.apply(
tf.contrib.data.parallel_interleave(
prefetch_dataset,
cycle_length=FLAGS.num_files_infeed,
sloppy=True))
if FLAGS.followup_shuffle_buffer_size > 0:
dataset = dataset.shuffle(
buffer_size=FLAGS.followup_shuffle_buffer_size)
# Preprocessing
dataset = dataset.map(
self.dataset_parser,
num_parallel_calls=FLAGS.num_parallel_calls)
dataset = dataset.prefetch(batch_size)
dataset = dataset.apply(
tf.contrib.data.batch_and_drop_remainder(batch_size))
dataset = dataset.prefetch(2) # Prefetch overlaps in-feed with training
images, labels = dataset.make_one_shot_iterator().get_next()
# Transfer
return images, labels
The storage section begins with the creation of a dataset and includes the
reading of TFRecords from storage (using
tf.data.TFRecordDataset). Special
purpose functions
repeat() and
shuffle() are used as needed. Function
tf.contrib.data.parallel_interleave() maps function
prefetch_dataset()
across its input to produce nested datasets, and outputs their elements
interleaved. It gets elements from
cycle_length nested datasets in parallel,
which increases throughput. The
sloppy argument relaxes the requirement that
the outputs be produced in a deterministic order, and allows the implementation
to skip over nested datasets whose elements are not readily available when
requested.
The preprocessing section calls
dataset.map(parser) which in turn calls
the parser function where images are preprocessed. The details of the
preprocessing stage are discussed in the next section.
The transfer section (at the end of the function) includes the line
return images, labels. TPUEstimator takes the returned values and
automatically transfers them to the device.
The following figure shows a sample Cloud TPU performance trace of Inception v3. TPU compute time, ignoring any in-feeding stalls, is approximately 815 msecs.
Host storage is written to the trace and shown in the following screenshot:
Host preprocessing, which includes image decoding and a series of image distortion functions is shown in the following screenshot:
Host/TPU transfer is shown in the following screenshot:
Preprocessing Stage
Image preprocessing is a crucial part of the system and can influence the maximum accuracy that the model attains during training. At a minimum, images need to be decoded and resized to fit the model. For Inception, images need to be 299x299x3 pixels.
However, simply decoding and resizing are not enough to get good accuracy. The ImageNet training dataset contains 1,281,167 images. One pass over the set of training images is referred to as an epoch. During training, the model requires several passes through the training dataset to improve its image recognition capabilities. To train Inception v3 to sufficient accuracy, use between 140 and 200 epochs depending on the global batch size.
It is useful to continuously alter the images before feeding them to the model so that a particular image is slightly different at every epoch. How to best do this preprocessing of images is as much art as it is science. A well-designed preprocessing stage can significantly boost the recognition capabilities of a model. Too simple a preprocessing stage may create an artificial ceiling on the accuracy that the same model can attain during training.
Inception v3 offers options for the preprocessing stage, ranging from relatively simple and computationally inexpensive to fairly complex and computationally expensive. Two distinct flavors of such can be found in files vgg_preprocessing.py and inception_preprocessing.py.
File vgg_preprocessing.py defines a preprocessing stage that has been used
successfully to train
resnet to 75% accuracy, but yields suboptimal
results when applied on Inception v3.
File inception_preprocessing.py contains a preprocessing stage that has been used to train Inception v3 with accuracies between 78.1 and 78.5% when run on TPUs.
Preprocessing differs depending on whether the model is undergoing training or being used for inference/evaluation.
At evaluation time, preprocessing is straightforward: crop a central region of the image and then resize it to the default 299x299 size. The following code snippet shows a preprocessing implementation:
def preprocess_for_eval(image, height, width, central_fraction=0.875):
with tf.name_scope(scope, 'eval_image', [image, height, width]):
if image.dtype != tf.float32:
image = tf.image.convert_image_dtype(image, dtype=tf.float32)
image = tf.image.central_crop(image, central_fraction=central_fraction)
image = tf.expand_dims(image, 0)
image = tf.image.resize_bilinear(image, [height, width], align_corners=False)
image = tf.squeeze(image, [0])
image = tf.subtract(image, 0.5)
image = tf.multiply(image, 2.0)
image.set_shape([height, width, 3])
return image
During training, the cropping is randomized: a bounding box is chosen randomly to select a region of the image which is then resized. The resized image is then optionally flipped and its colors are distorted. The following code snippet shows an implementation of these operations:
def preprocess_for_train(image, height, width, bbox, fast_mode=True, scope=None):
with tf.name_scope(scope, 'distort_image', [image, height, width, bbox]):
if bbox is None:
bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])
if image.dtype != tf.float32:
image = tf.image.convert_image_dtype(image, dtype=tf.float32)
distorted_image, distorted_bbox = distorted_bounding_box_crop(image, bbox)
distorted_image.set_shape([None, None, 3])
num_resize_cases = 1 if fast_mode else 4
distorted_image = apply_with_random_selector(
distorted_image,
lambda x, method: tf.image.resize_images(x, [height, width], method),
num_cases=num_resize_cases)
distorted_image = tf.image.random_flip_left_right(distorted_image)
if FLAGS.use_fast_color_distort:
distorted_image = distort_color_fast(distorted_image)
else:
num_distort_cases = 1 if fast_mode else 4
distorted_image = apply_with_random_selector(
distorted_image,
lambda x, ordering: distort_color(x, ordering, fast_mode),
num_cases=num_distort_cases)
distorted_image = tf.subtract(distorted_image, 0.5)
distorted_image = tf.multiply(distorted_image, 2.0)
return distorted_image
Function
distort_color is in charge of color alteration. It offers a fast mode
where only brightness and saturation are modified. The full mode modifies
brightness, saturation, and hue, in a random order.
def distort_color(image, color_ordering=0, fast_mode=True, scope=None):
with tf.name_scope(scope, 'distort_color', [image]):
if fast_mode:
if color_ordering == 0:
image = tf.image.random_brightness(image, max_delta=32. / 255.)
image = tf.image.random_saturation(image, lower=0.5, upper=1.5)
else:
image = tf.image.random_saturation(image, lower=0.5, upper=1.5)
image = tf.image.random_brightness(image, max_delta=32. / 255.)
else:
if color_ordering == 0:
image = tf.image.random_brightness(image, max_delta=32. / 255.)
image = tf.image.random_saturation(image, lower=0.5, upper=1.5)
image = tf.image.random_hue(image, max_delta=0.2)
image = tf.image.random_contrast(image, lower=0.5, upper=1.5)
elif color_ordering == 1:
image = tf.image.random_saturation(image, lower=0.5, upper=1.5)
image = tf.image.random_brightness(image, max_delta=32. / 255.)
image = tf.image.random_contrast(image, lower=0.5, upper=1.5)
image = tf.image.random_hue(image, max_delta=0.2)
elif color_ordering == 2:
image = tf.image.random_contrast(image, lower=0.5, upper=1.5)
image = tf.image.random_hue(image, max_delta=0.2)
image = tf.image.random_brightness(image, max_delta=32. / 255.)
image = tf.image.random_saturation(image, lower=0.5, upper=1.5)
elif color_ordering == 3:
image = tf.image.random_hue(image, max_delta=0.2)
image = tf.image.random_saturation(image, lower=0.5, upper=1.5)
image = tf.image.random_contrast(image, lower=0.5, upper=1.5)
image = tf.image.random_brightness(image, max_delta=32. / 255.)
return tf.clip_by_value(image, 0.0, 1.0)
Function
distort_color is computationally expensive, partly due to the
nonlinear
RGB to HSV and HSV to RGB conversions that are required in order to
access hue
and saturation. Both fast and full modes require these conversions and although
fast mode is less computationally expensive, it still pushes the model to the
CPU-compute-bound region, when enabled.
As an alternative, a new function
distort_color_fast has been added to the
list
of options. This function maps the image from RGB to YCrCb using the
[JPEG conversion](https://en.wikipedia.org/wiki/YCbCr#JPEG_conversion)
scheme and randomly alters brightness and the Cr/Cb chromas before mapping back
to RGB. The following code snippet shows an implementation of this function:
def distort_color_fast(image, scope=None):
with tf.name_scope(scope, 'distort_color', [image]):
br_delta = random_ops.random_uniform([], -32./255., 32./255., seed=None)
cb_factor = random_ops.random_uniform(
[], -FLAGS.cb_distortion_range, FLAGS.cb_distortion_range, seed=None)
cr_factor = random_ops.random_uniform(
[], -FLAGS.cr_distortion_range, FLAGS.cr_distortion_range, seed=None)
channels = tf.split(axis=2, num_or_size_splits=3, value=image)
red_offset = 1.402 * cr_factor + br_delta
green_offset = -0.344136 * cb_factor - 0.714136 * cr_factor + br_delta
blue_offset = 1.772 * cb_factor + br_delta
channels[0] += red_offset
channels[1] += green_offset
channels[2] += blue_offset
image = tf.concat(axis=2, values=channels)
image = tf.clip_by_value(image, 0., 1.)
return image
Here's a sample image that has undergone preprocessing. A randomly chosen region
of the image has been selected and colors altered using the
distort_color_fast
function.
Function
distort_color_fast is computationally efficient and still allows
training to be TPU execution time bound. In addition, it
has been used to train the Inception v3 model to an accuracy greater
than 78.1% using batch sizes in the 1,024-16,384 range.
Optimizer
The current model showcases three flavors of optimizers: SGD, momentum, and RMSProp.
Stochastic gradient descent (SGD) is the simplest update: the weights
are nudged in the negative gradient direction. Despite its simplicity, good
results can still be obtained on some models. The updates dynamics can be
written as:
Momentum is a popular optimizer that frequently leads to faster convergence than SGD. This optimizer updates weights much like SGD but also adds a component in the direction of the previous update. The following equations describe the updates performed by the momentum optimizer:
which can be written as:
The last term is the component in the direction of the previous update.
For the momentum \({\beta}\), we use the value of 0.9.
RMSprop is a popular optimizer first proposed by Geoff Hinton in one of his
[lectures](http://www.cs.toronto.edu/%7Etijmen/csc321/slides/lecture_slides_lec6.pdf).
The following equations describe how the optimizer works:
For Inception v3, tests show RMSProp giving the best results in terms of maximum accuracy and time to attain it, with momentum a close second. Thus RMSprop is set as the default optimizer. The parameters used are: decay \({\alpha}\) = 0.9, momentum \({\beta}\) = 0.9, and \({\epsilon}\) = 1.0.
The following code snippet shows how to set these parameters:
if FLAGS.optimizer == 'sgd':
tf.logging.info('Using SGD optimizer')
optimizer = tf.train.GradientDescentOptimizer(
learning_rate=learning_rate)
elif FLAGS.optimizer == 'momentum':
tf.logging.info('Using Momentum optimizer')
optimizer = tf.train.MomentumOptimizer(
learning_rate=learning_rate, momentum=0.9)
elif FLAGS.optimizer == 'RMS':
tf.logging.info('Using RMS optimizer')
optimizer = tf.train.RMSPropOptimizer(
learning_rate,
RMSPROP_DECAY,
momentum=RMSPROP_MOMENTUM,
epsilon=RMSPROP_EPSILON)
else:
tf.logging.fatal('Unknown optimizer:', FLAGS.optimizer)
When running on TPUs and using the Estimator API, the optimizer needs to be
wrapped in a
CrossShardOptimizer function in order to ensure synchronization
among the replicas (along with any necessary cross communication). The following
code snippet shows how the Inception v3 model wraps the optimizer:
if FLAGS.use_tpu:
optimizer = tpu_optimizer.CrossShardOptimizer(optimizer)
update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
with tf.control_dependencies(update_ops):
train_op = optimizer.minimize(loss, global_step=global_step)
Exponential moving average
While training, the trainable parameters are updated during backpropagation according to the optimizer's update rules. The equations describing these rules were discussed in the previous section and repeated here for convenience:
[Exponential moving
average](https://en.wikipedia.org/wiki/Exponential_smoothing)
(also known as exponential smoothing) is an optional post-processing step that
is applied to the updated weights and can sometimes lead to noticeable
improvements in performance. TensorFlow provides the function
[tf.train.ExponentialMovingAverage](https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage)
that computes the ema \({\hat{\theta}}\) of weight \({\theta}\) using the
formula:
where \({\alpha}\) is a decay factor (close to 1.0). In the Inception v3 model, \({\alpha}\) is set to 0.995.
Even though this calculation is an Infinite Impulse Response (IIR) filter, the decay factor establishes an effective window where most of the energy (or relevant samples) resides, as shown in the following diagram:
We can rewrite the filter equation, as follows:
where we used \({\hat\theta_{-1}}=0\).
The \({\alpha}^k\) values decay with increasing k, so only a subset of the samples will have a sizable influence on \(\hat{\theta}_{t+T+1}\). The rule of thumb for the decay factor value is: \(\frac {1} {1-\alpha}\), which corresponds to \({\alpha}\) = 200 for =0.995.
We first get a collection of trainable variables and then use the
apply()
method to create shadow variables for each trained variable.
The following code snippet shows the Inception v3 model implementation:
update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
with tf.control_dependencies(update_ops):
train_op = optimizer.minimize(loss, global_step=global_step)
if FLAGS.moving_average:
ema = tf.train.ExponentialMovingAverage(
decay=MOVING_AVERAGE_DECAY, num_updates=global_step)
variables_to_average = (tf.trainable_variables() +
tf.moving_average_variables())
with tf.control_dependencies([train_op]), tf.name_scope('moving_average'):
train_op = ema.apply(variables_to_average)
We'd like to use the ema variables during evaluation. We
define class
LoadEMAHook that applies method
variables_to_restore() to the
checkpoint file to evaluate using the shadow variable names:
class LoadEMAHook(tf.train.SessionRunHook):
def __init__(self, model_dir):
super(LoadEMAHook, self).__init__()
self._model_dir = model_dir
def begin(self):
ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY)
variables_to_restore = ema.variables_to_restore()
self._load_ema = tf.contrib.framework.assign_from_checkpoint_fn(
tf.train.latest_checkpoint(self._model_dir), variables_to_restore)
def after_create_session(self, sess, coord):
tf.logging.info('Reloading EMA...')
self._load_ema(sess)
The
hooks function is passed to
evaluate() as shown in the following code
snippet:
if FLAGS.moving_average:
eval_hooks = [LoadEMAHook(FLAGS.model_dir)]
else:
eval_hooks = []

eval_results = inception_classifier.evaluate(
input_fn=InputPipeline(False), steps=eval_steps, hooks=eval_hooks)
Batch normalization
[Batch normalization](https://arxiv.org/abs/1502.03167) is a widely used
technique for normalizing input features on models that can lead to substantial
reduction in convergence time. It is one of the more popular and useful
algorithmic improvements in machine learning of recent years and is used across
a wide range of models, including Inception v3.
Activation inputs are normalized by subtracting the mean and dividing by the standard deviation. To keep things balanced in the presence of back propagation, two trainable parameters are introduced in every layer. Normalized outputs \({\hat{x}}\) undergo a subsequent operation \({\gamma\hat{x}}+\beta\), where \({\gamma}\) and \({\beta}\) are a sort of standard deviation and mean learned by the model itself.
The full set of equations is in the
[paper](https://arxiv.org/abs/1502.03167)
and is repeated here for convenience:
Input: Values of x over a mini-batch: \(\Phi=\) { \({x_{1..m}\\} \) } Parameters to be learned: \({\gamma}\),\({\beta}\)
Output: { \({y_i}=BN_{\gamma,\beta}{(x_i)}\) }
\[{\mu_\phi} \leftarrow {\frac{1}{m}}{\sum_{i=1}^m}x_i \qquad \mathsf(mini-batch\ mean)\]
\[{\sigma_\phi}^2 \leftarrow {\frac{1}{m}}{\sum_{i=1}^m} {(x_i - {\mu_\phi})^2} \qquad \mathbf(mini-batch\ variance)\]
\[{\hat{x_i}} \leftarrow {\frac{x_i-{\mu_\phi}}{\sqrt {\sigma^2_\phi}+{\epsilon}}}\qquad \mathbf(normalize)\]
\[{y_i}\leftarrow {\gamma \hat{x_i}} + \beta \equiv BN_{\gamma,\beta}{(x_i)}\qquad \mathbf(scale \ and \ shift)\]
Normalization happens during training, but come evaluation time, we'd like the model to behave in a deterministic fashion: the classification result of an image should depend solely on the input image and not the set of images that are being fed to the model. Thus, we need to fix \({\mu}\) and \({\sigma}^2\) and use values that represent the image population statistics.
The model computes moving averages of the mean and variance over the minibatches:
\[{\hat\mu_i} = {\alpha \hat\mu_{t-1}}+{(1-\alpha)\mu_t}\]
\[{\hat\sigma_t}^2 = {\alpha{\hat\sigma^2_{t-1}}} + {(1-\alpha) {\sigma_t}^2}\]
In the specific case of Inception v3, a sensible decay factor had been obtained (using hyperparameter tuning) for use in GPUs. We would like to use this value on TPUs as well, but in order to do that, we need to make some adjustments.
Batch normalization moving mean and variance are both calculated using a loss pass filter, as shown in the following equation (here, \({y_t}\) represents moving mean or variance):
\[{y_t}={\alpha y_{t-1}}+{(1-\alpha)}{x_t} \]
(1)
In an 8x1 GPU (synchronous) job, each replica reads the current moving mean and updates it. The current replica must write the new moving variable before the next replica can read it.
When there are 8 replicas, the set of operations for an ensemble update is as follows:
\[{y_t}={\alpha y_{t-1}}+{(1-\alpha)}{x_t} \]
\[{y_{t+1}}={\alpha y_{t}}+{(1-\alpha)}{x_{t+1}} \]
\[{y_{t+2}}={\alpha y_{t+1}}+{(1-\alpha)}{x_{t+2}} \]
\[{y_{t+3}}={\alpha y_{t+2}}+{(1-\alpha)}{x_{t+3}} \]
\[{y_{t+4}}={\alpha y_{t+3}}+{(1-\alpha)}{x_{t+4}} \]
\[{y_{t+5}}={\alpha y_{t+4}}+{(1-\alpha)}{x_{t+5}} \]
\[{y_{t+6}}={\alpha y_{t+5}}+{(1-\alpha)}{x_{t+6}} \]
\[{y_{t+7}}={\alpha y_{t+6}}+{(1-\alpha)}{x_{t+7}} \]
This set of 8 sequential updates can be written as:
\[{y_{t+7}}={\alpha^8y_{t-1}}+(1-\alpha){\sum_{k=0}^7} {\alpha^{7-k}}{x_{t+k}}\]
(2)
In the current moving moment calculation implementation on TPUs, each shard performs calculations independently and there is no cross-shard communication. Batches are distributed to every shard and each of them processes 1/8th of the total number of batches (when there are 8 shards).
Although each shard computes the moving moments (that is, mean and variance), only the results from shard 0 are communicated back to the host CPU. So, effectively, only one replica is doing the moving mean/variance update:
\[{z_t}={\beta {z_{t-1}}}+{(1-\beta)u_t}\]
(3)
and this update happens at 1/8th the rate of its sequential counterpart. In order to compare GPU and TPU update equations, we need to align the respective timescales. Specifically, the set of operations that make up a set of 8 sequential updates on the GPU should be compared against a single update on the TPU as illustrated in the following diagram:
Let us show the equations with the modified time indexes:
\[{y_t}={\alpha^8y_{t-1}}+(1-\alpha){\sum_{k=0}^7} {\alpha^{7-k}}{x_{t-k/8}} \qquad \mathsf(GPU)\]
\[{z_t}={\beta {z_{t-1}}}+{(1-\beta)u_t}\qquad \mathsf(TPU) \]
If we make the assumption that 8 mini batches (normalized across all relevant dimensions) yield similar values within the GPU 8-minibatch sequential update, then we can approximate these equations as follows:
\[{y_t}={\alpha^8y_{t-1}}+(1-\alpha){\sum_{k=0}^7} {\alpha^{7-k}}{\hat{x_t}}={\alpha^8y_{t-1}+(1-\alpha^8){\hat{x_t}}} \qquad \mathsf(GPU)\]
\[{z_t}={\beta {z_{t-1}}}+{(1-\beta)u_t}\qquad \mathsf(TPU) \]
To match the effect of a given decay factor on the GPU, we modify the decay factor on the TPU accordingly. Specifically, we set \({\beta}\)=\({\alpha}^8\).
For Inception v3, the decay value used in the GPU is \({\alpha}\)=0.9997, which translates to a TPU decay value of \({\beta}\)=0.9976.
Learning rate adaptation
As batch sizes become larger, training becomes more difficult. Different
techniques continue to be proposed to allow efficient training for large batch
sizes (see
[here](https://arxiv.org/abs/1709.05011),
[here](https://arxiv.org/abs/1706.02677), and
[here](https://arxiv.org/abs/1711.04325), for example).
One of these techniques is increasing the learning rate gradually (also called ramp-up). Ramp-up was used to train the model to greater than 78.1% accuracy for batch sizes ranging from 4,096 to 16,384. For Inception v3, the learning rate is first set to about 10% of what would normally be the starting learning rate. The learning rate remains constant at this low value for a specified (small) number of 'cold epochs', and then begins a linear increase for a specified number of 'warm-up epochs'. At the end the 'warm-up epochs', the learning rate intersects with the normal exponential decay learning. This is illustrated in the following diagram.
The following code snippet shows how to do this:
initial_learning_rate = FLAGS.learning_rate * FLAGS.train_batch_size / 256
if FLAGS.use_learning_rate_warmup:
warmup_decay = FLAGS.learning_rate_decay**(
(FLAGS.warmup_epochs + FLAGS.cold_epochs) /
FLAGS.learning_rate_decay_epochs)
adj_initial_learning_rate = initial_learning_rate * warmup_decay
final_learning_rate = 0.0001 * initial_learning_rate
train_op = None
if training_active:
batches_per_epoch = _NUM_TRAIN_IMAGES / FLAGS.train_batch_size
global_step = tf.train.get_or_create_global_step()
current_epoch = tf.cast(
(tf.cast(global_step, tf.float32) / batches_per_epoch), tf.int32)
learning_rate = tf.train.exponential_decay(
learning_rate=initial_learning_rate,
global_step=global_step,
decay_steps=int(FLAGS.learning_rate_decay_epochs * batches_per_epoch),
decay_rate=FLAGS.learning_rate_decay,
staircase=True)
if FLAGS.use_learning_rate_warmup:
wlr = 0.1 * adj_initial_learning_rate
wlr_height = tf.cast(
0.9 * adj_initial_learning_rate /
(FLAGS.warmup_epochs + FLAGS.learning_rate_decay_epochs - 1),
tf.float32)
epoch_offset = tf.cast(FLAGS.cold_epochs - 1, tf.int32)
exp_decay_start = (FLAGS.warmup_epochs + FLAGS.cold_epochs +
FLAGS.learning_rate_decay_epochs)
lin_inc_lr = tf.add(
wlr, tf.multiply(
tf.cast(tf.subtract(current_epoch, epoch_offset), tf.float32),
wlr_height))
learning_rate = tf.where(
tf.greater_equal(current_epoch, FLAGS.cold_epochs),
(tf.where(tf.greater_equal(current_epoch, exp_decay_start),
learning_rate, lin_inc_lr)),
wlr)
# Set a minimum boundary for the learning rate.
learning_rate = tf.maximum(
learning_rate, final_learning_rate, name='learning_rate')",Advanced Guide to Inception v3 | Cloud TPU | Google Cloud,
id,url,body,title,description
50,https://cloud.google.com/tpu/docs/reference/rest/v2/projects.locations.nodes,"[Resource: Node](#Node) [State](#State) [NetworkConfig](#NetworkConfig) [ServiceAccount](#ServiceAccount) [SchedulingConfig](#SchedulingConfig) [NetworkEndpoint](#NetworkEndpoint) [AccessConfig](#AccessConfig) [Health](#Health) [AttachedDisk](#AttachedDisk) [DiskMode](#DiskMode) [ApiVersion](#ApiVersion) [Symptom](#Symptom) [SymptomType](#SymptomType) [ShieldedInstanceConfig](#ShieldedInstanceConfig) [Methods](#METHODS_SUMMARY)
Resource: Node
A TPU instance.
|JSON representation
|
{ ""name"": string, ""description"": string, ""acceleratorType"": string, ""state"": enum (
|Fields
|
name
|
Output only. Immutable. The name of the TPU.
|
description
|
The user-supplied description of the TPU. Maximum of 512 characters.
|
acceleratorType
|
Optional. The type of hardware accelerators associated with this node.
|
state
|
Output only. The current state for the TPU Node.
|
healthDescription
|
Output only. If this field is populated, it contains a description of why the TPU Node is unhealthy.
|
runtimeVersion
|
Required. The runtime version running in the Node.
|
networkConfig
|
Network configurations for the TPU node.
|
cidrBlock
|
The CIDR block that the TPU node will use when selecting an IP address. This CIDR block must be a /29 block; the Compute Engine networks API forbids a smaller block, and using a larger block would be wasteful (a node can only consume one IP address). Errors will occur if the CIDR block has already been used for a currently existing TPU node, the CIDR block conflicts with any subnetworks in the user's provided network, or the provided network is peered with another network that is using that CIDR block.
|
serviceAccount
|
The Google Cloud Platform Service Account to be used by the TPU node VMs. If None is specified, the default compute service account will be used.
|
createTime
|
Output only. The time when the node was created.
A timestamp in RFC3339 UTC ""Zulu"" format, with nanosecond resolution and up to nine fractional digits. Examples:
|
schedulingConfig
|
The scheduling options for this node.
|
networkEndpoints[]
|
Output only. The network endpoints where TPU workers can be accessed and sent work. It is recommended that runtime clients of the node reach out to the 0th entry in this map first.
|
health
|
The health status of the TPU node.
|
labels
|
Resource labels to represent user-provided metadata.
An object containing a list of
|
metadata
|
Custom metadata to apply to the TPU Node. Can set startup-script and shutdown-script
An object containing a list of
|
tags[]
|
Tags to apply to the TPU Node. Tags are used to identify valid sources or targets for network firewalls.
|
id
|
Output only. The unique identifier for the TPU Node.
|
dataDisks[]
|
The additional data disks for the Node.
|
apiVersion
|
Output only. The API version that created this Node.
|
symptoms[]
|
Output only. The Symptoms that have occurred to the TPU Node.
|
shieldedInstanceConfig
|
Shielded Instance options.
|
acceleratorConfig
|
The AccleratorConfig for the TPU Node.
|
queuedResource
|
Output only. The qualified name of the QueuedResource that requested this Node.
|
multisliceNode
|
Output only. Whether the Node belongs to a Multislice group.
State
Represents the different states of a TPU node during its lifecycle.
|Enums
|
STATE_UNSPECIFIED
|TPU node state is not known/set.
|
CREATING
|TPU node is being created.
|
READY
|TPU node has been created.
|
RESTARTING
|TPU node is restarting.
|
REIMAGING
|TPU node is undergoing reimaging.
|
DELETING
|TPU node is being deleted.
|
REPAIRING
|TPU node is being repaired and may be unusable. Details can be found in the 'help_description' field.
|
STOPPED
|TPU node is stopped.
|
STOPPING
|TPU node is currently stopping.
|
STARTING
|TPU node is currently starting.
|
PREEMPTED
|TPU node has been preempted. Only applies to Preemptible TPU Nodes.
|
TERMINATED
|TPU node has been terminated due to maintenance or has reached the end of its life cycle (for preemptible nodes).
|
HIDING
|TPU node is currently hiding.
|
HIDDEN
|TPU node has been hidden.
|
UNHIDING
|TPU node is currently unhiding.
NetworkConfig
Network related configurations.
|JSON representation
|
{ ""network"": string, ""subnetwork"": string, ""enableExternalIps"": boolean, ""canIpForward"": boolean, ""queueCount"": integer }
|Fields
|
network
|
The name of the network for the TPU node. It must be a preexisting Google Compute Engine network. If none is provided, ""default"" will be used.
|
subnetwork
|
The name of the subnetwork for the TPU node. It must be a preexisting Google Compute Engine subnetwork. If none is provided, ""default"" will be used.
|
enableExternalIps
|
Indicates that external IP addresses would be associated with the TPU workers. If set to false, the specified subnetwork or network should have Private Google Access enabled.
|
canIpForward
|
Allows the TPU node to send and receive packets with non-matching destination or source IPs. This is required if you plan to use the TPU workers to forward routes.
|
queueCount
|
Optional. Specifies networking queue count for TPU VM instance's network interface.
ServiceAccount
A service account.
|JSON representation
|
{ ""email"": string, ""scope"": [ string ] }
|Fields
|
email
|
Email address of the service account. If empty, default Compute service account will be used.
|
scope[]
|
The list of scopes to be made available for this service account. If empty, access to all Cloud APIs will be allowed.
SchedulingConfig
Sets the scheduling options for this node.
|JSON representation
|
{ ""preemptible"": boolean, ""reserved"": boolean }
|Fields
|
preemptible
|
Defines whether the node is preemptible.
|
reserved
|
Whether the node is created under a reservation.
NetworkEndpoint
A network endpoint over which a TPU worker can be reached.
|JSON representation
|
{
""ipAddress"": string,
""port"": integer,
""accessConfig"": {
object (
|Fields
|
ipAddress
|
The internal IP address of this network endpoint.
|
port
|
The port of this network endpoint.
|
accessConfig
|
The access config for the TPU worker.
AccessConfig
An access config attached to the TPU worker.
|JSON representation
|
{ ""externalIp"": string }
|Fields
|
externalIp
|
Output only. An external IP address associated with the TPU worker.
Health
Health defines the status of a TPU node as reported by Health Monitor.
|Enums
|
HEALTH_UNSPECIFIED
|Health status is unknown: not initialized or failed to retrieve.
|
HEALTHY
|The resource is healthy.
|
TIMEOUT
|The resource is unresponsive.
|
UNHEALTHY_TENSORFLOW
|The in-guest ML stack is unhealthy.
|
UNHEALTHY_MAINTENANCE
|The node is under maintenance/priority boost caused rescheduling and will resume running once rescheduled.
AttachedDisk
A node-attached disk resource. Next ID: 8;
|JSON representation
|
{
""sourceDisk"": string,
""mode"": enum (
|Fields
|
sourceDisk
|
Specifies the full path to an existing disk. For example: ""projects/my-project/zones/us-central1-c/disks/my-disk"".
|
mode
|
The mode in which to attach this disk. If not specified, the default is READ_WRITE mode. Only applicable to dataDisks.
DiskMode
The different mode of the attached disk.
|Enums
|
DISK_MODE_UNSPECIFIED
|The disk mode is not known/set.
|
READ_WRITE
|Attaches the disk in read-write mode. Only one TPU node can attach a disk in read-write mode at a time.
|
READ_ONLY
|Attaches the disk in read-only mode. Multiple TPU nodes can attach a disk in read-only mode at a time.
ApiVersion
TPU API Version.
|Enums
|
API_VERSION_UNSPECIFIED
|API version is unknown.
|
V1_ALPHA1
|TPU API V1Alpha1 version.
|
V1
|TPU API V1 version.
|
V2_ALPHA1
|TPU API V2Alpha1 version.
|
V2
|TPU API V2 version.
Symptom
A Symptom instance.
|JSON representation
|
{
""createTime"": string,
""symptomType"": enum (
|Fields
|
createTime
|
Timestamp when the Symptom is created.
A timestamp in RFC3339 UTC ""Zulu"" format, with nanosecond resolution and up to nine fractional digits. Examples:
|
symptomType
|
Type of the Symptom.
|
details
|
Detailed information of the current Symptom.
|
workerId
|
A string used to uniquely distinguish a worker within a TPU node.
SymptomType
SymptomType represents the different types of Symptoms that a TPU can be at.
|Enums
|
SYMPTOM_TYPE_UNSPECIFIED
|Unspecified symptom.
|
LOW_MEMORY
|TPU VM memory is low.
|
OUT_OF_MEMORY
|TPU runtime is out of memory.
|
EXECUTE_TIMED_OUT
|TPU runtime execution has timed out.
|
MESH_BUILD_FAIL
|TPU runtime fails to construct a mesh that recognizes each TPU device's neighbors.
|
HBM_OUT_OF_MEMORY
|TPU HBM is out of memory.
|
PROJECT_ABUSE
|Abusive behaviors have been identified on the current project.
ShieldedInstanceConfig
A set of Shielded Instance options.
|JSON representation
|
{ ""enableSecureBoot"": boolean }
|Fields
|
enableSecureBoot
|
Defines whether the instance has Secure Boot enabled.
|
Methods
|
|Creates a node.
|
|Deletes a node.
|
|Gets the details of a node.
|
|Retrieves the guest attributes for the node.
|
|Lists nodes.
|
|Updates the configurations of a node.
|
|Starts a node.
|
|Stops a node.",REST Resource: projects.locations.nodes | Cloud TPU | Google Cloud,
id,url,body,title,description
36,https://cloud.google.com/tpu/docs/shared-vpc-networks,"Connect a TPU to a shared VPC network
How you connect a TPU host to a Shared VPC depends upon which TPU architecture
(TPU VM or TPU Node) you are using. For more information about TPU architectures,
see
[TPU System Architecture](/tpu/docs/system-architecture-tpu-vm).
Connect a TPU VM to a Shared VPC Network
Configure a VPC host project
When you use the TPU VM architecture, you need to grant the TPU Service Account
in your
[service project](/vpc/docs/shared-vpc#concepts_and_terminology)
permissions to manage resources in the [host project](/vpc/docs/shared-vpc#concepts_and_terminology).
You do this using the ""TPU Shared VPC Agent"" (
roles/tpu.xpnAgent) role. Run
the following gcloud commands to grant this role binding.
gcloud projects add-iam-policy-binding host-project-id \ --member=serviceAccount:service-your-service-project-number@gcp-sa-tpu.iam.gserviceaccount.com \ --role=roles/tpu.xpnAgent
Create a TPU VM connected to a Shared VPC Network
First determine which accelerator types and versions are available in the zone
gcloud compute tpus accelerator-types list --zone zone
gcloud compute tpus versions list --zone zone
You connect a TPU VM to a shared VPC network when you create your TPU. Specify
your shared VPC using the
--network tag:
gcloud compute tpus tpu-vm create tpu-name \ --zone zone \ --accelerator-type accelerator-type \ --network projects/host-project-id/global/networks/host-network \ --version runtime-version \ --project your-service-project-id
You can verify your TPU VM is connected to your shared VPC using the
gcloud compute tpus tpu-vm describe
command:
$ gcloud compute tpus tpu-vm describe tpu-name --zone zone
The response includes the network to which your TPU VM is attached:
acceleratorType: v3-8 apiVersion: V2 cidrBlock: 10.128.0.0/20 createTime: '2022-06-17T21:32:13.859274143Z' health: HEALTHY id: '0000000000000000000' name: projects/my-project/locations/us-central1-b/nodes/my-tpu networkConfig: enableExternalIps: true network: projects/my-project/global/networks/default subnetwork: projects/my-project/regions/us-central1/subnetworks/default networkEndpoints: - accessConfig: externalIp: 000.000.000.000 ipAddress: 10.128.0.104 port: 8470 runtimeVersion: tpu-vm-tf-2.8.0 schedulingConfig: {} serviceAccount: email: 00000000000-compute@developer.gserviceaccount.com scope: - https://www.googleapis.com/auth/devstorage.read_write - https://www.googleapis.com/auth/logging.write - https://www.googleapis.com/auth/service.management - https://www.googleapis.com/auth/servicecontrol - https://www.googleapis.com/auth/cloud-platform - https://www.googleapis.com/auth/pubsub shieldedInstanceConfig: {} state: READY
Delete the TPU VM
When you are done with the TPU VM, make sure to delete it.
gcloud compute tpus tpu-vm delete tpu-name \ --zone zone \
Connecting a TPU Node to a Shared VPC
Configure Private Service Access
Before you use TPU Nodes with Shared VPCs, you need to
[establish a private service access connection](https://cloud.google.com/vpc/docs/configure-private-services-access).
Enable the Service Networking API using the following Google Cloud CLI command. This only needs to be done once per Cloud Platform Project.
gcloud services enable servicenetworking.googleapis.com
[Allocate a reserved address range](https://cloud.google.com/sdk/gcloud/reference/compute/addresses/create)for use by Service Networking. The
prefix-lengthneeds to be 24 or less. For example:
gcloud compute addresses create sn-range-1 --global \ --addresses=10.110.0.0 \ --prefix-length=16 \ --purpose=VPC_PEERING \ --network=network-name
Establish a private service access connection.
$ gcloud services vpc-peerings connect --service=servicenetworking.googleapis.com \ --ranges=sn-range-1 \ --network=network-name
Verify the VPC peering has been created. The following command lists all VPC peerings for the specified network.
gcloud services vpc-peerings list --network=network-name
Connect a TPU Node with a Shared VPC Network
You connect a TPU Node to a shared VPC network when you create your TPU. Specify
your shared VPC using the
--network tag:
$ gcloud compute tpus execution-groups create \
--name=tpu-name \
--zone=zone \
--tf-version=2.12.0 \
--machine-type=n1-standard-1 \
--accelerator-type=v3-8 \
--network=network-name
Retrieve information about a specific TPU Node
$ gcloud compute tpus describe tpu-name --zone zone
The response contains the following information:
acceleratorType: v3-8 apiVersion: V1 cidrBlock: 00.0.000.000/29 createTime: '2022-11-30T18:59:20.655858097Z' health: HEALTHY ipAddress: 00.000.0.000 name: projects/ml-writers/locations/us-central1-a/nodes/mikegre-vcp network: global/networks/mikegre-vpc networkEndpoints: - ipAddress: 00.0.000.000 port: 8470 port: '8470' schedulingConfig: {} serviceAccount: service-00000000000@cloud-tpu.iam.gserviceaccount.com state: READY tensorflowVersion: 2.10.0
Delete the TPU Node
When you are done with the TPU Node, make sure to delete it.
$ gcloud compute tpus execution-groups delete tpu-name \
--zone=zone
### Delete the VPC peering
A peering connection can be disconnected using the compute networking API.
These calls should be made in Shared VPC host projects.
1. List all VPC peerings to find the name of the peering to delete.
$ gcloud compute networks peerings list --network=network-name
Delete a VPC peering.
$ gcloud compute networks peerings delete peering-name --network=network-name",Connect a TPU to a shared VPC network | Google Cloud,
id,url,body,title,description
2,https://cloud.google.com/tpu/docs/reference/rest/v2alpha1/projects.locations.operations/list,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Query parameters](#body.QUERY_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [Try it!](#try-it)
Lists operations that match the specified filter in the request. If the server doesn't support this method, it returns
UNIMPLEMENTED.
HTTP request
GET https://tpu.googleapis.com/v2alpha1/{name=projects/*/locations/*}/operations
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
The name of the operation's parent resource.
Query parameters
|Parameters
|
filter
|
The standard list filter.
|
pageSize
|
The standard list page size.
|
pageToken
|
The standard list page token.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[ListOperationsResponse](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse)
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).",Method: projects.locations.operations.list | Cloud TPU | Google Cloud,
id,url,body,title,description
70,https://cloud.google.com/tpu/docs/quick-starts,"Before starting one of the Cloud TPU VM quickstarts, read
[Introduction to Cloud TPU](/tpu/docs/intro-to-tpu) which gives an overview
of working with Cloud TPUs.
Follow the instructions in
[Set up the Cloud TPU environment](/tpu/docs/setup-gcp-account) to prepare
to create a Cloud TPU.
The VM and Pod quickstarts provide a brief introduction to working with Cloud TPU VMs using TensorFlow, JAX, and PyTorch. These topics explain how to install an ML framework and run a sample application on a Cloud TPU VM.
Set up your environment
Cloud TPU VM quickstarts
Cloud TPU Pod quickstarts
-
Train on Cloud TPU Pods
Learn about best practices and common issues for Cloud TPU Pods.
-
Run TensorFlow on Cloud TPU Pod slices
Run a simple calculation on Cloud TPU Pod slice with TensorFlow.
-
Run JAX on Cloud TPU Pod slices
Run a simple calculation on Cloud TPU Pod slice with JAX.
-
Run PyTorch on Cloud TPU Pod slices
Run a simple calculation on Cloud TPU Pod slice with PyTorch.",Get started | Cloud TPU | Google Cloud,
id,url,body,title,description
52,https://cloud.google.com/tpu/docs/reference/rest/Shared.Types/ListLocationsResponse,"The response message for
.
[Locations.ListLocations](/tpu/docs/reference/rest/v1alpha1/projects.locations/list#google.cloud.location.Locations.ListLocations)
|JSON representation
|
{
""locations"": [
{
object (
|Fields
|
locations[]
|
A list of locations that matches the specified filter in the request.
|
nextPageToken
|
The standard List next-page token.
Location
A resource that represents a Google Cloud location.
|JSON representation
|
{ ""name"": string, ""locationId"": string, ""displayName"": string, ""labels"": { string: string, ... }, ""metadata"": { ""@type"": string, field1: ..., ... } }
|Fields
|
name
|
Resource name for the location, which may vary between implementations. For example:
|
locationId
|
The canonical id for this location. For example:
|
displayName
|
The friendly name for this location, typically a nearby city name. For example, ""Tokyo"".
|
labels
|
Cross-service attributes for the location. For example
An object containing a list of
|
metadata
|
Service-specific metadata. For example the available capacity at the given location.
An object containing fields of an arbitrary type. An additional field",ListLocationsResponse | Cloud TPU | Google Cloud,
id,url,body,title,description
33,https://cloud.google.com/tpu/docs/reference/rest/v2alpha1/projects.locations.queuedResources,"[Resource: QueuedResource](#QueuedResource) [Tpu](#Tpu) [NodeSpec](#NodeSpec) [MultiNodeParams](#MultiNodeParams) [BestEffort](#BestEffort) [Guaranteed](#Guaranteed) [Spot](#Spot) [QueueingPolicy](#QueueingPolicy) [Interval](#Interval) [QueuedResourceState](#QueuedResourceState) [State](#State) [CreatingData](#CreatingData) [AcceptedData](#AcceptedData) [ProvisioningData](#ProvisioningData) [FailedData](#FailedData) [DeletingData](#DeletingData) [ActiveData](#ActiveData) [SuspendingData](#SuspendingData) [SuspendedData](#SuspendedData) [StateInitiator](#StateInitiator) [Methods](#METHODS_SUMMARY)
Resource: QueuedResource
A QueuedResource represents a request for resources that will be placed in a queue and fulfilled when the necessary resources are available.
|JSON representation
|
{ ""name"": string, ""createTime"": string, ""queueingPolicy"": { object (
|Fields
|
name
|
Output only. Immutable. The name of the QueuedResource.
|
createTime
|
Output only. The time when the QueuedResource was created.
A timestamp in RFC3339 UTC ""Zulu"" format, with nanosecond resolution and up to nine fractional digits. Examples:
|
queueingPolicy
|
The queueing policy of the QueuedRequest.
|
state
|
Output only. State of the QueuedResource request.
|
reservationName
|
Name of the reservation in which the resource should be provisioned. Format: projects/{project}/locations/{zone}/reservations/{reservation}
|Union field
resource. Resource specification.
resource can be only one of the following:
|
tpu
|
Defines a TPU resource.
|Union field
tier. Tier specifies the required tier.
tier can be only one of the following:
|
bestEffort
|
The BestEffort tier.
|
guaranteed
|
The Guaranteed tier.
|
spot
|
Optional. The Spot tier.
Tpu
Details of the TPU resource(s) being requested.
|JSON representation
|
{
""nodeSpec"": [
{
object (
|Fields
|
nodeSpec[]
|
The TPU node(s) being requested.
NodeSpec
Details of the TPU node(s) being requested. Users can request either a single node or multiple nodes. NodeSpec provides the specification for node(s) to be created.
|JSON representation
|
{ ""parent"": string, ""nodeId"": string, ""multiNodeParams"": { object (
|Fields
|
parent
|
Required. The parent resource name.
|
nodeId
|
The unqualified resource name. Should follow the
|
multiNodeParams
|
Optional. Fields to specify in case of multi-node request.
|
node
|
Required. The node.
MultiNodeParams
Parameters to specify for multi-node QueuedResource requests. This field must be populated in case of multi-node requests instead of nodeId. It's an error to specify both nodeId and multiNodeParams.
|JSON representation
|
{ ""nodeCount"": integer, ""nodeIdPrefix"": string }
|Fields
|
nodeCount
|
Required. Number of nodes with this spec. The system will attempt to provison ""nodeCount"" nodes as part of the request. This needs to be > 1.
|
nodeIdPrefix
|
Prefix of node_ids in case of multi-node request Should follow the
BestEffort
This type has no fields.
BestEffort tier definition.
Guaranteed
Guaranteed tier definition.
|JSON representation
|
{ ""minDuration"": string, ""reserved"": boolean }
|Fields
|
minDuration
|
Optional. Defines the minimum duration of the guarantee. If specified, the requested resources will only be provisioned if they can be allocated for at least the given duration.
A duration in seconds with up to nine fractional digits, ending with '
|
reserved
|
Optional. Specifies the request should be scheduled on reserved capacity.
Spot
This type has no fields.
Spot tier definition.
QueueingPolicy
Defines the policy of the QueuedRequest.
|JSON representation
|
{ // Union field
|Fields
|Union field
start_timing_constraints. Time flexibility specification.
start_timing_constraints can be only one of the following:
|
validUntilDuration
|
A relative time after which resources should not be created. If the request cannot be fulfilled by this time the request will be failed.
A duration in seconds with up to nine fractional digits, ending with '
|
validUntilTime
|
An absolute time after which resources should not be created. If the request cannot be fulfilled by this time the request will be failed.
A timestamp in RFC3339 UTC ""Zulu"" format, with nanosecond resolution and up to nine fractional digits. Examples:
|
validAfterDuration
|
A relative time after which resources may be created.
A duration in seconds with up to nine fractional digits, ending with '
|
validAfterTime
|
An absolute time at which resources may be created.
A timestamp in RFC3339 UTC ""Zulu"" format, with nanosecond resolution and up to nine fractional digits. Examples:
|
validInterval
|
An absolute time interval within which resources may be created.
Interval
Represents a time interval, encoded as a Timestamp start (inclusive) and a Timestamp end (exclusive).
The start must be less than or equal to the end. When the start equals the end, the interval is empty (matches no time). When both start and end are unspecified, the interval matches any time.
|JSON representation
|
{ ""startTime"": string, ""endTime"": string }
|Fields
|
startTime
|
Optional. Inclusive start of the interval.
If specified, a Timestamp matching this interval will have to be the same or after the start.
A timestamp in RFC3339 UTC ""Zulu"" format, with nanosecond resolution and up to nine fractional digits. Examples:
|
endTime
|
Optional. Exclusive end of the interval.
If specified, a Timestamp matching this interval will have to be before the end.
A timestamp in RFC3339 UTC ""Zulu"" format, with nanosecond resolution and up to nine fractional digits. Examples:
QueuedResourceState
QueuedResourceState defines the details of the QueuedResource request.
|JSON representation
|
{ ""state"": enum (
|Fields
|
state
|
State of the QueuedResource request.
|
stateInitiator
|
Output only. The initiator of the QueuedResources's current state.
|Union field
state_data. Further data for the state.
state_data can be only one of the following:
|
creatingData
|
Further data for the creating state.
|
acceptedData
|
Further data for the accepted state.
|
provisioningData
|
Further data for the provisioning state.
|
failedData
|
Further data for the failed state.
|
deletingData
|
Further data for the deleting state.
|
activeData
|
Further data for the active state.
|
suspendingData
|
Further data for the suspending state.
|
suspendedData
|
Further data for the suspended state.
State
Output only state of the request
|Enums
|
STATE_UNSPECIFIED
|State of the QueuedResource request is not known/set.
|
CREATING
|The QueuedResource request has been received. We're still working on determining if we will be able to honor this request.
|
ACCEPTED
|The QueuedResource request has passed initial validation/admission control and has been persisted in the queue.
|
PROVISIONING
|The QueuedResource request has been selected. The associated resources are currently being provisioned (or very soon will begin provisioning).
|
FAILED
|The request could not be completed. This may be due to some late-discovered problem with the request itself, or due to unavailability of resources within the constraints of the request (e.g., the 'valid until' start timing constraint expired).
|
DELETING
|The QueuedResource is being deleted.
|
ACTIVE
|The resources specified in the QueuedResource request have been provisioned and are ready for use by the end-user/consumer.
|
SUSPENDING
|The resources specified in the QueuedResource request are being deleted. This may have been initiated by the user, or the Cloud TPU service. Inspect the state data for more details.
|
SUSPENDED
|The resources specified in the QueuedResource request have been deleted.
|
WAITING_FOR_RESOURCES
|The QueuedResource request has passed initial validation and has been persisted in the queue. It will remain in this state until there are sufficient free resources to begin provisioning your request. Wait times will vary significantly depending on demand levels. When demand is high, not all requests can be immediately provisioned. If you need more reliable obtainability of TPUs consider purchasing a reservation. To put a limit on how long you are willing to wait, use
CreatingData
This type has no fields.
Further data for the creating state.
AcceptedData
This type has no fields.
Further data for the accepted state.
ProvisioningData
This type has no fields.
Further data for the provisioning state.
FailedData
Further data for the failed state.
|JSON representation
|
{
""error"": {
object (
|Fields
|
error
|
The error that caused the queued resource to enter the FAILED state.
DeletingData
This type has no fields.
Further data for the deleting state.
ActiveData
This type has no fields.
Further data for the active state.
SuspendingData
This type has no fields.
Further data for the suspending state.
SuspendedData
This type has no fields.
Further data for the suspended state.
StateInitiator
The initiator of the QueuedResource's SUSPENDING/SUSPENDED state.
|Enums
|
STATE_INITIATOR_UNSPECIFIED
|The state initiator is unspecified.
|
USER
|The current QueuedResource state was initiated by the user.
|
SERVICE
|The current QueuedResource state was initiated by the service.
|
Methods
|
|Creates a QueuedResource TPU instance.
|
|Deletes a QueuedResource TPU instance.
|
|Gets details of a queued resource.
|
|Lists queued resources.
|
|Resets a QueuedResource TPU instance",REST Resource: projects.locations.queuedResources | Cloud TPU | Google Cloud,
id,url,body,title,description
69,https://cloud.google.com/tpu/docs/reference/rest/v1alpha1/projects.locations.operations/list,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Query parameters](#body.QUERY_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [Try it!](#try-it)
Lists operations that match the specified filter in the request. If the server doesn't support this method, it returns
UNIMPLEMENTED.
HTTP request
GET https://tpu.googleapis.com/v1alpha1/{name=projects/*/locations/*}/operations
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
The name of the operation's parent resource.
Query parameters
|Parameters
|
filter
|
The standard list filter.
|
pageSize
|
The standard list page size.
|
pageToken
|
The standard list page token.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[ListOperationsResponse](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse)
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).",Method: projects.locations.operations.list | Cloud TPU | Google Cloud,
id,url,body,title,description
23,https://cloud.google.com/tpu/docs/reference/rest/v2alpha1/projects.locations.nodes/simulateMaintenanceEvent,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [Try it!](#try-it)
Simulates a maintenance event.
HTTP request
POST https://tpu.googleapis.com/v2alpha1/{name=projects/*/locations/*/nodes/*}:simulateMaintenanceEvent
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
Required. The resource name.
Request body
The request body contains data with the following structure:
|JSON representation
|
{ ""workerIds"": [ string ] }
|Fields
|
workerIds[]
|
The 0-based worker ID. If it is empty, worker ID 0 will be selected for maintenance event simulation. A maintenance event will only be fired on the first specified worker ID. Future implementations may support firing on multiple workers.
Response body
If successful, the response body contains an instance of
.
[Operation](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse#Operation)
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).",Method: projects.locations.nodes.simulateMaintenanceEvent | Cloud TPU | Google Cloud,
id,url,body,title,description
34,https://cloud.google.com/tpu/docs/audit-logs,"Cloud TPU audit logging
This document describes the audit logs created by Cloud TPU as part of
[Cloud Audit Logs](/logging/docs/audit).
Overview
Google Cloud services write audit logs to help you answer the questions, ""Who did what, where, and when?"" within your Google Cloud resources.
Your Google Cloud projects contain only the audit logs for resources that are directly within the Google Cloud project. Other Google Cloud resources, such as folders, organizations, and billing accounts, contain the audit logs for the entity itself.
For a general overview of Cloud Audit Logs, see
[Cloud Audit Logs overview](/logging/docs/audit). For a deeper understanding
of the audit log format, see
[Understand audit logs](/logging/docs/audit/understanding-audit-logs).
Available audit logs
The following types of audit logs are available for Cloud TPU:
-
Admin Activity audit logs
Includes ""admin write"" operations that write metadata or configuration information.
You can't disable Admin Activity audit logs.
-
Data Access audit logs
Includes ""admin read"" operations that read metadata or configuration information. Also includes ""data read"" and ""data write"" operations that read or write user-provided data.
To receive Data Access audit logs, you must
[explicitly enable](/logging/docs/audit/configure-data-access#config-console-enable)them.
-
System Event audit logs
Identifies automated Google Cloud actions that modify the configuration of resources.
You can't disable System Event audit logs.
For fuller descriptions of the audit log types, see
[Types of audit logs](/logging/docs/audit#types).
Audited operations
The following table summarizes which API operations correspond to each audit log type in Cloud TPU:
|Audit logs category
|Cloud TPU methods
|Admin Activity audit logs
|CreateNode
DeleteNode
|Data Access (ADMIN_READ) audit logs
|ListNodes
DescribeNodes
|Data Access (DATA_WRITE) audit logs
|CreateNode
DeleteNode
|System Event audit logs
|
tpu.node.terminate
Occurs when a node has been terminated involuntarily, such as due to a maintenance event.
The reason for termination is available in
protoPayload.metadata.terminateReason.
|
tpu.node.restart
Occurs when a node that was terminated has been repaired and restarted.
Audit log format
Audit log entries include the following objects:
The log entry itself, which is an object of type
[. Useful fields include the following:](/logging/docs/reference/v2/rest/v2/LogEntry)
LogEntry
- The
logNamecontains the resource ID and audit log type.
- The
resourcecontains the target of the audited operation.
- The
timeStampcontains the time of the audited operation.
- The
protoPayloadcontains the audited information.
- The
The audit logging data, which is an
[object held in the](/logging/docs/reference/audit/auditlog/rest/Shared.Types/AuditLog)
AuditLog
protoPayloadfield of the log entry.
Optional service-specific audit information, which is a service-specific object. For earlier integrations, this object is held in the
serviceDatafield of the
AuditLogobject; later integrations use the
metadatafield.
For other fields in these objects, and how to interpret them, review
[Understand audit logs](/logging/docs/audit/understanding-audit-logs).
Log name
Cloud Audit Logs log names include resource identifiers indicating the Google Cloud project or other Google Cloud entity that owns the audit logs, and whether the log contains Admin Activity, Data Access, Policy Denied, or System Event audit logging data.
The following are the audit log names, including variables for the resource identifiers:
projects/PROJECT_ID/logs/cloudaudit.googleapis.com%2Factivity projects/PROJECT_ID/logs/cloudaudit.googleapis.com%2Fdata_access projects/PROJECT_ID/logs/cloudaudit.googleapis.com%2Fsystem_event projects/PROJECT_ID/logs/cloudaudit.googleapis.com%2Fpolicy folders/FOLDER_ID/logs/cloudaudit.googleapis.com%2Factivity folders/FOLDER_ID/logs/cloudaudit.googleapis.com%2Fdata_access folders/FOLDER_ID/logs/cloudaudit.googleapis.com%2Fsystem_event folders/FOLDER_ID/logs/cloudaudit.googleapis.com%2Fpolicy billingAccounts/BILLING_ACCOUNT_ID/logs/cloudaudit.googleapis.com%2Factivity billingAccounts/BILLING_ACCOUNT_ID/logs/cloudaudit.googleapis.com%2Fdata_access billingAccounts/BILLING_ACCOUNT_ID/logs/cloudaudit.googleapis.com%2Fsystem_event billingAccounts/BILLING_ACCOUNT_ID/logs/cloudaudit.googleapis.com%2Fpolicy organizations/ORGANIZATION_ID/logs/cloudaudit.googleapis.com%2Factivity organizations/ORGANIZATION_ID/logs/cloudaudit.googleapis.com%2Fdata_access organizations/ORGANIZATION_ID/logs/cloudaudit.googleapis.com%2Fsystem_event organizations/ORGANIZATION_ID/logs/cloudaudit.googleapis.com%2Fpolicy
Service name
Cloud TPU audit logs use the service name
tpu.googleapis.com.
For a list of all the Cloud Logging API service names and their corresponding
monitored resource type, see
[Map services to resources](/logging/docs/api/v2/resource-list#service-names).
Resource types
Cloud TPU audit logs use the resource type
audited_resource for all audit logs.
For a list of all the Cloud Logging monitored resource types and descriptive
information, see
[Monitored resource types](/logging/docs/api/v2/resource-list#resource-types).
Caller identities
The IP address of the caller is held in the
RequestMetadata.caller_ip field of
the
[ object. Logging might redact certain
caller identities and IP addresses. AuditLog](/logging/docs/reference/audit/auditlog/rest/Shared.Types/AuditLog)
For information about what information is redacted in audit logs, see
[Caller identities in audit logs](/logging/docs/audit#user-id).
Enable audit logging
System Event audit logs are always enabled; you can't disable them.
Admin Activity audit logs are always enabled; you can't disable them.
Data Access audit logs are disabled by default and aren't written unless explicitly enabled (the exception is Data Access audit logs for BigQuery, which can't be disabled).
For information about enabling some or all of your Data Access audit logs, see
[Enable Data Access audit logs](/logging/docs/audit/configure-data-access).
Permissions and roles
[IAM](/iam/docs) permissions and roles determine your ability to
access audit logs data in Google Cloud resources.
When deciding which
[Logging-specific permissions and roles](/logging/docs/access-control#permissions_and_roles)
apply to your use case, consider the following:
The Logs Viewer role (
roles/logging.viewer) gives you read-only access to Admin Activity, Policy Denied, and System Event audit logs. If you have just this role, you cannot view Data Access audit logs that are in the
_Defaultbucket.
The Private Logs Viewer role
(roles/logging.privateLogViewer) includes the permissions contained in
roles/logging.viewer, plus the ability to read Data Access audit logs in the
_Defaultbucket.
Note that if these private logs are stored in user-defined buckets, then any user who has permissions to read logs in those buckets can read the private logs. For more information about log buckets, see
[Routing and storage overview](/logging/docs/routing/overview).
For more information about the IAM permissions and roles that
apply to audit logs data, see
[Access control with IAM](/logging/docs/access-control).
View logs
You can query for all audit logs or you can query for logs by their
[audit log name](#log_name). The audit log name includes the
[resource identifier](/resource-manager/docs/creating-managing-projects#identifying_projects)
of the Google Cloud project, folder, billing account, or
organization for which you want to view audit logging information.
Your queries can specify indexed [ fields, and if you use
the Log Analytics page, which supports SQL queries, then you can
LogEntry](/logging/docs/reference/v2/rest/v2/LogEntry) [view your query results as a chart](/logging/docs/analyze/charts).
For more information about querying your logs, see the following pages:
[Build queries in the Logs Explorer](/logging/docs/view/building-queries). [Query and view logs in Log Analytics](/logging/docs/analyze/query-and-view). [Sample queries for security insights](/logging/docs/analyze/analyze-audit-logs).
Console
In the Google Cloud console, you can use the Logs Explorer to retrieve your audit log entries for your Google Cloud project, folder, or organization:
-
In the navigation panel of the Google Cloud console, select Logging, and then select Logs Explorer:
Select an existing Google Cloud project, folder, or organization.
To display all audit logs, enter either of the following queries into the query-editor field, and then click Run query:
logName:""cloudaudit.googleapis.com""
protoPayload.""@type""=""type.googleapis.com/google.cloud.audit.AuditLog""
To display the audit logs for a specific resource and audit log type, in the Query builder pane, do the following:
In Resource type, select the Google Cloud resource whose audit logs you want to see.
In Log name, select the audit log type that you want to see:
- For Admin Activity audit logs, select activity.
- For Data Access audit logs, select data_access.
- For System Event audit logs, select system_event.
- For Policy Denied audit logs, select policy.
Click Run query.
If you don't see these options, then there aren't any audit logs of that type available in the Google Cloud project, folder, or organization.
If you're experiencing issues when trying to view logs in the Logs Explorer, see the
[troubleshooting](/logging/docs/view/logs-explorer-interface#troubleshooting)information.
For more information about querying by using the Logs Explorer, see
[Build queries in the Logs Explorer](/logging/docs/view/building-queries). For information about summarizing log entries in the Logs Explorer by using Duet AI, see [Summarize log entries with Duet AI assistance](/logging/docs/view/summarize-log-entries-duet-ai).
-
gcloud
The Google Cloud CLI provides a command-line interface to the Logging API. Supply a valid resource identifier in each of the log names. For example, if your query includes a PROJECT_ID, then the project identifier you supply must refer to the currently selected Google Cloud project.
To read your Google Cloud project-level audit log entries, run the following command:
gcloud logging read ""logName : projects/PROJECT_ID/logs/cloudaudit.googleapis.com"" \ --project=PROJECT_ID
To read your folder-level audit log entries, run the following command:
gcloud logging read ""logName : folders/FOLDER_ID/logs/cloudaudit.googleapis.com"" \ --folder=FOLDER_ID
To read your organization-level audit log entries, run the following command:
gcloud logging read ""logName : organizations/ORGANIZATION_ID/logs/cloudaudit.googleapis.com"" \ --organization=ORGANIZATION_ID
To read your Cloud Billing account-level audit log entries, run the following command:
gcloud logging read ""logName : billingAccounts/BILLING_ACCOUNT_ID/logs/cloudaudit.googleapis.com"" \ --billing-account=BILLING_ACCOUNT_ID
Add the
[
to your command to read logs that are more than 1 day old. --freshness flag](/sdk/gcloud/reference/logging/read#--freshness)
For more information about using the gcloud CLI, see
[. gcloud logging read](/sdk/gcloud/reference/logging/read)
API
When building your queries, supply a valid resource identifier in each of the log names. For example, if your query includes a PROJECT_ID, then the project identifier you supply must refer to the currently selected Google Cloud project.
For example, to use the Logging API to view your project-level audit log entries, do the following:
Go to the Try this API section in the documentation for the
[method.](/logging/docs/reference/v2/rest/v2/entries/list)
entries.list
Put the following into the Request body part of the Try this API form. Clicking this
[prepopulated form](/logging/docs/reference/v2/rest/v2/entries/list?apix_params=%7B%22resource%22%3A%7B%22resourceNames%22%3A%5B%22projects%2F%5BPROJECT_ID%5D%22%5D%2C%22pageSize%22%3A5%2C%22filter%22%3A%22logName%3D(projects%2F%5BPROJECT_ID%5D%2Flogs%2Fcloudaudit.googleapis.com%252Factivity%20OR%20projects%2F%5BPROJECT_ID%5D%2Flogs%2Fcloudaudit.googleapis.com%252Fsystem_events%20OR%20projects%2F%5BPROJECT_ID%5D%2Flogs%2Fcloudaudit.googleapis.com%252Fdata_access)%22%7D%7D)automatically fills the request body, but you need to supply a valid PROJECT_ID in each of the log names.
{ ""resourceNames"": [ ""projects/PROJECT_ID"" ], ""pageSize"": 5, ""filter"": ""logName : projects/PROJECT_ID/logs/cloudaudit.googleapis.com"" }
Click Execute.
Sample queries
To find audit logs for Cloud TPU, use the following queries in the Logs Explorer:
|Query/filter name
|Expression
|Audit log type
|TPU service logs
|
logName=(""projects/PROJECT_ID/logs/cloudaudit.googleapis.com%2Factivity"" OR ""projects/PROJECT_ID/logs/cloudaudit.googleapis.com%2Fdata_access"" OR ""projects/PROJECT_ID/logs/cloudaudit.googleapis.com%2Fsystem_event"" OR ""projects/PROJECT_ID/logs/cloudaudit.googleapis.com%2Fpolicy"") protoPayload.serviceName=""tpu.googleapis.com""
|All
|TPU node maintenance events
|
logName=""projects/PROJECT_ID/logs/cloudaudit.googleapis.com%2Fsystem_event"" protoPayload.serviceName=""tpu.googleapis.com"" protoPayload.methodName=(""tpu.nodes.terminate"" OR ""tpu.nodes.restart"")
|System Event
To use the sample queries, do the following:
Replace the variables with your own project information, then copy the expression using the clipboard icon content_copy.
-
In the navigation panel of the Google Cloud console, select Logging, and then select Logs Explorer:
Enable Show query to open the query-editor field, then paste the expression into the query-editor field:
Click Run query. Logs that match your query are listed in the Query results pane.
Route audit logs
You can
[route audit logs](/logging/docs/routing/overview) to supported
destinations in the same way that you can route other kinds of logs. Here are
some reasons you might want to route your audit logs:
To keep audit logs for a longer period of time or to use more powerful search capabilities, you can route copies of your audit logs to Cloud Storage, BigQuery, or Pub/Sub. Using Pub/Sub, you can route to other applications, other repositories, and to third parties.
To manage your audit logs across an entire organization, you can create
[aggregated sinks](/logging/docs/export/aggregated_sinks)that can route logs from any or all Google Cloud projects in the organization.
- If your enabled Data Access audit logs are pushing your Google Cloud projects over your log allotments, you can create sinks that exclude the Data Access audit logs from Logging.
For instructions about routing logs, see
[Route logs to supported destinations](/logging/docs/export/configure_export_v2).
Pricing
For information about Cloud Logging pricing, see
[Google Cloud's operations suite pricing: Cloud Logging](/stackdriver/pricing#logging-costs).",Cloud TPU audit logging | Google Cloud,
id,url,body,title,description
15,https://cloud.google.com/tpu/docs/reference/rest/v2/projects.locations.operations/list,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Query parameters](#body.QUERY_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [Try it!](#try-it)
Lists operations that match the specified filter in the request. If the server doesn't support this method, it returns
UNIMPLEMENTED.
HTTP request
GET https://tpu.googleapis.com/v2/{name=projects/*/locations/*}/operations
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
The name of the operation's parent resource.
Query parameters
|Parameters
|
filter
|
The standard list filter.
|
pageSize
|
The standard list page size.
|
pageToken
|
The standard list page token.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[ListOperationsResponse](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse)
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).",Method: projects.locations.operations.list | Cloud TPU | Google Cloud,
id,url,body,title,description
30,https://cloud.google.com/tpu/docs/reference/rest/Shared.Types/OperationMetadata,"Represents the metadata of the long-running operation.
|JSON representation
|
{ ""createTime"": string, ""endTime"": string, ""target"": string, ""verb"": string, ""statusDetail"": string, ""cancelRequested"": boolean, ""apiVersion"": string }
|Fields
|
createTime
|
Output only. The time the operation was created.
A timestamp in RFC3339 UTC ""Zulu"" format, with nanosecond resolution and up to nine fractional digits. Examples:
|
endTime
|
Output only. The time the operation finished running.
A timestamp in RFC3339 UTC ""Zulu"" format, with nanosecond resolution and up to nine fractional digits. Examples:
|
target
|
Output only. Server-defined resource path for the target of the operation.
|
verb
|
Output only. Name of the verb executed by the operation.
|
statusDetail
|
Output only. Human-readable status of the operation, if any.
|
cancelRequested
|
Output only. Identifies whether the user has requested cancellation of the operation. Operations that have been cancelled successfully have [Operation.error][] value with a
|
apiVersion
|
Output only. API version used to start the operation.",OperationMetadata | Cloud TPU | Google Cloud,
id,url,body,title,description
113,https://cloud.google.com/tpu/docs/regions-zones,"TPU regions and zones
Overview
The main differences between
[TPU types](/tpu/docs/types-topologies) are price,
performance, memory capacity, and zonal availability.
Google Cloud uses regions, subdivided into zones, to define the geographic
location of physical compute resources. For example, the
us-central1 region denotes a region near the geographic center of
the United States. When you create a TPU node, you specify the zone in which you
want to create it. See the Compute Engine
[Global, regional, and zonal
resources](/compute/docs/regions-zones/global-regional-zonal-resources) document
for more information about regional and zonal resources.
You can create TPU configurations in the zones shown in the following table.
US
|TPU type (v2)
|TPU v2 cores
|Available zones
|v2-8
|8
|
us-central1-b
us-central1-c
us-central1-f
|v2-32
|32
|
us-central1-a
|v2-128
|128
|
us-central1-a
|v2-256
|256
|
us-central1-a
|v2-512
|512
|
us-central1-a
|TPU type (v3)
|TPU v3 cores
|Available zones
|v3-8
|8
|
us-central1-a
us-central1-b
us-central1-f
|TPU type (v4)
|TPU v4 chips
|Available zones
|
us-central2-b
[All v5 configurations](/tpu/docs/supported-tpu-configurations#tpu-v5e-config)
us-west4-a
us-east5-b
Europe
|TPU type (v2)
|TPU v2 cores
|Available zones
|v2-8
|8
|
europe-west4-a
|v2-32
|32
|
europe-west4-a
|v2-128
|128
|
europe-west4-a
|v2-256
|256
|
europe-west4-a
|v2-512
|512
|
europe-west4-a
|TPU type (v3)
|TPU v3 cores
|Available zones
|v3-8
|8
|
europe-west4-a
|v3-32
|32
|
europe-west4-a
|v3-64
|64
|
europe-west4-a
|v3-128
|128
|
europe-west4-a
|v3-256
|256
|
europe-west4-a
|v3-512
|512
|
europe-west4-a
|v3-1024
|1024
|
europe-west4-a
|v3-2048
|2048
|
europe-west4-a
Asia Pacific
|TPU type (v2)
|TPU v2 cores
|Available zones
|v2-8
|8
|
asia-east1-c
TPU types with higher numbers of chips or cores are available only in limited quantities. TPU types with lower chip or core counts are more likely to be available.
Calculating price and performance tradeoffs
To decide which TPU type you want to use, you can do experiments using a
[
Cloud TPU tutorial](/tpu/docs/tutorials) to train a model that is
similar to your application.
Run the tutorial for 5 - 10% of the number of steps you will use to run the full
training on a
v2-8, or a
v3-8 TPU type. The result
tells you how long it takes to run that number of steps for that model on each
TPU type.
Because performance on TPU types scales linearly, if you know how long it takes
to run a task on a
v2-8 or
v3-8 TPU type, you can
estimate how much you can reduce task time by running your model on a larger TPU
type with more chips or cores.
For example, if a
v2-8 TPU type takes 60 minutes to 10,000 steps, a
v2-32 node should take approximately 15 minutes to perform the same
task.
When you know the approximate training time for your model on a few different TPU types, you can weigh the VM/TPU cost against training time to help you decide your best price and performance tradeoff.
To determine the difference in cost between the different TPU types for
Cloud TPU and the associated Compute Engine VM, see the
[TPU pricing page](/tpu/docs/pricing).
Specifying the TPU type
Regardless of which framework you are using, you specify a
v2 or
v3
TPU type with the
accelerator-type parameter when you
[launch a TPU](/tpu/docs/managing-vm-tpu-resources). For a TPU v4 or later, you
can specify the type and size using either
AcceleratorType or
AcceleratorConfig. For more information, see
[TPU
configurations](/tpu/docs/supported-tpu-configurations). The TPU type command
depends on whether you are using TPU VMs or TPU Nodes. Example commands are
shown in [Managing TPUs](/tpu/docs/managing-tpus-tpu-vm).
What's next
- To see pricing for TPUs in each region, see the
[Pricing](/tpu/pricing)page.
- Learn more about TPU architecture in the
[System Architecture](/tpu/docs/system-architecture-tpu-vm)page.
- See
[When to use TPUs](/tpu/docs/tpus#when_to_use_tpus)to learn about the types of models that are well suited to Cloud TPU.",TPU regions and zones | Google Cloud,
id,url,body,title,description
102,https://cloud.google.com/tpu/docs/reference/rest/v1alpha1/projects.locations.nodes/stop,"Method: projects.locations.nodes.stop
Stay organized with collections
Save and categorize content based on your preferences.
Stops a node. This operation is only available with single TPU nodes.
HTTP request
POST https://tpu.googleapis.com/v1alpha1/{name=projects/*/locations/*/nodes/*}:stop
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
string
The resource name.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[Operation](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse#Operation)
Authorization scopes
Requires one of the following OAuth scopes:
-
https://www.googleapis.com/auth/cloud.tpu
-
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Method: projects.locations.nodes.stop | Cloud TPU | Google Cloud,
id,url,body,title,description
64,https://cloud.google.com/tpu/docs/reference/rest/v2alpha1/projects.locations.operations/cancel,"Starts asynchronous cancellation on a long-running operation. The server makes a best effort to cancel the operation, but success is not guaranteed. If the server doesn't support this method, it returns
google.rpc.Code.UNIMPLEMENTED. Clients can use
or other methods to check whether the cancellation succeeded or whether the operation completed despite cancellation. On successful cancellation, the operation is not deleted; instead, it becomes an operation with an
[Operations.GetOperation](/tpu/docs/reference/rest/v1alpha1/projects.locations.operations/get#google.longrunning.Operations.GetOperation)
value with a
[Operation.error](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse#Operation.FIELDS.error)
of 1, corresponding to
[google.rpc.Status.code](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse#Status.FIELDS.code)
Code.CANCELLED.
HTTP request
POST https://tpu.googleapis.com/v2alpha1/{name=projects/*/locations/*/operations/*}:cancel
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
The name of the operation resource to be cancelled.
Request body
The request body must be empty.
Response body
If successful, the response body is empty.
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).",Method: projects.locations.operations.cancel | Cloud TPU | Google Cloud,
id,url,body,title,description
85,https://cloud.google.com/tpu/docs/autocheckpoint,"Cloud TPU Autocheckpoint [Public Preview]
Overview
Historically, when a TPU VM requires
[maintenance](/tpu/docs/maintenance-events),
the procedure is initiated immediately, without leaving time for users to
perform progress-preserving actions such as saving a checkpoint. This is
shown in Figure 1(a).
Fig. 1. Illustration of the Autocheckpoint feature: (a) Without Autocheckpoint, the training progress from the last checkpoint is lost when there is an upcoming maintenance event. (b) With Autocheckpoint, the training progress since the last checkpoint can be preserved when there is an upcoming maintenance event.
You can use Autocheckpoint (Figure 1(b)) to preserve training progress by configuring your code to save a non-scheduled checkpoint when a maintenance event occurs. When a maintenance event occurs, progress since the last checkpoint is automatically saved. The feature works on both single slices and Multislice.
The Autocheckpoint feature works with frameworks that can capture
SIGTERM and subsequently save a checkpoint. The supported frameworks include
[MaxText](https://github.com/google/maxtext),
[Pax](https://github.com/google/paxml),
and
JAX with [Orbax](https://github.com/google/orbax).
Support for additional frameworks will be announced as they become available.
Only TPUs (v2-v4, and v5e) created through the Cloud TPU API can use this feature for now. Support for TPUs in GKE will be announced when it becomes available.
Using Autocheckpoint
Autocheckpoint functionality is disabled by default. When you create a
TPU or a
[queued resource](/tpu/docs/queued-resources),
you can enable it by adding the
--autocheckpoint-enabled flag when provisioning
the TPU.
With the feature enabled, Cloud TPU
performs the following steps once it receives notification of a
maintenance event:
- Capture SIGTERM sent to the process using the TPU device,
- Waits until the process exits, or 5 minutes have elapsed, whichever comes first, and performs maintenance on the impacted slices.
Note that the infrastructure used by Autocheckpoint is ML framework-independent. Any ML framework can support Autocheckpoint provided it can capture the SIGTERM signal and initiate a checkpointing process.
In the application code, you need to enable the Autocheckpoint
capabilities provided by the ML framework. In Pax, for example,
this means enabling command-line flags when launching the
training (see
[the autocheckpoint Quickstart with Pax](#pax-single-slice)).
Behind the scenes, the frameworks save a
non-scheduled checkpoint when a SIGTERM is received
and the impacted TPU VM goes through maintenance when the TPU is no longer
in use.
Quickstart: Autocheckpoint with MaxText
[MaxText](https://github.com/google/maxtext) is a ""high performance,
arbitrarily scalable, open source, well-tested LLM written in pure Python/JAX
targeting Cloud TPUs"".
MaxText contains all the necessary setup to use the Autocheckpoint
feature.
The MaxText README describes two ways to run MaxText at scale:
- Using
, recommended for experimentation
[multihost_runner.py](https://github.com/google/maxtext/blob/main/multihost_runner.py)
- Using
, recommended for production
[multihost_job.job](https://github.com/google/maxtext/blob/main/multihost_job.py)
When using
multihost_runner.py, the only change required
is to set the
autocheckpoint-enabled flag when provisioning
the queued resource. When using
multihost_job.py, the only change required is to specify the
ENABLE_AUTOCHECKPOINT=true command line flag when launching the job.
Quickstart: Autocheckpoint with Pax on single slices
In this section, we provide an example of how to set up and use Autocheckpoint with Pax on a single slice. With the appropriate setup:
- A checkpoint will be saved when a maintenance event occurs.
- Cloud TPU will perform maintenance on the affected TPU VM(s) after the checkpoint is saved.
- When Cloud TPU completes maintenance, you can use the TPU VM as usual.
Use the
autocheckpoint-enabledflag when creating the TPU VM or queued resource.
For example:
PROJECT=your-gcp-project-name ZONE=zone-you-want-to-use NODE_ID=your-node-id ACCELERATOR_TYPE=your-accelerator-type gcloud config set project $PROJECT gcloud config set compute/zone $ZONE
gcloud alpha compute tpus tpu-vm create $NODE_ID \ --accelerator-type $ACCELERATOR_TYPE \ --version tpu-ubuntu2204-base \ --autocheckpoint-enabled
Install Pax on a single slice
The Autocheckpoint feature works on Pax versions >= 1.1.0. On the TPU VMs, install
jax[tpu]and the latest
paxml:
pip install paxml && pip install jax[tpu] -f https://storage.googleapis.com/jax-releases/libtpu_releases.html
Launch the training with the appropriate configuration
The following example shows how to configure the
model to save checkpoints triggered by Autocheckpoint to a Google Cloud Storage bucket:
[LmCloudSpmd2B](https://github.com/google/paxml/blob/b18a8d109ec45bbd7e4bcab04e3e53c2e65f3035/paxml/tasks/lm/params/lm_cloud.py#L166)
JOB_LOG_DIR=gs://your-storage-bucket { python3 .local/lib/python3.10/site-packages/paxml/main.py --jax_fully_async_checkpoint=1 \ --exit_after_ondemand_checkpoint=1 \ --exp=tasks.lm.params.lm_cloud.LmCloudSpmd2B \ --job_log_dir=$JOB_LOG_DIR; } 2>&1 | tee pax_logs.txt
Note the two flags that are passed to the command:
jax_fully_async_checkpoint: With this flag on,
will be used. The
[orbax.checkpoint.AsyncCheckpointer](https://github.com/google/orbax/blob/986f23ff728c0ed5273f17662fa49011a08342bc/checkpoint/orbax/checkpoint/async_checkpointer.py#L43C53-L43C53)
AsyncCheckpointerclass automatically saves a checkpoint when the training script receives a SIGTERM signal.
exit_after_ondemand_checkpoint: With this flag on, the TPU processes exits after the Autocheckpoint is successfully saved, which triggers the maintenance to be performed immediately. If you do not use this flag, the training will continue after the checkpoint is saved and Cloud TPU will wait for a timeout to occur (5 minutes) before performing the required maintenance..
-
Quickstart: Autocheckpoint with Pax on Multislice
Autocheckpoint works not only for single slices, but also
for
[Multislice](https://cloud.google.com/tpu/docs/multislice-introduction).
This section
details the steps needed to use Autocheckpoint with Multislice.
Specify Autocheckpoint during queued resource creation.
A Multislice environment can only be provisioned through a queued resource request. Similar to the single-slice case, use the
autocheckpoint-enabledflag in the call to create a queued resource.
QR_ID=your-qr-id NODE_COUNT=your-node-count ACCELERATOR_TYPE=your-accelerator-type gcloud alpha compute tpus queued-resources create $QR_ID \ --node-count $NODE_COUNT \ --accelerator-type $ACCELERATOR_TYPE \ --runtime-version tpu-ubuntu2204-base \ --autocheckpoint-enabled
Refer to the
[Multislice User Guide](/tpu/docs/multislice-introduction)for details on all available options. Once the queued resource request is created and in the
ACTIVEstate, follow the next steps to run Pax with Autocheckpoint.
Install Pax on all VMs in the Multislice environment.
On the TPU VMs, install
jax[tpu]and the latest
paxmlon all of the TPU VMs in your Multislice environment:
pip install paxml && pip install jax[tpu] -f https://storage.googleapis.com/jax-releases/libtpu_releases.html
Launch the training with the appropriate configuration
This example shows how to configure the model
[for Autocheckpoint when training in a Multislice environment. Before running the training script, set DCN_MESH_SHAPE to [2, 1, 1] as shown in the following code:](https://github.com/google/paxml/blob/b18a8d109ec45bbd7e4bcab04e3e53c2e65f3035/paxml/tasks/lm/params/lm_cloud.py#L166)
LmCloudSpmd2B
@experiment_registry.register class LmCloudSpmd2B(LmCloudSpmd): """"""SPMD model with 2B params. Global batch size = 2 * 2 * 1 * 32 = 128 """""" PERCORE_BATCH_SIZE = 8 NUM_LAYERS = 18 MODEL_DIMS = 3072 HIDDEN_DIMS = MODEL_DIMS * 4 CHECKPOINT_POLICY = layers.AutodiffCheckpointType.SAVE_NOTHING ICI_MESH_SHAPE = [1, 4, 1] DCN_MESH_SHAPE = [2, 1, 1]
When launching the training, in addition to the command line flags discussed in the single-slice case, three more are required:
num_hosts: the total number of hosts. In this case, it is 2.
host_index: the index of the host launching the training. It varies from 0 to
N-1where
Nis the total number of hosts.
server_addr: the IP address of worker 0 of node 0, with an unused port (for example, 8476). To find this information, use
hostname -ion worker 0 of node 0.
-
Autocheckpoint with Orbax
The Autocheckpoint feature is not limited to MaxText or Pax. Any framework
that can capture the SIGTERM signal and initiate a
checkpointing process works with the infrastructure provided by Autocheckpoint.
[Orbax](https://github.com/google/orbax), a namespace that provides
common utility libraries for JAX users, provides these capabilities.
As explained in the
[Orbax documentation](https://github.com/google/orbax/blob/986f23ff728c0ed5273f17662fa49011a08342bc/docs/preemption_checkpointing.ipynb),
these capabilities are enabled by default for users
of
orbax.checkpoint.CheckpointManager. The
save method
that is called after every step automatically checks whether a maintenance
event is impending, and if so, saves a checkpoint even if the step number
is not a multiple of
save_interval_steps.
The
[GitHub documentation](https://github.com/google/orbax/blob/986f23ff728c0ed5273f17662fa49011a08342bc/docs/preemption_checkpointing.ipynb)
also illustrates how to make the training exit after saving an
Autocheckpoint, with a modification in the user code.",Cloud TPU Autocheckpoint [Public Preview] | Google Cloud,
id,url,body,title,description
25,https://cloud.google.com/tpu/docs/reference/rest/v2/projects.locations/get,"Method: projects.locations.get
Stay organized with collections
Save and categorize content based on your preferences.
Gets information about a location.
HTTP request
GET https://tpu.googleapis.com/v2/{name=projects/*/locations/*}
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
string
Resource name for the location.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[Location](/tpu/docs/reference/rest/Shared.Types/ListLocationsResponse#Location)
Authorization scopes
Requires one of the following OAuth scopes:
-
https://www.googleapis.com/auth/cloud.tpu
-
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Method: projects.locations.get | Cloud TPU | Google Cloud,
id,url,body,title,description
21,https://cloud.google.com/tpu/docs/tpus-in-gke,"TPUs in GKE introduction
Google Kubernetes Engine (GKE) customers can now create
Kubernetes node pools containing TPU v4 and v5e Pods. A TPU Pod is a group of
TPU devices connected by high-speed interconnects. For workloads that don't
require a full TPU Pod, you can use a subset of a full TPU Pod called a
TPU slice. Like full TPU Pods, each TPU device in a slice has its own TPU VM.
We refer to a TPU VM and its connected device as a host or TPU node. For
more information about TPU Pods, see
[System Architecture](/tpu/docs/system-architecture-tpu-vm).
Since the term Pod used in the context of GKE typically
means a
[Kubernetes Pod](https://kubernetes.io/docs/concepts/workloads/pods/),
to avoid any confusion, we will always refer to a collection of one or more TPU
devices as a slice.
When you work with GKE you first have to create a
[GKE cluster](/kubernetes-engine/docs/how-to/creating-a-zonal-cluster).
You then
[add node pools](/kubernetes-engine/docs/how-to/node-pools) to your
cluster. GKE node pools are collections of VMs that share
the same attributes. For TPU workloads, node pools consist of TPU VMs.
Node pool types
GKE supports two types of TPU node pools:
Multi-host TPU slice node pool
A multi-host TPU slice node pool is a node pool that contains two or more
interconnected TPU VMs. Each VM has a TPU device connected to it. The TPUs in
a multi-host slice are connected over a high speed interconnect (ICI). A
multi-host TPU slice node pool is immutable. Once a multi-host slice node pool
is created, you cannot add nodes to it. For example,
you cannot create a
v4-32 node pool and then later add an additional Kubernetes
node (TPU VM) to the node pool. To add an additional TPU slice to a
GKE cluster, you must create a new node pool.
The hosts in a multi-host TPU slice node pool are treated as a single atomic unit. If GKE is unable to deploy one node in the slice, all nodes in the slice will fail to be deployed.
If a node within a multi-host TPU slice needs to be repaired, GKE will shutdown all TPU VMs (nodes) in the slice, forcing all GKE Pods in the workload to be evicted. Once all TPU VMs in the slice are up and running, the GKE Pods can be scheduled on the TPU VMs in the new slice.
The following diagram shows an example of a TPU v5litepod-16 (v5e) multi-host slice. This slice has four TPU VMs. Each TPU VM has four TPU v5e chips connected with high-speed interconnects (ICI), and each TPU v5e chip has one TensorCore.
The following diagram shows a GKE cluster containing one
TPU
v5litepod-16 (v5e) slice (topology: 4x4) and one TPU
v5litepod-8 (v5e)
slice (topology: 2x4):
For an example of running a workload on a multi-host TPU slice, see
[Run workload on a multi-host TPU slice](/kubernetes-engine/docs/how-to/tpus#tpu_chips_node_pool).
Single-host TPU slice node pools
A single-host slice node pool is a node pool that contains one or more
independent TPU VMs. Each of these VMs has a TPU device connected to it. While
the VMs within a single-host slice node pool can communicate over Data Center
Network (DCN), the TPUs attached to the VMs are not interconnected.
The following diagram shows an example of a single-host TPU slice with seven
v4-8 machines:
For an example of running a workload on a single-host TPU slice, see
[Run your workloads on TPU nodes](/kubernetes-engine/docs/how-to/tpus#tpu_chips_vm).
TPU machine types for GKE node pools
Before creating node pools, you need to choose the TPU version and size of the
TPU slice your workload requires. TPU v4 is supported in GKE
version
1.26.1-gke.1500 and later, v5e in GKE
version
1.27.2-gke.2100 and later, and v5p in GKE version
1.28.3-gke.1024000 and later.
For more information about the hardware specifications of the different TPU
versions, see
[System architecture](/tpu/docs/system-architecture-tpu-vm). When
creating a TPU node pool, select a TPU slice size (a TPU topology) based on the
size of your model and how much memory it requires. The machine type you specify
when creating your node pools depends on the version and size of your slices.
v5e
The following are the TPU v5e machine types and topologies that are supported for training and inference use cases:
|Machine Type
|Topology
|Number of TPU chips
|Number of VMs
|Recommended use case
|
ct5lp-hightpu-1t
|1x1
|1
|1
|Training, single-host inference
|
ct5lp-hightpu-4t
|2x2
|4
|1
|Training, single-host inference
|
ct5lp-hightpu-8t
|2x4
|8
|1
|Training, single-host inference
|
ct5lp-hightpu-4t
|2x4
|8
|2
|Training, multi-host inference
|
ct5lp-hightpu-4t
|4x4
|16
|4
|Large-scale training, multi-host inference
|
ct5lp-hightpu-4t
|4x8
|32
|8
|Large-scale training, multi-host inference
|
ct5lp-hightpu-4t
|8x8
|64
|16
|Large-scale training, multi-host inference
|
ct5lp-hightpu-4t
|8x16
|128
|32
|Large-scale training, multi-host inference
|
ct5lp-hightpu-4t
|16x16
|256
|64
|Large-scale training, multi-host inference
Cloud TPU v5e is a combined training and inference product. Training jobs are
optimized for throughput and availability while inference jobs are optimized for
latency. For more information see
[v5e Training accelerator types](/tpu/docs/v5e-training#accelerator-types)
and [v5e Inference accelerator types](/tpu/docs/v5e-inference#accelerator-types).
TPU v5e machines are available in
us-west4-a,
us-east5-b and
us-east1-c.
Your GKE clusters must run control plane version
1.27.2-gke.2100 or later. For more information about v5e, see
[Cloud TPU v5e training](/tpu/docs/v5e-training).
Machine type comparison:
|Machine Type
|ct5lp-hightpu-1t
|ct5lp-hightpu-4t
|ct5lp-hightpu-8t
|Number of v5e chips
|1
|4
|8
|Number of vCPUs
|24
|112
|224
|RAM (GB)
|48
|192
|384
|Number of NUMA nodes
|1
|1
|2
|Likelihood of preemption
|High
|Medium
|Low
To make space for VMs with more chips, the GKE scheduler may preempt and reschedule VMs with fewer chips. So 8-chip VMs are likely to preempt 1 and 4-chip VMs.
v4 and v5p
The following are the TPU v4 and v5p machine types:
|Machine type
|Number of vCPUs
|Memory (GB)
|Number of
ct4p-hightpu-4t
ct5p-hightpu-4t
When creating a TPU v4 slice, use the
ct4p-hightpu-4t machine type which has
one host and contains 4 chips. See
[v4 topologies](/tpu/docs/types-topologies#small_v4_topologies)
and [TPU system architecture](/tpu/docs/system-architecture-tpu-vm) for more
information. TPU v4 Pod machines types are available in
us-central2-b. Your
GKE clusters must run control plane version
1.26.1-gke.1500
or later.
When creating a TPU v5p slice, use the
ct5p-hightpu-4t machine type
which has one host and contains 4 chips. TPU v5p Pod machine types are available
in
us-west4-a and
us-east5-b. Your GKE clusters must
run control plane version
1.28.3-gke.1024000 or later. For more information
about v5p, see
[v5p training introduction](/tpu/docs/v5p-training).
Known issues and limitations
- Maximum number of Docker pods: You can run a maximum of 256 Docker pods in a single TPU VM.
- SPECIFIC reservations only: When using TPUs in GKE,
SPECIFICis the only supported value for the
--reservation-affinityflag of the
gcloud container node-pools createcommand.
- Only the Spot VMs variant of preemptible TPUs are supported:
[Spot VMs](/kubernetes-engine/docs/concepts/spot-vms)are similar to preemptible VMs and are subject to the same availability limitations, but don't have a 24h maximum duration.
- Autopilot clusters are not supported: TPUs are not available in
[GKE Autopilot](/kubernetes-engine/docs/concepts/autopilot-overview)clusters.
- No cost allocation support:
[GKE cost allocation](/kubernetes-engine/docs/how-to/cost-allocations)and [usage metering](/kubernetes-engine/docs/how-to/cluster-usage-metering)don't include any data about the usage or costs of TPUs.
- Autoscaler may calculate capacity: Cluster autoscaler might calculate capacity incorrectly for new TPU nodes before those nodes are available. Cluster autoscaler might then perform additional scale up and as a result create more nodes than needed. Cluster autoscaler will scale down additional nodes, if they are not needed, after regular scale down operation.
- Autoscaler cancels scale up: Cluster autoscaler cancels scaling up of TPU node pools that remain in waiting status for more than 15 minutes. Cluster Autoscaler will retry such scale up operations later. This behavior might reduce TPU obtainability for customers who don't use reservations.
- Taint may prevent scale down: Non-TPU workloads that have a toleration for the TPU taint may prevent scale down of the node pool if they are recreated during draining of the TPU node pool.
Ensure sufficient TPU and GKE quotas
You may need to increase certain GKE-related quotas in the regions where your resources are created.
The following quotas have default values that will likely need to be increased:
- Persistent Disk SSD (GB) quota: The boot disk of each Kubernetes node requires 100GB by default. Therefore, this quota should be set at least as high as (the maximum number of GKE nodes you anticipate creating) * 100GB.
- In-use IP addresses quota: Each Kubernetes node consumes one IP address. Therefore, this quota should be set at least as high as the maximum number of GKE nodes you anticipate creating.
To request an increase in quota, see
[Request higher quota](/docs/quota_detail/view_manage#requesting_higher_quota).
For more information about the types of TPU quotas, see [TPU Quota](/tpu/docs/quota).
It may take a few days for your quota increase requests to be approved. If you experience any difficulty getting your quota increase requests approved within a few days, contact your Google Account team.
Migrate your TPU reservation
If you don't plan to use an existing TPU reservation with TPUs in GKE,
skip this section and go to
[Create a Google Kubernetes Engine cluster](#create-cluster).
In order to use reserved TPUs with GKE, you must first migrate your TPU reservation to a new Compute Engine-based reservation system.
There are several important things to know about this migration:
- TPU capacity migrated to the new Compute Engine-based reservation
system cannot be used with the Cloud TPU
[Queued Resource API](/tpu/docs/queued-resources). If you intend to use TPU queued resources with your reservation, then you will need to migrate only a portion of your TPU reservation to the new Compute Engine-based reservation system.
- No workloads can be actively running on the TPUs when they are migrated to the new Compute Engine-based reservation system.
- Select a time to perform the migration, and work with your Google Cloud account team to schedule the migration. The migration time window needs to be during business hours (Monday - Friday, 9am-5pm Pacific Time).
Create a Google Kubernetes Engine cluster
See
[Create a cluster](/kubernetes-engine/docs/how-to/tpus#create-cluster) in
the Google Kubernetes Engine documentation.
Create a TPU node pool
See
[Create a node pool](/kubernetes-engine/docs/how-to/tpus#create-node-pool)
in the Google Kubernetes Engine documentation.
Running without privileged mode
If you want to reduce the permission scope on your container see
[TPU privilege mode](/kubernetes-engine/docs/how-to/tpus#privileged-mode).
Run workloads on TPU nodes
See
[Run your workloads on TPU nodes](/kubernetes-engine/docs/how-to/tpus#run)
in the Google Kubernetes Engine documentation.
Node selectors
In order for Kubernetes to schedule your workload on TPU nodes, you must specify two selectors for each TPU node in your Google Kubernetes Engine manifest:
- Set
cloud.google.com/gke-accelerator-typeto
tpu-v5-lite-podsliceor
tpu-v4-podslice.
- Set
cloud.google.com/gke-tpu-topologyto the TPU topology of the TPU node.
The
[Training workloads](#training-workloads) and [Inference workloads](#inference-workloads)
sections contain example manifests that illustrate using these node selectors.
Workload scheduling considerations
TPUs have unique characteristics that require special workload scheduling and
management in Kubernetes. For more information, see
[Workload scheduling considerations](/kubernetes-engine/docs/how-to/tpus#workload-scheduling)
in the GKE documentation.
TPU node repair
If a TPU node in a multi-host TPU slice node pool is unhealthy, the entire node
pool is recreated. For more information, see
[Node auto repair](/kubernetes-engine/docs/how-to/tpus#node_auto_repair)
in the GKE documentation.
Multislice - going beyond a single slice
You can aggregate smaller slices together in a multislice to handle larger
training workloads. For more information, see
[Cloud TPU Multislice](/tpu/docs/multislice-introduction).
Training workload tutorials
These tutorials focus on training workloads on a multi-host TPU slice (for example, 4 v5e machines). They cover the following models:
- Hugging Face FLAX Models: Train Diffusion on Pokmon
- PyTorch/XLA: GPT2 on WikiText
Download tutorial resources
Download the tutorial Python scripts and YAML specs for each pre-trained model with the following command:
git clone https://github.com/GoogleCloudPlatform/ai-on-gke.git
Create & connect to cluster
Create a regional GKE standard cluster, so the Kubernetes
control plane is replicated in three zones, providing higher availability.
Create your cluster in
us-west4,
us-east1 or
us-central2 depending upon which
TPU version you are using. For more information about TPUs and zones, see
[Cloud TPU regions and zones](/tpu/docs/regions-zones).
The following command creates a new GKE regional cluster subscribed to the rapid release channel with a node pool that initially contains one node per zone. The command also enables Workload Identity and Cloud Storage FUSE CSI driver features on your cluster because the example inference workloads in this guide use Cloud Storage buckets to store pre-trained models.
gcloud container clusters create cluster-name \ --region your-region \ --release-channel rapid \ --num-nodes=1 \ --workload-pool=project-id.svc.id.goog \ --addons GcsFuseCsiDriver
To enable Workload Identity and Cloud Storage FUSE CSI driver features for existing clusters, run the following command:
gcloud container clusters update cluster-name \ --region your-region \ --update-addons GcsFuseCsiDriver=ENABLED \ --workload-pool=project-id.svc.id.goog
The example workloads are configured with the following assumptions:
- the node pool is using
tpu-topology=4x4with four nodes
- the node pool is using
machine-type
ct5lp-hightpu-4t
Run the following command to connect to your newly created cluster:
gcloud container clusters get-credentials cluster-name \ --location=cluster-region
Hugging Face FLAX Models: Train Diffusion on Pokmon
This example trains the Stable Diffusion model from HuggingFace using the
[Pokmon](https://huggingface.co/datasets/lambdalabs/pokemon-blip-captions)
dataset.
The Stable Diffusion model is a latent text-to-image model that generates photo-realistic images from any text input. For more information about Stable Diffusion, see:
Create Docker image
The Dockerfile is located under the folder
ai-on-gke/gke-tpu-examples/training/diffusion/. Run the following commands to
build and push the Docker image.
cd ai-on-gke/gke-tpu-examples/training/diffusion/ docker build -t gcr.io/project-id/diffusion:latest . docker push gcr.io/project-id/diffusion:latest
Deploy workload
Create a file with the following content and name it
tpu_job_diffusion.yaml.
Fill in the image field with the image that you just created.
apiVersion: v1
kind: Service
metadata:
name: headless-svc
spec:
clusterIP: None
selector:
job-name: tpu-job-diffusion
---
apiVersion: batch/v1
kind: Job
metadata:
name: tpu-job-diffusion
spec:
backoffLimit: 0
# Completions and parallelism should be the number of chips divided by 4.
# (e.g. 4 for a v5litepod-16)
completions: 4
parallelism: 4
completionMode: Indexed
template:
spec:
subdomain: headless-svc
restartPolicy: Never
nodeSelector:
cloud.google.com/gke-tpu-accelerator: tpu-v5-lite-podslice
cloud.google.com/gke-tpu-topology: 4x4
containers:
- name: tpu-job-diffusion
image: gcr.io/<var>project-name</var>/diffusion:latest
ports:
- containerPort: 8471 # Default port using which TPU VMs communicate
- containerPort: 8431 # Port to export TPU usage metrics, if supported
securityContext:
privileged: true
command:
- bash
- -c
- |
cd examples/text_to_image
python3 train_text_to_image_flax.py --pretrained_model_name_or_path=duongna/stable-diffusion-v1-4-flax --dataset_name=lambdalabs/pokemon-blip-captions --resolution=128 --center_crop --random_flip --train_batch_size=4 --mixed_precision=fp16 --max_train_steps=1500 --learning_rate=1e-05 --max_grad_norm=1 --output_dir=sd-pokemon-model
resources:
requests:
google.com/tpu: 4
limits:
google.com/tpu: 4
Then deploy it using:
kubectl apply -f tpu_job_diffusion.yaml
Clean-up
After your Job finishes running you can delete it using:
kubectl delete -f tpu_job_diffusion.yaml
PyTorch/XLA: GPT2 on WikiText
This tutorial shows how to run GPT2 on v5e TPUs using
[HuggingFace](https://github.com/huggingface/transformers.git)
on PyTorch/XLA using the [wikitext dataset](https://huggingface.co/datasets/wikitext).
Create Docker image
The Dockerfile is located under the folder
ai-on-gke/gke-tpu-examples/training/gpt/.
Run the following commands to build and push the Docker image.
cd ai-on-gke/gke-tpu-examples/training/gpt/ docker build -t gcr.io/project-id/gpt:latest . docker push gcr.io/project-id/gpt:latest
Deploy workload
Copy the following YAML and save it in a file called
tpu_job_gpt.yaml. Fill in
the image field with the image that you just created.
kind: Service
metadata:
name: headless-svc
spec:
clusterIP: None
selector:
job-name: tpu-job-gpt
---
apiVersion: batch/v1
kind: Job
metadata:
name: tpu-job-gpt
spec:
backoffLimit: 0
# Completions and parallelism should be the number of chips divided by 4.
# (for example, 4 for a v5litepod-16)
completions: 4
parallelism: 4
completionMode: Indexed
template:
spec:
subdomain: headless-svc
restartPolicy: Never
nodeSelector:
cloud.google.com/gke-tpu-accelerator: tpu-v5-lite-podslice
cloud.google.com/gke-tpu-topology: 4x4
containers:
- name: tpu-job-gpt
image: gcr.io/<var>project-name</var>/gpt:latest
ports:
- containerPort: 8479
- containerPort: 8478
- containerPort: 8477
- containerPort: 8476
- containerPort: 8431 # Port to export TPU usage metrics, if supported.
securityContext:
privileged: true
env:
- name: PJRT_DEVICE
value: 'TPU_C_API'
- name: XLA_USE_BF16
value: '1'
command:
- bash
- -c
- |
numactl --cpunodebind=0 python3 -u examples/pytorch/xla_spawn.py --num_cores 4 examples/pytorch/language-modeling/run_clm.py --num_train_epochs 3 --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --per_device_train_batch_size 16 --per_device_eval_batch_size 16 --do_train --do_eval --output_dir /tmp/test-clm --overwrite_output_dir --config_name my_config_2.json --cache_dir /tmp --tokenizer_name gpt2 --block_size 1024 --optim adafactor --adafactor true --save_strategy no --logging_strategy no --fsdp ""full_shard"" --fsdp_config fsdp_config.json
resources:
requests:
google.com/tpu: 4
limits:
google.com/tpu: 4
Deploy the workflow using:
kubectl apply -f tpu_job_gpt.yaml
Clean-up
After your job finishes running you can delete it using:
kubectl delete -f tpu_job_gpt.yaml
Tutorial: Single-Host inference workloads
This tutorial shows how to run a single-host inference workload on GKE v5e TPUs for pre-trained models with JAX, TensorFlow, and PyTorch. At a high level, there are four separate steps to perform on the GKE cluster:
Create a Cloud Storage bucket and set up access to the bucket. You use a Cloud Storage bucket is used to store the pre-trained model.
Download and convert a pre-trained model into a TPU-compatible model. Apply a GKE Pod that downloads the pre-trained model, uses the Cloud TPU Converter and stores the converted models into a Cloud Storage bucket using the Cloud Storage FUSE CSI driver. The Cloud TPU Converter doesn't require specialized hardware. This tutorial shows you how to download the model and run the Cloud TPU Converter in the CPU node pool.
Launch the server for the converted model. Apply a
[Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/)that serves the model using a server framework backed by the volume stored in the ReadOnlyMany (ROX) Persistent Volume. The deployment replicas must be run on a v5e Pod TPU node with one Kubernetes Pod per node.
Deploy a load balancer to test the model server. The server is exposed to external requests using the
[LoadBalancer Service](https://kubernetes.io/docs/concepts/services-networking/service/). A Python script has been provided with an example request to test out the model server.
The following diagram shows how requests are routed by the Load Balancer.
Server deployment examples
These example workloads are configured with the following assumptions:
- The cluster is running with a TPU v5 node pool with 3 nodes
- The node pool is using machine type
ct5lp-hightpu-1twhere:
- topology is 1x1
- number of TPU chips is 1
The following GKE manifest defines a single host server Deployment.
apiVersion: apps/v1
kind: Deployment
metadata:
name: bert-deployment
spec:
selector:
matchLabels:
app: tf-bert-server
replicas: 3 # number of nodes in node pool
template:
metadata:
annotations:
gke-gcsfuse/volumes: ""true""
labels:
app: tf-bert-server
spec:
nodeSelector:
cloud.google.com/gke-tpu-topology: 1x1 # target topology
cloud.google.com/gke-tpu-accelerator: tpu-v5-lite-podslice # target version
containers:
- name: serve-bert
image: us-docker.pkg.dev/cloud-tpu-images/inference/tf-serving-tpu:2.13.0
securityContext:
privileged: true
env:
- name: MODEL_NAME
value: ""bert""
volumeMounts:
- mountPath: ""/models/""
name: bert-external-storage
ports:
- containerPort: 8500
- containerPort: 8501
- containerPort: 8431 # Port to export TPU usage metrics, if supported.
resources:
requests:
google.com/tpu: 1 # TPU chip request
limits:
google.com/tpu: 1 # TPU chip request
volumes:
- name: bert-external-storage
persistentVolumeClaim:
claimName: external-storage-pvc
If you are using a different number of nodes in your TPU node pool, change the
replicas field to the number of nodes.
If you are using a different machine type:
- Set
cloud.google.com/gke-tpu-topologyto the topology for the
[machine type](#tpu-machine-types)you are using.
- Set both
google.com/tpufields under
resourcesto match the number of chips for the corresponding
[machine type](#tpu-machine-types).
Setup
Download the tutorial Python scripts and YAML manifests using the following command:
git clone https://github.com/GoogleCloudPlatform/ai-on-gke.git
Go to the
single-host-inference directory:
cd ai-on-gke/gke-tpu-examples/single-host-inference/
Set up Python environment
The Python scripts you use in this tutorial require Python version 3.9 or greater.
Remember to install the
requirements.txt for each tutorial before running the
Python test scripts.
If you don't have the proper Python setup in your local environment, you can
use
[Cloud Shell](https://console.cloud.google.com/?cloudshell=true) to download and run the
Python scripts in this tutorial.
Set up the cluster
Create a cluster using the
e2-standard-4machine type.
gcloud container clusters create cluster-name \ --region your-region \ --release-channel rapid \ --num-nodes=1 \ --machine-type=e2-standard-4 \ --workload-pool=project-id.svc.id.goog \ --addons GcsFuseCsiDriver
The example workloads assume the following:
- Your cluster is running with a TPU v5e node pool with 3 nodes.
- TPU node pool is using machine-type
ct5lp-hightpu-1t.
If you are using a different cluster configuration than previously described,
you will need to edit
[server deployment manifest](#server-deployment).
For the JAX Stable Diffusion demo, you will need a CPU node pool with a
machine type that has 16 Gi+ available memory (for example
e2-standard-4).
This is configured in the
gcloud container clusters create command or by
adding an additional node pool to the existing cluster with the following
command:
gcloud beta container node-pools create your-pool-name \ --zone=your-cluster-zone \ --cluster=your-cluster-name \ --machine-type=e2-standard-4 \ --num-nodes=1
Replace the following:
your-pool-name: The name of the node pool to create.
your-cluster-zone: The zone in which your cluster was created.
your-cluster-name: The name of the cluster in which to add the node pool.
your-machine-type: The
[machine type](/compute/docs/machine-resource)of the nodes to create in your node pool.
Set up model storage
There are several ways you can store your model for serving. In this tutorial, we will use the following approach:
- For converting the pre-trained model to work on TPUs, we will use a
Virtual Private Cloud backed by Persistent Disk with
ReadWriteMany(RWX) access.
- For serving the model on multiple single-host TPUs, we will use the same VPC backed by the Cloud Storage bucket.
Run the following command to create a Cloud Storage bucket.
gcloud storage buckets create gs://your-bucket-name \ --project=your-bucket-project-id \ --location=your-bucket-location
Replace the following:
your-bucket-name: The name of the Cloud Storage bucket.
your-bucket-project-id: The project ID in which you created the Cloud Storage bucket.
your-bucket-location: The
[location](/storage/docs/locations)of your Cloud Storage bucket. To improve performance, specify the location where your GKE cluster is running.
Use the following steps to give your GKE cluster access to
the bucket. To simplify the setup, the following examples use the default
namespace and the default Kubernetes service account. For details, see
[Configure access to Cloud Storage buckets using GKE Workload Identity](/kubernetes-engine/docs/how-to/persistent-volumes/cloud-storage-fuse-csi-driver#authentication).
Create an IAM service account for your application or use an existing IAM service account instead. You can use any IAM service account in your Cloud Storage bucket's project.
gcloud iam service-accounts create your-iam-service-acct \ --project=your-bucket-project-id
Replace the following:
your-iam-service-acct: the name of the new IAM service account.
your-bucket-project-id: the ID of the project in which you created your IAM service account. The IAM service account must be in the same project as your Cloud Storage bucket.
-
Ensure that your IAM service account has the storage roles you need.
gcloud storage buckets add-iam-policy-binding gs://your-bucket-name \ --member ""serviceAccount:your-iam-service-acct@your-bucket-project-id.iam.gserviceaccount.com"" \ --role ""roles/storage.objectAdmin""
Replace the following:
your-bucket-name: The name of your Cloud Storage bucket.
your-iam-service-acct: the name of the new IAM service account.
your-bucket-project-id: the ID of the project in which you created your IAM service account.
-
Allow the Kubernetes service account to impersonate the IAM service account by adding an IAM policy binding between the two service accounts. This binding allows the Kubernetes service account to act as the IAM service account.
gcloud iam service-accounts add-iam-policy-binding your-iam-service-acct@your-bucket-project-id.iam.gserviceaccount.com \ --role roles/iam.workloadIdentityUser \ --member ""serviceAccount:your-project-id.svc.id.goog[default/default]""
Replace the following:
your-iam-service-acct: the name of the new IAM service account.
your-bucket-project-id: the ID of the project in which you created your IAM service account.
your-project-id: the ID of the project in which you created your GKE cluster. Your Cloud Storage buckets and GKE cluster can be in the same or different projects.
-
Annotate the Kubernetes service account with the email address of the IAM service account.
kubectl annotate serviceaccount default \ --namespace default \ iam.gke.io/gcp-service-account=your-iam-service-acct@your-bucket-project-id.iam.gserviceaccount.com
Replace the following:
your-iam-service-acct: the name of the new IAM service account.
your-bucket-project-id: the ID of the project in which you created your IAM service account.
-
Run the following command to populate your bucket name in the YAML files of this demo:
find . -type f -name ""*.yaml"" | xargs sed -i ""s/BUCKET_NAME/your-bucket-name/g""
Replace
your-bucket-namewith the name of your Cloud Storage bucket.
Create the Persistent Volume and Persistent Volume Claim with the following command:
kubectl apply -f pvc-pv.yaml
JAX Model inference and serving
Install Python dependencies for running tutorial Python scripts that send requests to the JAX model service.
pip install -r jax/requirements.txt
Run JAX BERT E2E serving demo:
This demo uses a pre trained
[BERT model](https://huggingface.co/bert-base-uncased)
from Hugging Face.
The Kubernetes Pod performs the following steps:
- Downloads and uses the Python script
export_bert_model.pyfrom the example resources to download the pre-trained bert model to a temporary directory.
- Uses the Cloud TPU Converter image to convert the pre-trained model from
CPU to TPU and stores the model in the Cloud Storage bucket you
created during
[setup](#storage-setup).
This Kubernetes Pod is configured to run on the default node pool CPU. Run the Pod with the following command:
kubectl apply -f jax/bert/install-bert.yaml
Verify the model was installed correctly with the following:
kubectl get pods install-bert
It can take a couple of minutes for the
STATUS to read
Completed.
Launch the TF model server for the model
The example workloads in this tutorial assume the following:
- The cluster is running with a TPU v5 node pool with three nodes
- The node pool is using the
ct5lp-hightpu-1tmachine type that contains one TPU chip.
If you are using a different cluster configuration than previously described,
you will need to edit
[server deployment manifest](#server-deployment).
Apply deployment
kubectl apply -f jax/bert/serve-bert.yaml
Verify the server is running with the following:
kubectl get deployment bert-deployment
It can take a minute for
AVAILABLE to read
3.
Apply load balancer service
kubectl apply -f jax/bert/loadbalancer.yaml
Verify that the load balancer is ready for external traffic with the following:
kubectl get svc tf-bert-service
It may take a few minutes for
EXTERNAL_IP to have an IP listed.
Send the request to the model server
Get external IP from load balancer service:
EXTERNAL_IP=$(kubectl get services tf-bert-service --output jsonpath='{.status.loadBalancer.ingress[0].ip}')
Run a script for sending a request to the server:
python3 jax/bert/bert_request.py $EXTERNAL_IP
Expected output:
For input ""The capital of France is [MASK]."", the result is "". the capital of france is paris..""
For input ""Hello my name [MASK] Jhon, how can I [MASK] you?"", the result is "". hello my name is jhon, how can i help you?.""
Clean-up
To clean up resources, run
kubectl delete in reverse order.
kubectl delete -f jax/bert/loadbalancer.yaml kubectl delete -f jax/bert/serve-bert.yaml kubectl delete -f jax/bert/install-bert.yaml
Run JAX Stable Diffusion E2E serving demo
This demo uses the pretrained
[Stable Diffusion model](https://huggingface.co/CompVis/stable-diffusion-v1-4/tree/bf16)
from Hugging Face.
Export TPU-compatible TF2 saved model from Flax Stable Diffusion model
Exporting the stable diffusion models requires that the cluster has a CPU node
pool with a machine type that has 16Gi+ available memory as described in
[Setup cluster](#setup-cluster).
The Kubernetes Pod executes the following steps:
- Downloads and uses the Python script
export_stable_diffusion_model.pyfrom the example resources to download the pre-trained stable diffusion model to a temporary directory.
- Uses the Cloud TPU Converter image to convert the pre-trained model from
CPU to TPU and stores the model in the Cloud Storage bucket you created
during
[storage setup](#storage-setup).
This Kubernetes Pod is configured to run on the default CPU node pool. Run the Pod with the following command:
kubectl apply -f jax/stable-diffusion/install-stable-diffusion.yaml
Verify the model was installed correctly with the following:
kubectl get pods install-stable-diffusion
It can take a couple of minutes for the
STATUS to read
Completed.
Launch the TF model server container for the model
The example workloads have been configured with the following assumptions:
- the cluster is running with a TPU v5 node pool with three nodes
- the node pool is using the
ct5lp-hightpu-1tmachine type where:
- topology is 1x1
- number of TPU chips is 1
If you are using a different cluster configuration than previously described,
you will need to edit
[server deployment manifest](#server-deployment).
Apply the deployment:
kubectl apply -f jax/stable-diffusion/serve-stable-diffusion.yaml
Verify the server is running as expected:
kubectl get deployment stable-diffusion-deployment
It can take a minute for
AVAILABLE to read
3.
Apply load balancer service:
kubectl apply -f jax/stable-diffusion/loadbalancer.yaml
Verify that the load balancer is ready for external traffic with the following:
kubectl get svc tf-stable-diffusion-service
It may take a few minutes for
EXTERNAL_IP to have an IP listed.
Send the request to the model server
Get an external IP from the load balancer:
EXTERNAL_IP=$(kubectl get services tf-stable-diffusion-service --output jsonpath='{.status.loadBalancer.ingress[0].ip}')
Run script for sending a request to the server
python3 jax/stable-diffusion/stable_diffusion_request.py $EXTERNAL_IP
Expected output:
The prompt is
Painting of a squirrel skating in New York and the output image
will be saved as
stable_diffusion_images.jpg in your current directory.
Clean-up
To clean up resources, run
kubectl delete in reverse order.
kubectl delete -f jax/stable-diffusion/loadbalancer.yaml kubectl delete -f jax/stable-diffusion/serve-stable-diffusion.yaml kubectl delete -f jax/stable-diffusion/install-stable-diffusion.yaml
Run TensorFlow ResNet-50 E2E serving demo:
Install Python dependencies for running tutorial Python scripts that send requests to the TF model service.
pip install -r tf/resnet50/requirements.txt
Step 1: Convert the model
Apply model conversion:
kubectl apply -f tf/resnet50/model-conversion.yml
Verify the model was installed correctly with the following:
kubectl get pods resnet-model-conversion
It can take a couple of minutes for the
STATUS to read
Completed.
Step 2: Serve the model with TensorFlow serving
Apply model serving deployment:
kubectl apply -f tf/resnet50/deployment.yml
Verify the server is running as expected with the following command:
kubectl get deployment resnet-deployment
It can take a minute for
AVAILABLE to read
3.
Apply load balancer service:
kubectl apply -f tf/resnet50/loadbalancer.yml
Verify that the load balancer is ready for external traffic with the following:
kubectl get svc resnet-service
It may take a few minutes for
EXTERNAL_IP to have an IP listed.
Step 3: Send test request to model server
Get the external IP from the load balancer:
EXTERNAL_IP=$(kubectl get services resnet-service --output jsonpath='{.status.loadBalancer.ingress[0].ip}')
Run the test request (HTTP) script to send request to model server.
python3 tf/resnet50/request.py --host $EXTERNAL_IP
The response should look like the following:
Predict result: ['ImageNet ID: n07753592, Label: banana, Confidence: 0.94921875', 'ImageNet ID: n03532672, Label: hook, Confidence: 0.0223388672', 'ImageNet ID: n07749582, Label: lemon, Confidence: 0.00512695312
Step 4: Clean up
To clean up resources, run the following
kubectl delete commands:
kubectl delete -f tf/resnet50/loadbalancer.yml kubectl delete -f tf/resnet50/deployment.yml kubectl delete -f tf/resnet50/model-conversion.yml
Make sure you delete the GKE
[node pool](/kubernetes-engine/docs/how-to/node-pools#deleting_a_node_pool)
and [cluster](/kubernetes-engine/docs/how-to/deleting-a-cluster)
when you are done with them.
PyTorch model inference and serving
Install Python dependencies for running tutorial Python scripts that send requests to the PyTorch model service:
pip install -r pt/densenet161/requirements.txt
Run TorchServe Densenet161 E2E serving demo:
Generate model archive.
- Apply model archive:
kubectl apply -f pt/densenet161/model-archive.yml
- Verify the model was installed correctly with the following:
kubectl get pods densenet161-model-archive
It can take a couple of minutes for the
STATUSto read
Completed.
Serve the Model with TorchServe:
Apply Model Serving Deployment:
kubectl apply -f pt/densenet161/deployment.yml
Verify the server is running as expected with the following command:
kubectl get deployment densenet161-deployment
It can take a minute for
AVAILABLEto read
3.
Apply load balancer service:
kubectl apply -f pt/densenet161/loadbalancer.yml
Verify that the load balancer is ready for external traffic with the following command:
kubectl get svc densenet161-service
It may take a few minutes for
EXTERNAL_IPto have an IP listed.
-
Send test request to model server:
Get external IP from load balancer:
EXTERNAL_IP=$(kubectl get services densenet161-service --output jsonpath='{.status.loadBalancer.ingress[0].ip}')
Run the test request script to send request (HTTP) to model server.:
python3 pt/densenet161/request.py --host $EXTERNAL_IP
You should see a response like this:
Request successful. Response: {'tabby': 0.47878125309944153, 'lynx': 0.20393909513950348, 'tiger_cat': 0.16572578251361847, 'tiger': 0.061157409101724625, 'Egyptian_cat': 0.04997897148132324
-
Clean up resources, by running the following
kubectl deletecommands:
kubectl delete -f pt/densenet161/loadbalancer.yml kubectl delete -f pt/densenet161/deployment.yml kubectl delete -f pt/densenet161/model-archive.yml
Make sure you delete the GKE
[node pool](/kubernetes-engine/docs/how-to/node-pools#deleting_a_node_pool)and [cluster](/kubernetes-engine/docs/how-to/deleting-a-cluster)when you are done with them.
Troubleshooting common issues
You can find GKE troubleshooting information at
[Troubleshoot TPU inGKE](/kubernetes-engine/docs/troubleshooting/troubleshoot-tpus).
TPU initialization failed
If you encounter the following error, make sure you are either running your TPU
container in privileged mode or you have increased the
ulimit inside your
container. For more information, see
[Running without privileged mode](#privileged-mode).
TPU platform initialization failed: FAILED_PRECONDITION: Couldn't mmap: Resource
temporarily unavailable.; Unable to create Node RegisterInterface for node 0,
config: device_path: ""/dev/accel0"" mode: KERNEL debug_data_directory: """"
dump_anomalies_only: true crash_in_debug_dump: false allow_core_dump: true;
could not create driver instance
Scheduling deadlock
Suppose you have two jobs (Job A and Job B) and both are to be scheduled on TPU
slices with a given TPU topology (say,
v4-32). Also suppose that you have
two
v4-32 TPU slices within the GKE cluster; we'll
call those slice X and slice Y. Since your cluster has ample capacity to
schedule both jobs, in theory both jobs should be quickly scheduled  one job on
each of the two TPU
v4-32 slices.
However, without careful planning, it is possible to get into a scheduling deadlock. Suppose the Kubernetes scheduler schedules one Pod from Job A on slice X and then schedules one Pod from Job B on slice X. In this case, given the Pod affinity rules for Job A, the scheduler will attempt to schedule all remaining Pods for Job A on slice X. Same for Job B. And thus neither Job A nor Job B will be able to be fully scheduled on a single slice. The result will be a scheduling deadlock.
In order to avoid the risk of a scheduling deadlock, you can use Pod
anti-affinity with
cloud.google.com/gke-nodepool as the
topologyKey as shown
in the following example:
apiVersion: batch/v1
kind: Job
metadata:
name: pi
spec:
parallelism: 2
template:
metadata:
labels:
job: pi
spec:
affinity:
podAffinity:
requiredDuringSchedulingIgnoredDuringExecution:
- labelSelector:
matchExpressions:
- key: job
operator: In
values:
- pi
topologyKey: cloud.google.com/gke-nodepool
podAntiAffinity:
requiredDuringSchedulingIgnoredDuringExecution:
- labelSelector:
matchExpressions:
- key: job
operator: NotIn
values:
- pi
topologyKey: cloud.google.com/gke-nodepool
namespaceSelector:
matchExpressions:
- key: kubernetes.io/metadata.name
operator: NotIn
values:
- kube-system
containers:
- name: pi
image: perl:5.34.0
command: [""sleep"", ""60""]
restartPolicy: Never
backoffLimit: 4
Creating TPU node pool resources with Terraform
You can also use
[Terraform](https://registry.terraform.io/providers/hashicorp/google-beta/latest/docs/resources/container_node_pool)
to manage your cluster and node pool resources.
Create a multi-host TPU slice node pool in an existing GKE Cluster
If you have an existing Cluster in which you want to create a multi-host TPU node pool, you can use the following Terraform snippet:
resource ""google_container_cluster"" ""cluster_multi_host"" {

release_channel {
channel = ""RAPID""
}
workload_identity_config {
workload_pool = ""my-gke-project.svc.id.goog""
}
addons_config {
gcs_fuse_csi_driver_config {
enabled = true
}
}
}
resource ""google_container_node_pool"" ""multi_host_tpu"" {
provider = google-beta
project = ""<var>your-project</var>""
name = ""<var>your-node-pool</var>""
location = ""<var>us-central2</var>""
node_locations = [""<var>us-central2-b</var>""]
cluster = google_container_cluster.cluster_multi_host.name
initial_node_count = 2
node_config {
machine_type = ""ct4p-hightpu-4t""
reservation_affinity {
consume_reservation_type = ""SPECIFIC_RESERVATION""
key = ""compute.googleapis.com/reservation-name""
values = [""<var>your-reservation-name</var>""]
}
workload_metadata_config {
mode = ""GKE_METADATA""
}
}
placement_policy {
type = ""COMPACT""
tpu_topology = ""2x2x2""
}
}
Replace the following values:
your-project: Your Google Cloud project in which you are running your workload.
your-node-pool: The name of the node pool you are creating.
us-central2: The region in which you are running your workload.
us-central2-b: The zone in which you are running your workload.
your-reservation-name: The name of your reservation.
Create a single-host TPU slice node pool in an existing GKE Cluster
Use the following Terraform snippet:
resource ""google_container_cluster"" ""cluster_single_host"" {

cluster_autoscaling {
autoscaling_profile = ""OPTIMIZE_UTILIZATION""
}
release_channel {
channel = ""RAPID""
}
workload_identity_config {
workload_pool = ""<var>your-project-id</var>.svc.id.goog""
}
addons_config {
gcs_fuse_csi_driver_config {
enabled = true
}
}
}
resource ""google_container_node_pool"" ""single_host_tpu"" {
provider = google-beta
project = ""<var>your-project</var>""
name = ""<var>your-node-pool</var>""
location = ""<var>us-central2</var>""
node_locations = [""<var>us-central2-b</var>""]
cluster = google_container_cluster.cluster_single_host.name
initial_node_count = 0
autoscaling {
total_min_node_count = 2
total_max_node_count = 22
location_policy = ""ANY""
}
node_config {
machine_type = ""ct4p-hightpu-4t""
workload_metadata_config {
mode = ""GKE_METADATA""
}
}
}
Replace the following values:
your-project: Your Google Cloud project in which you are running your workload.
your-node-pool: The name of the node pool you are creating.
us-central2: The region in which you are running your workload.
us-central2-b: The zone in which you are running your workload.",TPUs in GKE introduction | Google Cloud,
id,url,body,title,description
99,https://cloud.google.com/tpu/docs/tensorflow-pods,"Run TensorFlow code on TPU Pod slices
This document shows you how to perform a simple calculation using TensorFlow on a TPU Pod. You will perform the following steps:
- Create a TPU Pod slice with TensorFlow software
- Connect to the TPU VM using SSH
- Create and run a simple script
The TPU VM relies on a
[Service Accounts](/compute/docs/access/service-accounts#serviceaccount)
for permissions to call Cloud TPU API. By default, your TPU VM will use the
default Compute Engine [service account](/compute/docs/access/service-accounts)
which includes all needed Cloud TPU permissions. If you use your own
service account you need to add the [TPU Viewer](/iam/docs/understanding-roles#tpu.viewer)
role to your service account. For more information on Google Cloud roles, see [Understanding roles](/iam/docs/understanding-roles).
You can specify your own service account using the
--service-account flag when
creating your TPU VM.
Create a v3-32 TPU Pod slice with TensorFlow runtime
$ gcloud compute tpus tpu-vm create tpu-name \
--zone=europe-west4-a \
--accelerator-type=v3-32 \
--version=tpu-vm-tf-2.15.0-pod-pjrt
Command flag descriptions
zone
- The
[zone](/tpu/docs/types-zones)where you plan to create your Cloud TPU.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
version
- The Cloud TPU
[software version](/tpu/docs/supported-tpu-versions#tpu_software_versions).
Connect to your Cloud TPU VM using SSH
$ gcloud compute tpus tpu-vm ssh tpu-name \
--zone europe-west4-a
Create and run a simple calculation script
Set the following environment variables.
(vm)$ export TPU_NAME=tpu-name (vm)$ export TPU_LOAD_LIBRARY=0
Create a file named
tpu-test.pyin the current directory and copy and paste the following script into it.
import tensorflow as tf print(""Tensorflow version "" + tf.__version__) cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver() print('Running on TPU ', cluster_resolver.cluster_spec().as_dict()['worker']) tf.config.experimental_connect_to_cluster(cluster_resolver) tf.tpu.experimental.initialize_tpu_system(cluster_resolver) strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver) @tf.function def add_fn(x,y): z = x + y return z x = tf.constant(1.) y = tf.constant(1.) z = strategy.run(add_fn, args=(x,y)) print(z)
Run this script with the following command:
(vm)$ python3 tpu-test.py
This script performs a simple computation on a each TensorCore of a TPU. The output will look similar to the following:
PerReplica:{ 0: tf.Tensor(2.0, shape=(), dtype=float32), 1: tf.Tensor(2.0, shape=(), dtype=float32), 2: tf.Tensor(2.0, shape=(), dtype=float32), 3: tf.Tensor(2.0, shape=(), dtype=float32), 4: tf.Tensor(2.0, shape=(), dtype=float32), 5: tf.Tensor(2.0, shape=(), dtype=float32), 6: tf.Tensor(2.0, shape=(), dtype=float32), 7: tf.Tensor(2.0, shape=(), dtype=float32), 8: tf.Tensor(2.0, shape=(), dtype=float32), 9: tf.Tensor(2.0, shape=(), dtype=float32), 10: tf.Tensor(2.0, shape=(), dtype=float32), 11: tf.Tensor(2.0, shape=(), dtype=float32), 12: tf.Tensor(2.0, shape=(), dtype=float32), 13: tf.Tensor(2.0, shape=(), dtype=float32), 14: tf.Tensor(2.0, shape=(), dtype=float32), 15: tf.Tensor(2.0, shape=(), dtype=float32), 16: tf.Tensor(2.0, shape=(), dtype=float32), 17: tf.Tensor(2.0, shape=(), dtype=float32), 18: tf.Tensor(2.0, shape=(), dtype=float32), 19: tf.Tensor(2.0, shape=(), dtype=float32), 20: tf.Tensor(2.0, shape=(), dtype=float32), 21: tf.Tensor(2.0, shape=(), dtype=float32), 22: tf.Tensor(2.0, shape=(), dtype=float32), 23: tf.Tensor(2.0, shape=(), dtype=float32), 24: tf.Tensor(2.0, shape=(), dtype=float32), 25: tf.Tensor(2.0, shape=(), dtype=float32), 26: tf.Tensor(2.0, shape=(), dtype=float32), 27: tf.Tensor(2.0, shape=(), dtype=float32), 28: tf.Tensor(2.0, shape=(), dtype=float32), 29: tf.Tensor(2.0, shape=(), dtype=float32), 30: tf.Tensor(2.0, shape=(), dtype=float32), 31: tf.Tensor(2.0, shape=(), dtype=float32) }
Clean up
When you are done with your TPU VM follow these steps to clean up your resources.
Disconnect from the Compute Engine:
(vm)$ exit
Delete your Cloud TPU.
$ gcloud compute tpus tpu-vm delete tpu-name \ --zone europe-west4-a
Verify the resources have been deleted by running the following command. Make sure your TPU is no longer listed. The deletion might take several minutes.
$ gcloud compute tpus tpu-vm list \ --zone europe-west4-a",Run TensorFlow code on TPU Pod slices | Google Cloud,
id,url,body,title,description
87,https://cloud.google.com/tpu/docs/v5e-training,"Cloud TPU v5e training
Cloud TPU v5e is Google Cloud's latest generation AI accelerator. With a smaller 256-chip footprint per Pod, a v5e is optimized to be the highest value product for transformer, text-to-image, and Convolutional Neural Network (CNN) training, fine-tuning, and serving.
Cloud TPU v5e concepts, system architecture, and configurations
If you are new to Cloud TPUs, check out the
[TPU documentation home](/tpu/docs).
General Cloud TPU concepts (for example, slices, hosts, chips,
and TensorCores), and Cloud TPU system architecture are described in
the
[Cloud TPU System Architecture](/tpu/docs/system-architecture-tpu-vm)
page.
Each Cloud TPU version requires specific accelerator types for
training and inference. These accelerator types
are described in the
[TPU configurations](/tpu/docs/supported-tpu-configurations) page.
Inference
Inference is the process of using a trained model to make predictions
on new data. It is used by the
[serving](#serving) process.
Slices
A slice represents a collection of chips all located inside the same
Pod connected by high-speed inter chip interconnects (ICI).
v5e has 2D slice shapes. See the table in the
[Accelerator Types](/tpu/docs/supported-tpu-configurations#tpu-v5e-config)
section for supported slice shapes.
Chip shape and chip topology also refer to slice shapes.
Serving
Serving is the process of deploying a trained machine learning model to a production environment where it can be used to make predictions or decisions. Latency and service-level availability are important for serving.
Single host versus multi host
A host is a physical computer (CPU) that runs VMs. A host can run multiple VMs at once.
Slices using fewer than 8 chips use at most one host. Slices greater
than 8 chips have access to more than a single host and can run
distributed training using multiple hosts. See the
[TPU System Architecture](/tpu/docs/system-architecture-tpu-vm) page for
details on slices and chips.
v5e supports multi-host training and multi-host inference
(using
[SAX](https://github.com/google/saxml)).
TPU VM
A virtual machine running Linux that has access to the underlying TPUs. For v5e TPUs, each TPU VM has direct access to 1, 4, or 8 chips depending on the user-specified accelerator type. A TPU VM is also known as a worker.
Worker
See
[TPU VM](#tpu-vm).
Get started
For information about v5e TPU hardware, see
[System Architecture](/tpu/docs/system-architecture-tpu-vm).
Securing capacity
Contact
[Cloud Sales](https://cloud.google.com/contact)
to start using Cloud TPU v5e for your AI workloads.
Prepare a Google Cloud Project
[Sign in](https://accounts.google.com/Login)to your Google Account. If you haven't already, [sign up for a new account](https://accounts.google.com/SignUp).
- In the
[Google Cloud console](https://console.cloud.google.com/), [select](/resource-manager/docs/creating-managing-projects#get_an_existing_project)or [create](/resource-manager/docs/creating-managing-projects#creating_a_project)a Cloud project from the project selector page. [Billing setup](/billing/docs)is required for all Google Cloud usage so make sure billing is enabled for your project.
Install
[gcloud alpha components](/sdk/gcloud/reference/components/install).
If you are a TPU user reusing existing
gcloud alphacomponents, update these to ensure that relevant commands and flags are supported:
gcloud components update
Enable the TPU API through the following
gcloudcommand in the Cloud Shell. (You may also enable it
[from the Google Cloud console](https://console.cloud.google.com/apis/library/tpu.googleapis.com).)
gcloud services enable tpu.googleapis.com
Enable the TPU service account.
Service accounts allow the Cloud TPU service to access other Google Cloud services. A user-managed service account is a recommended Google Cloud practice. Follow these guides to
[create](/iam/docs/service-accounts-create)and [grant roles](/iam/docs/granting-changing-revoking-access). The following roles are necessary:
- TPU Admin
- Storage Admin: Needed for accessing Cloud Storage
- Logs Writer: Needed for writing logs with the Logging API
- Monitoring Metric Writer: Needed for writing metrics to Cloud Monitoring
Configure the project and zone.
Your project ID is the name of your project
[shown on the Cloud console](/resource-manager/docs/creating-managing-projects#identifying_projects).
export PROJECT_ID=your-project-id export ZONE=us-west4-a gcloud alpha compute tpus tpu-vm service-identity create --zone=${ZONE} gcloud auth login gcloud config set project ${PROJECT_ID} gcloud config set compute/zone ${ZONE}
Provision the Cloud TPU environment
The best practice is to provision Cloud TPU v5es as
[Queued Resources](/tpu/docs/queued-resources) using the
queued-resource create command. However, you can
also use the Create Node API (
gcloud alpha compute tpus tpu-vm create) to
provision Cloud TPU v5es.
Create environment variables
Set necessary environment variables for TPU creation.
Replace the variables (in red) in the following list with values you will use for your training or inference job.
export PROJECT_ID=your_project_ID export ACCELERATOR_TYPE=v5litepod-16 export ZONE=us-west4-a export RUNTIME_VERSION=v2-alpha-tpuv5-lite export SERVICE_ACCOUNT=your_service_account export TPU_NAME=your_tpu_name export QUEUED_RESOURCE_ID=your_queued_resource_id export QUOTA_TYPE=quota_type export VALID_UNTIL_DURATION=1d
|Variable
|Description
|PROJECT_ID
|Google Cloud Project Name
|ACCELERATOR_TYPE
|See the
[Queued Resources](/tpu/docs/queued-resources)document for information about queued resources.
reserved or
best-effort.
See
[TPU Quota types](/tpu/docs/quota#quota_types)for information on the different types of quotas supported by Cloud TPU. best-effort quota is the default, so you don't need to set that value. [Queued resources](/tpu/docs/queued-resources)for information about the different valid durations.
[Create a TPU resource](/tpu/docs/queued-resources)
gcloud alpha compute tpus queued-resources create ${QUEUED_RESOURCE_ID} \ --node-id ${TPU_NAME} \ --project ${PROJECT_ID} \ --zone ${ZONE} \ --accelerator-type ${ACCELERATOR_TYPE} \ --runtime-version ${RUNTIME_VERSION} \ --valid-until-duration ${VALID_UNTIL_DURATION} \ --service-account ${SERVICE_ACCOUNT} \ --${QUOTA_TYPE}
If the queued resource is created successfully, the state within
the
response field will be either
WAITING_FOR_RESOURCES or
FAILED.
If the queued resource is in the
WAITING_FOR_RESOURCES state, that means that the
queued resource has passed preliminary validation and is awaiting capacity.
Once capacity is available the request will transition to
PROVISIONING.
Being in
WAITING_FOR_RESOURCES state does not mean it is guaranteed that you
will get the quota allocated and it may take some time to change
from
WAITING_FOR_RESOURCES status to
ACTIVE status.
If the queued resource is in the
FAILED
state, the failure reason will be in the output.
The request will expire if a request is not filled in the
--valid-until-duration, and the state becomes ""FAILED"".
You will be able to access your TPU VM using SSH once your QueuedResource is in the
ACTIVE state.
Use the
or
[list](/tpu/docs/managing-tpus-tpu-vm)
commands to query the status of your queued resource.
[describe](/tpu/docs/managing-tpus-tpu-vm)
gcloud alpha compute tpus queued-resources describe ${QUEUED_RESOURCE_ID}
--project ${PROJECT_ID}
--zone ${ZONE}
The state represents the status of a queued resource. The states are defined as:
|State
|Description
|
WAITING_FOR_RESOURCES
|The queued resource
create command has been received and will
begin provisioning, as soon as capacity is available.
|
PROVISIONING
|The TPU slices are being provisioned.
|
ACTIVE
|All TPUs are provisioned, and are ready to use. If a startup
script is given, it will start executing on all TPUs, when the
queued resource state transitions to
ACTIVE.
|
FAILED
|The slices couldn't be provisioned.
|
SUSPENDING
|One or more slices are being deleted.
|
SUSPENDED
|All underlying slices are deleted but the queued resource stays intact, until explicitly deleted. Right now, a suspended queued resource cannot be resumed and should be deleted.
|
DELETING
|The queued resource is being deleted.
Connect to the TPU VM using SSH
The following section describes how you can install binaries on each TPU VM in your TPU slice and run code. In this context, a TPU VM is also known as a worker.
See the
[VM Types](/tpu/docs/supported-tpu-configurations#lite-pod-types) section
to determine how many VMs your slice will have.
To install the binaries or run code, connect to your TPU VM using the
.
[tpu-vm ssh command](/sdk/gcloud/reference/alpha/compute/tpus/tpu-vm/ssh)
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME}
To access a specific TPU VM with SSH, use the
--worker flag which follows
a 0-based index:
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} --worker=1
If you have slice shapes greater than 8 chips, you will have multiple VMs in one
slice. In this case, use the
--worker=all flag to run the installation on all
TPU VMs without having to connect to each one separately. For example:
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} --project ${PROJECT_ID} \ --zone ${ZONE} \ --worker=all \ --command='pip install ""jax[tpu]==0.4.16"" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html'
Manage
All commands you can use to manage your TPU VMs are described in
[Managing TPUs.](/tpu/docs/managing-tpus-tpu-vm)
Framework Setup
This section describes the general setup process for custom model training using
JAX or PyTorch with TPU v5e. TensorFlow support is available in the
tpu-vm-tf-2.15.0-pjrt and
tpu-vm-tf-2.15.0-pod-pjrt TPU runtime versions.
For inference setup instructions, see
[v5e inference introduction](/tpu/docs/v5e-inference).
Setup for JAX
If you have slice shapes greater than 8 chips, you will have multiple VMs in one
slice. In this case, you need to use the
--worker=all flag to run the
installation on all TPU VMs in a single step without using SSH to log into each
separately:
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} --project ${PROJECT_ID} \ --zone ${ZONE} \ --worker=all \ --command='pip install ""jax[tpu]==0.4.16"" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html'
You can run the following command to check number of devices (the outputs shown here were produced with a v5litepod-16 slice). This code tests that everything is installed correctly by checking that JAX sees the Cloud TPU TensorCores and can run basic operations:
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \ --project ${PROJECT_ID} \ --zone ${ZONE} \ --worker=all \ --command='python3 -c ""import jax; print(jax.device_count()); print(jax.local_device_count())""'
The output will be similar to the following:
SSH: Attempting to connect to worker 0... SSH: Attempting to connect to worker 1... SSH: Attempting to connect to worker 2... SSH: Attempting to connect to worker 3... 16 4 16 4 16 4 16 4
jax.device_count() shows the total number of chips in the given
slice.
jax.local_device_count() indicates the count of chips
accessible by a single VM in this slice.
# Check the number of chips in the given slice by summing the count of chips # from all VMs through the # jax.local_device_count() API call. gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \ --project ${PROJECT_ID} \ --zone ${ZONE} \ --worker=all \ --command='python3 -c ""import jax; xs=jax.numpy.ones(jax.local_device_count()); print(jax.pmap(lambda x: jax.lax.psum(x, \""i\""), axis_name=\""i\"")(xs))""'
The output will be similar to the following:
SSH: Attempting to connect to worker 0... SSH: Attempting to connect to worker 1... SSH: Attempting to connect to worker 2... SSH: Attempting to connect to worker 3... [16. 16. 16. 16.] [16. 16. 16. 16.] [16. 16. 16. 16.] [16. 16. 16. 16.]
Try the
[JAX Tutorials](#jax-flax-examples) in this document to get started with
v5e training using JAX.
Setup for PyTorch
Note that v5e only supports the
[PJRT runtime](https://github.com/pytorch/xla/blob/master/docs/pjrt.md)
and PyTorch 2.1+ will use PJRT as the default runtime for all TPU versions.
This section describes how to start using PJRT on v5e with PyTorch/XLA with commands for all workers.
Install dependencies
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \ --project ${PROJECT_ID} \ --zone ${ZONE} \ --worker=all \ --command=' sudo apt-get update -y sudo apt-get install libomp5 -y pip3 install mkl mkl-include pip3 install tf-nightly tb-nightly tbp-nightly pip3 install numpy sudo apt-get install libopenblas-dev -y sudo apt-get install libopenblas-dev -y pip3 install torch~=2.1.0 torchvision torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html pip3 install torch_xla[tpu] -f https://storage.googleapis.com/libtpu-releases/index.html '
If you failed to install wheels for torch/torch_xla/torchvision and see
an error like
pkg_resources.extern.packaging.requirements.InvalidRequirement:
Expected end or semicolon (after name and no valid version specifier) torch==nightly+20230222,
downgrade your version with this command:
pip3 install setuptools==62.1.0
Run a script with PJRT:
unset LD_PRELOAD
The following is an example using a Python script to do a calculation on a v5e VM:
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} --project ${PROJECT_ID} --zone ${ZONE} --worker all --command=' export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/$USER/.local/lib/ export PJRT_DEVICE=TPU_C_API export PT_XLA_DEBUG=0 export USE_TORCH=ON unset LD_PRELOAD export TPU_LIBRARY_PATH=/home/$USER/.local/lib/python3.10/site-packages/libtpu/libtpu.so python3 -c ""import torch; import torch_xla; import torch_xla.core.xla_model as xm; print(xm.xla_device()); dev = xm.xla_device(); t1 = torch.randn(3,3,device=dev); t2 = torch.randn(3,3,device=dev); print(t1 + t2)"" '
This generates output similar to the following:
SSH: Attempting to connect to worker 0... SSH: Attempting to connect to worker 1... xla:0 tensor([[ 1.8611, -0.3114, -2.4208], [-1.0731, 0.3422, 3.1445], [ 0.5743, 0.2379, 1.1105]], device='xla:0') xla:0 tensor([[ 1.8611, -0.3114, -2.4208], [-1.0731, 0.3422, 3.1445], [ 0.5743, 0.2379, 1.1105]], device='xla:0')
Try the
[PyTorch Tutorials](#pytorch-xla) in this document to get started with
v5e training using PyTorch.
Delete your TPU and queued resource at the end of your session. To delete a queued resource, delete the slice and then the queued resource in 2 steps:
gcloud alpha compute tpus tpu-vm delete ${TPU_NAME} \ --project=${PROJECT_ID} \ --zone=${ZONE} \ --quiet gcloud alpha compute tpus queued-resources delete ${QUEUED_RESOURCE_ID} \ --project ${PROJECT_ID} \ --zone ${ZONE} \ --quiet
These two steps can also be used to remove queued resource requests
that are in the
FAILED state.
Monitor and Profile
Cloud TPU v5e supports monitoring and profiling using the
same methods as previous generations of Cloud TPU. You can
read
[Profile your model with Cloud TPU tools](/tpu/docs/cloud-tpu-tools)
to learn more about profiling and [Monitoring Cloud TPU VMs](/tpu/docs/troubleshooting/tpu-vm-monitoring)
to learn more about monitoring.
JAX/FLAX examples
Train ImageNet on v5e
This tutorial describes how to train ImageNet on v5e using fake
input data. If you want to use real data, refer to the
[README file on GitHub](https://github.com/google/flax/blob/main/examples/imagenet/README.md).
Set up
Create environment variables:
export PROJECT_ID=your_project_ID export ACCELERATOR_TYPE=v5litepod-16 export ZONE=us-west4-a export RUNTIME_VERSION=v2-alpha-tpuv5-lite export SERVICE_ACCOUNT=your_service_account export TPU_NAME=your_tpu_name export QUEUED_RESOURCE_ID=your_queued_resource_id export QUOTA_TYPE=quota_type export VALID_UNTIL_DURATION=1d
gcloud alpha compute tpus queued-resources create ${QUEUED_RESOURCE_ID} \ --node-id ${TPU_NAME} \ --project ${PROJECT_ID} \ --zone ${ZONE} \ --accelerator-type ${ACCELERATOR_TYPE} \ --runtime-version ${RUNTIME_VERSION} \ --valid-until-duration ${VALID_UNTIL_DURATION} \ --service-account ${SERVICE_ACCOUNT} \ --${QUOTA_TYPE}
gcloud alpha compute tpus queued-resources describe ${QUEUED_RESOURCE_ID} --project ${PROJECT_ID} --zone ${ZONE}
When the QueuedResource is in the
ACTIVEstate, the output will be similar to the following:  state: ACTIVE 
Install newest version of JAX and jaxlib:
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} --project=${PROJECT_ID} --zone=${ZONE} --worker=all --command='pip install ""jax[tpu]==0.4.16"" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html'
Clone the ImageNet model and install the corresponding requirements:
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} --project=${PROJECT_ID} --zone=${ZONE} --worker=all --command='git clone https://github.com/google/flax.git && cd flax/examples/imagenet && pip install -r requirements.txt && pip install flax==0.7.4'
To generate fake data, the model needs information on the dimensions of the dataset. This can be gathered from the ImageNet dataset's metadata:
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} --project=${PROJECT_ID} --zone=${ZONE} --worker=all --command='mkdir -p $HOME/flax/.tfds/metadata/imagenet2012/5.1.0 && curl https://raw.githubusercontent.com/tensorflow/datasets/v4.4.0/tensorflow_datasets/testing/metadata/imagenet2012/5.1.0/dataset_info.json --output $HOME/flax/.tfds/metadata/imagenet2012/5.1.0/dataset_info.json'
Train the model
Once all the previous steps are done, you can train the model.
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \ --project=${PROJECT_ID} \ --zone=${ZONE} \ --worker=all \ --command='cd flax/examples/imagenet && JAX_PLATFORMS=tpu python3 imagenet_fake_data_benchmark.py'
Delete the TPU and queued-resource
Delete your TPU and queued-resource at the end of your session.
gcloud alpha compute tpus tpu-vm delete ${TPU_NAME} \ --project=${PROJECT_ID} \ --zone=${ZONE} \ --quiet
gcloud alpha compute tpus queued-resources delete ${QUEUED_RESOURCE_ID} \ --project ${PROJECT_ID} \ --zone ${ZONE} \ --quiet
Hugging Face FLAX Models
[Hugging Face](https://huggingface.co/) models implemented in FLAX
work out of the box on Cloud TPU v5e. This section provides
instructions for running popular models.
Train ViT on Imagenette
This tutorial shows you how to train the
[Vision Transformer](https://arxiv.org/abs/2010.11929)
(ViT) model from HuggingFace using the Fast AI [imagenette](https://github.com/fastai/imagenette)
dataset on Cloud TPU v5e.
The ViT model was the first one that successfully trained a Transformer encoder on ImageNet with excellent results compared to convolutional networks. For more information, see the following resources:
Set up
Create environment variables:
export PROJECT_ID=your_project_ID export ACCELERATOR_TYPE=v5litepod-16 export ZONE=us-west4-a export RUNTIME_VERSION=v2-alpha-tpuv5-lite export SERVICE_ACCOUNT=your_service_account export TPU_NAME=your_tpu_name export QUEUED_RESOURCE_ID=your_queued_resource_id export QUOTA_TYPE=quota_type export VALID_UNTIL_DURATION=1d
gcloud alpha compute tpus queued-resources create ${QUEUED_RESOURCE_ID} \ --node-id ${TPU_NAME} \ --project ${PROJECT_ID} \ --zone ${ZONE} \ --accelerator-type ${ACCELERATOR_TYPE} \ --runtime-version ${RUNTIME_VERSION} \ --valid-until-duration ${VALID_UNTIL_DURATION} \ --service-account ${SERVICE_ACCOUNT} \ --${QUOTA_TYPE}
You will be able to SSH to your TPU VM once your queued resource is in state
ACTIVE:
gcloud alpha compute tpus queued-resources describe ${QUEUED_RESOURCE_ID} \ --project ${PROJECT_ID} \ --zone ${ZONE}
When the queued resource is in the
ACTIVEstate, the output will be similar to the following:  state: ACTIVE 
Install JAX and its library:
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \ --project=${PROJECT_ID} \ --zone=${ZONE} \ --worker=all \ --command='pip install ""jax[tpu]==0.4.16"" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html'
Download Hugging Face
[repo](https://github.com/huggingface/transformers.git)and install requirements:
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \ --project=${PROJECT_ID} \ --zone=${ZONE} \ --worker=all \ --command='git clone https://github.com/huggingface/transformers.git && cd transformers && pip install . && pip install -r examples/flax/_tests_requirements.txt && pip install --upgrade huggingface-hub urllib3 zipp && pip install tensorflow==2.15.0 && pip install -r examples/flax/vision/requirements.txt'
Download the Imagenette dataset:
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} --project=${PROJECT_ID} --zone=${ZONE} --worker=all --command='cd transformers && wget https://s3.amazonaws.com/fast-ai-imageclas/imagenette2.tgz && tar -xvzf imagenette2.tgz'
Train the model
Train the model with a pre-mapped buffer at 4GB.
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} --project=${PROJECT_ID} --zone=${ZONE} --worker=all --command='cd transformers && JAX_PLATFORMS=tpu python3 examples/flax/vision/run_image_classification.py --train_dir ""imagenette2/train"" --validation_dir ""imagenette2/val"" --output_dir ""./vit-imagenette"" --learning_rate 1e-3 --preprocessing_num_workers 32 --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --model_name_or_path google/vit-base-patch16-224-in21k --num_train_epochs 3'
Delete the TPU and queued-resource
Delete your TPU and queued-resource at the end of your session.
gcloud alpha compute tpus tpu-vm delete ${TPU_NAME} --project=${PROJECT_ID} --zone=${ZONE} --quiet
gcloud alpha compute tpus queued-resources delete ${QUEUED_RESOURCE_ID} --project ${PROJECT_ID} --zone ${ZONE} --quiet
ViT benchmarking results
The training script was run on v5litepod-4, v5litepod-16, and v5litepod-64. The following table shows the throughputs with different accelerator types.
|Accelerator type
|v5litepod-4
|v5litepod-16
|v5litepod-64
|Epoch
|3
|3
|3
|Global batch size
|32
|128
|512
|Throughput (examples/sec)
|263.40
|429.34
|470.71
Train Diffusion on Pokmon
This tutorial shows you how to train the Stable Diffusion model from
HuggingFace using the
[Pokmon](https://huggingface.co/datasets/lambdalabs/pokemon-blip-captions)
dataset on Cloud TPU v5e.
The Stable Diffusion model is a latent text-to-image model that generates photo-realistic images from any text input. For more information, see the following resources:
Set up
Create environment variables:
export PROJECT_ID=your_project_ID export ACCELERATOR_TYPE=v5litepod-16 export ZONE=us-west4-a export RUNTIME_VERSION=v2-alpha-tpuv5-lite export SERVICE_ACCOUNT=your_service_account export TPU_NAME=your_tpu_name export QUEUED_RESOURCE_ID=queued_resource_id export QUOTA_TYPE=quota_type export VALID_UNTIL_DURATION=1d
gcloud alpha compute tpus queued-resources create ${QUEUED_RESOURCE_ID} \ --node-id ${TPU_NAME} \ --project ${PROJECT_ID} \ --zone ${ZONE} \ --accelerator-type ${ACCELERATOR_TYPE} \ --runtime-version ${RUNTIME_VERSION} \ --valid-until-duration ${VALID_UNTIL_DURATION} \ --service-account ${SERVICE_ACCOUNT} \ --${QUOTA_TYPE}
gcloud alpha compute tpus queued-resources describe ${QUEUED_RESOURCE_ID} --project ${PROJECT_ID} --zone ${ZONE}
When the queued resource is in the
ACTIVEstate, the output will be similar to the following:  state: ACTIVE 
Install JAX and its library.
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} --project=${PROJECT_ID} --zone=${ZONE} --worker=all --command='pip install ""jax[tpu]==0.4.16"" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html'
Download HuggingFace
[repo](https://github.com/huggingface/diffusers)and install requirements.
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} --project=${PROJECT_ID} --zone=${ZONE} --worker=all --command='git clone https://github.com/RissyRan/diffusers.git && cd diffusers && pip install . && pip install tensorflow==2.15.0 clu && pip install -U -r examples/text_to_image/requirements_flax.txt'
Train the model
Train the model with a pre-mapped buffer at 4GB.
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME}
--project=${PROJECT_ID}
--zone=${ZONE}
--worker=all
--command='cd diffusers/examples/text_to_image && JAX_PLATFORMS=tpu,cpu python3 train_text_to_image_flax.py --pretrained_model_name_or_path=duongna/stable-diffusion-v1-4-flax --dataset_name=lambdalabs/pokemon-blip-captions --resolution=128 --center_crop --random_flip --train_batch_size=4 --mixed_precision=fp16 --max_train_steps=1500 --learning_rate=1e-05 --max_grad_norm=1 --output_dir=sd-pokemon-model'
Delete the TPU and queued-resource
Delete your TPU and queued-resource at the end of your session.
gcloud alpha compute tpus tpu-vm delete ${TPU_NAME}
--project=${PROJECT_ID}
--zone=${ZONE}
--quiet
gcloud alpha compute tpus queued-resources delete ${QUEUED_RESOURCE_ID}
--project ${PROJECT_ID}
--zone ${ZONE}
--quiet
Benchmarking results for diffusion
The training script ran on v5litepod-4, v5litepod-16, and v5litepod-64. The following table shows the throughputs.
|Accelerator type
|v5litepod-4
|v5litepod-16
|v5litepod-64
|Train Step
|1500
|1500
|1500
|Global batch size
|32
|64
|128
|Throughput (examples/sec)
|36.53
|43.71
|49.36
Train GPT2 on the OSCAR dataset
This tutorial shows you how to train the
[ GPT2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) model from HuggingFace using the [OSCAR](https://huggingface.co/datasets/oscar) dataset on Cloud TPU v5e.
The GPT2 is a transformer model pre-trained on raw texts without human labeling. It was trained to predict the next word in sentences. For more information, see the following resources:
Set up
Create environment variables:
export PROJECT_ID=your_project_ID export ACCELERATOR_TYPE=v5litepod-16 export ZONE=us-west4-a export RUNTIME_VERSION=v2-alpha-tpuv5-lite export SERVICE_ACCOUNT=your_service_account export TPU_NAME=your_tpu_name export QUEUED_RESOURCE_ID=queued_resource_id export QUOTA_TYPE=quota_type export VALID_UNTIL_DURATION=1d
gcloud alpha compute tpus queued-resources create ${QUEUED_RESOURCE_ID} \ --node-id ${TPU_NAME} \ --project ${PROJECT_ID} \ --zone ${ZONE} \ --accelerator-type ${ACCELERATOR_TYPE} \ --runtime-version ${RUNTIME_VERSION} \ --valid-until-duration ${VALID_UNTIL_DURATION} \ --service-account ${SERVICE_ACCOUNT} \ --${QUOTA_TYPE}
gcloud alpha compute tpus queued-resources describe ${QUEUED_RESOURCE_ID} \ --project ${PROJECT_ID} \ --zone ${ZONE}
When the queued resource is in the
ACTIVEstate, the output will be similar to the following:  state: ACTIVE 
Install JAX and its library.
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \ --project=${PROJECT_ID} \ --zone=${ZONE} \ --worker=all \ --command='pip install ""jax[tpu]==0.4.16"" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html'
Download HuggingFace
[repo](https://github.com/huggingface/transformers.git)and install requirements.
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \ --project=${PROJECT_ID} \ --zone=${ZONE} \ --worker=all \ --command='git clone https://github.com/huggingface/transformers.git && cd transformers && pip install . && pip install -r examples/flax/_tests_requirements.txt && pip install --upgrade huggingface-hub urllib3 zipp && pip install tensorflow && pip install -r examples/flax/language-modeling/requirements.txt'
Download configs to train the model.
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \ --project=${PROJECT_ID} \ --zone=${ZONE} \ --worker=all \ --command='cd transformers/examples/flax/language-modeling && gsutil cp -r gs://cloud-tpu-tpuvm-artifacts/v5litepod-preview/jax/gpt .'
Train the model
Train the model with a pre-mapped buffer at 4GB.
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME}
--project=${PROJECT_ID}
--zone=${ZONE}
--worker=all
--command='cd transformers/examples/flax/language-modeling && TPU_PREMAPPED_BUFFER_SIZE=4294967296 JAX_PLATFORMS=tpu python3 run_clm_flax.py --output_dir=./gpt --model_type=gpt2 --config_name=./gpt --tokenizer_name=./gpt --dataset_name=oscar --dataset_config_name=unshuffled_deduplicated_no --do_train --do_eval --block_size=512 --per_device_train_batch_size=4 --per_device_eval_batch_size=4 --learning_rate=5e-3 --warmup_steps=1000 --adam_beta1=0.9 --adam_beta2=0.98 --weight_decay=0.01 --overwrite_output_dir --num_train_epochs=3 --logging_steps=500 --eval_steps=2500'
Delete the TPU and queued-resource
Delete your TPU and queued-resource at the end of your session.
gcloud alpha compute tpus tpu-vm delete ${TPU_NAME}
--project=${PROJECT_ID}
--zone=${ZONE}
--quiet
gcloud alpha compute tpus queued-resources delete ${QUEUED_RESOURCE_ID}
--project ${PROJECT_ID}
--zone ${ZONE}
--quiet
Benchmarking results for GPT2
The training script ran on v5litepod-4, v5litepod-16, and v5litepod-64. The following table shows the throughputs.
|v5litepod-4
|v5litepod-16
|v5litepod-64
|Epoch
|3
|3
|3
|Global batch size
|64
|64
|64
|Throughput (examples/sec)
|74.60
|72.97
|72.62
PyTorch/XLA
Train ResNet using the PJRT runtime
PyTorch/XLA is migrating from XRT to PjRt from PyTorch 2.0+. Here are the updated instructions to setup v5e for PyTorch/XLA training workloads.
Setup
Create environment variables:
export PROJECT_ID=your_project_ID export ACCELERATOR_TYPE=v5litepod-16 export ZONE=us-west4-a export RUNTIME_VERSION=v2-alpha-tpuv5-lite export SERVICE_ACCOUNT=your_service_account export TPU_NAME=tpu-name export QUEUED_RESOURCE_ID=queued_resource_id export QUOTA_TYPE=quota_type export VALID_UNTIL_DURATION=1d
gcloud alpha compute tpus queued-resources create ${QUEUED_RESOURCE_ID} \ --node-id ${TPU_NAME} \ --project ${PROJECT_ID} \ --zone ${ZONE} \ --accelerator-type ${ACCELERATOR_TYPE} \ --runtime-version ${RUNTIME_VERSION} \ --valid-until-duration ${VALID_UNTIL_DURATION} \ --service-account ${SERVICE_ACCOUNT} \ --{QUOTA_TYPE}
gcloud alpha compute tpus queued-resources describe ${QUEUED_RESOURCE_ID} \ --project ${PROJECT_ID} \ --zone ${ZONE}
When the queued resource is in the
ACTIVEstate, the output will be similar to the following:  state: ACTIVE 
Install Torch/XLA specific dependencies
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \ --project ${PROJECT_ID} \ --zone ${ZONE} \ --worker all --command=' sudo apt-get update -y sudo apt-get install libomp5 -y pip3 install mkl mkl-include pip3 install tf-nightly tb-nightly tbp-nightly pip3 install numpy sudo apt-get install libopenblas-dev -y pip3 install torch~=2.1.0 torchvision torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html pip3 install torch_xla[tpu] -f https://storage.googleapis.com/libtpu-releases/index.html'
Train the ResNet model
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \ --project ${PROJECT_ID} \ --zone ${ZONE} \ --worker=all \ --command=' date export PJRT_DEVICE=TPU_C_API export PT_XLA_DEBUG=0 export USE_TORCH=ON export XLA_USE_BF16=1 export LIBTPU_INIT_ARGS=--xla_jf_auto_cross_replica_sharding export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH export TPU_LIBRARY_PATH=/home/$USER/.local/lib/python3.10/site-packages/libtpu/libtpu.so git clone https://github.com/pytorch/xla.git cd xla/ git reset --hard caf5168785c081cd7eb60b49fe4fffeb894c39d9 python3 test/test_train_mp_imagenet.py --model=resnet50 --fake_data --num_epochs=1 num_workers=16 --log_steps=300 --batch_size=64 --profile'
Delete the TPU and queued-resource
Delete your TPU and queued-resource at the end of your session.
gcloud alpha compute tpus tpu-vm delete ${TPU_NAME} \ --project=${PROJECT_ID} \ --zone=${ZONE} \ --quiet
gcloud alpha compute tpus queued-resources delete ${QUEUED_RESOURCE_ID} \ --project ${PROJECT_ID} \ --zone ${ZONE} \ --quiet
Benchmark Result
The following table shows the benchmark throughputs.
|Accelerator type
|Throughput (examples/second)
|v5litepod-4
|4240 ex/s
|v5litepod-16
|10,810 ex/s
|v5litepod-64
|46,154 ex/s
Train GPT2 on v5e
This tutorial will cover how to run GPT2 on v5e using
HuggingFace
[repo](https://github.com/huggingface/transformers.git)
on PyTorch/XLA using the [wikitext dataset.](https://huggingface.co/datasets/wikitext)
Setup
Create environment variables:
export PROJECT_ID=your_project_ID export ACCELERATOR_TYPE=v5litepod-16 export ZONE=us-west4-a export RUNTIME_VERSION=v2-alpha-tpuv5-lite export SERVICE_ACCOUNT=your_service_account export TPU_NAME=your_tpu_name export QUEUED_RESOURCE_ID=queued_resource_id export QUOTA_TYPE=quota_type export VALID_UNTIL_DURATION=1d
gcloud alpha compute tpus queued-resources create ${QUEUED_RESOURCE_ID} \ --node-id ${TPU_NAME} \ --project ${PROJECT_ID} \ --zone ${ZONE} \ --accelerator-type ${ACCELERATOR_TYPE} \ --runtime-version ${RUNTIME_VERSION} \ --valid-until-duration ${VALID_UNTIL_DURATION} \ --service-account ${SERVICE_ACCOUNT} \ --${QUOTA_TYPE}
gcloud alpha compute tpus queued-resources describe ${QUEUED_RESOURCE_ID} \ --project ${PROJECT_ID} \ --zone ${ZONE}
When the queued resource is in the
ACTIVEstate, the output will be similar to the following: state: ACTIVE
Install torch/xla dependencies.
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \ --project=${PROJECT_ID} \ --zone=${ZONE} \ --worker=all \ --command=' sudo apt-get -y update sudo apt install -y libopenblas-base pip3 install torchvision pip3 uninstall -y torch pip3 install torch~=2.1.0 torchvision torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html pip3 install torch_xla[tpu] -f https://storage.googleapis.com/libtpu-releases/index.html'
Download HuggingFace
[repo](https://github.com/huggingface/transformers.git)and install requirements.
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \ --project=${PROJECT_ID} \ --zone=${ZONE} \ --worker=all \ --command=' git clone https://github.com/pytorch/xla.git pip install --upgrade accelerate git clone https://github.com/huggingface/transformers.git cd transformers git checkout ebdb185befaa821304d461ed6aa20a17e4dc3aa2 pip install . git log -1 pip install datasets evaluate scikit-learn '
Download configs of the pre-trained model.
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \ --project=${PROJECT_ID} \ --zone=${ZONE} \ --worker=all \ --command=' gsutil cp -r gs://cloud-tpu-tpuvm-artifacts/config/xl-ml-test/pytorch/gpt2/my_config_2.json transformers/examples/pytorch/language-modeling/ gsutil cp gs://cloud-tpu-tpuvm-artifacts/config/xl-ml-test/pytorch/gpt2/fsdp_config.json transformers/examples/pytorch/language-modeling/ '
Train the model
Train the 2B model using a batch size of 16.
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME}
--project=${PROJECT_ID}
--zone=${ZONE}
--worker=all
--command=' export PJRT_DEVICE=TPU_C_API cd transformers/ export LD_LIBRARY_PATH=/usr/local/lib/ export PT_XLA_DEBUG=0 export USE_TORCH=ON
python3 examples/pytorch/xla_spawn.py \ --num_cores=4 \ examples/pytorch/language-modeling/run_clm.py \ --num_train_epochs=3 \ --dataset_name=wikitext \ --dataset_config_name=wikitext-2-raw-v1 \ --per_device_train_batch_size=16 \ --per_device_eval_batch_size=16 \ --do_train \ --do_eval \ --logging_dir=./tensorboard-metrics \ --cache_dir=./cache_dir \ --output_dir=/tmp/test-clm \ --overwrite_output_dir \ --cache_dir=/tmp \ --config_name=examples/pytorch/language-modeling/my_config_2.json \ --tokenizer_name=gpt2 \ --block_size=1024 \ --optim=adafactor \ --adafactor=true \ --save_strategy=no \ --logging_strategy=no \ --fsdp=full_shard \ --fsdp_config=examples/pytorch/language-modeling/fsdp_config.json '
Delete the TPU and queued-resource
Delete your TPU and queued-resource at the end of your session.
gcloud alpha compute tpus tpu-vm delete ${TPU_NAME}
--project=${PROJECT_ID}
--zone=${ZONE}
--quiet
gcloud alpha compute tpus queued-resources delete ${QUEUED_RESOURCE_ID}
--project ${PROJECT_ID}
--zone ${ZONE}
--quiet
Benchmark Result
The training script ran on v5litepod-4, v5litepod-16, and v5litepod-64. The following table shows the benchmark throughputs for different accelerator types.
|v5litepod-4
|v5litepod-16
|v5litepod-64
|Epoch
|3
|3
|3
|config
|600M
|2B
|16B
|Global batch size
|64
|128
|256
|Throughput (examples/sec)
|66
|77
|31
Train ViT on v5e
This tutorial will cover how to run VIT on v5e using
the HuggingFace
[repo](https://github.com/huggingface/transformers.git)
on PyTorch/XLA on the [cifar10 dataset](https://huggingface.co/datasets/cifar10).
Setup
Create environment variables:
export PROJECT_ID=your_project_ID export ACCELERATOR_TYPE=v5litepod-16 export ZONE=us-west4-a export RUNTIME_VERSION=v2-alpha-tpuv5-lite export SERVICE_ACCOUNT=your_service_account export TPU_NAME=tpu-name export QUEUED_RESOURCE_ID=queued_resource_id export QUOTA_TYPE=quota_type export VALID_UNTIL_DURATION=1d
gcloud alpha compute tpus queued-resources create ${QUEUED_RESOURCE_ID} \ --node-id ${TPU_NAME} \ --project ${PROJECT_ID} \ --zone ${ZONE} \ --accelerator-type ${ACCELERATOR_TYPE} \ --runtime-version ${RUNTIME_VERSION} \ --valid-until-duration ${VALID_UNTIL_DURATION} \ --service-account ${SERVICE_ACCOUNT} \ --${QUOTA_TYPE}
gcloud alpha compute tpus queued-resources describe ${QUEUED_RESOURCE_ID} \ --project ${PROJECT_ID} \ --zone ${ZONE}
When the queued resource is in the
ACTIVEstate, the output will be similar to the following:  state: ACTIVE 
Install torch/xla dependencies
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} --project ${PROJECT_ID} --zone ${ZONE} --worker=all --command=' sudo apt-get update -y sudo apt-get install libomp5 -y pip3 install mkl mkl-include pip3 install tf-nightly tb-nightly tbp-nightly pip3 install numpy sudo apt-get install libopenblas-dev -y pip3 install torch~=2.1.0 torchvision torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html pip3 install torch_xla[tpu] -f https://storage.googleapis.com/libtpu-releases/index.html'
Download HuggingFace
[repo](https://github.com/huggingface/transformers.git)and install requirements.
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \ --project=${PROJECT_ID} \ --zone=${ZONE} \ --worker=all \ --command="" git clone https://github.com/suexu1025/transformers.git vittransformers; \ cd vittransformers; \ pip3 install .; \ pip3 install datasets; \ wget https://github.com/pytorch/xla/blob/master/scripts/capture_profile.py""
Train the model
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \ --project=${PROJECT_ID} \ --zone=${ZONE} \ --worker=all \ --command="" export PJRT_DEVICE=TPU_C_API export PT_XLA_DEBUG=0 export USE_TORCH=ON export TF_CPP_MIN_LOG_LEVEL=0 export XLA_USE_BF16=1 export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH export TPU_LIBRARY_PATH=/home/$USER/.local/lib/python3.10/site-packages/libtpu/libtpu.so cd vittransformers python3 -u examples/pytorch/xla_spawn.py --num_cores 4 examples/pytorch/image-pretraining/run_mae.py --dataset_name=cifar10 \ --remove_unused_columns=False \ --label_names=pixel_values \ --mask_ratio=0.75 \ --norm_pix_loss=True \ --do_train=true \ --do_eval=true \ --base_learning_rate=1.5e-4 \ --lr_scheduler_type=cosine \ --weight_decay=0.05 \ --num_train_epochs=3 \ --warmup_ratio=0.05 \ --per_device_train_batch_size=8 \ --per_device_eval_batch_size=8 \ --logging_strategy=steps \ --logging_steps=30 \ --evaluation_strategy=epoch \ --save_strategy=epoch \ --load_best_model_at_end=True \ --save_total_limit=3 \ --seed=1337 \ --output_dir=MAE \ --overwrite_output_dir=true \ --logging_dir=./tensorboard-metrics \ --tpu_metrics_debug=true""
Delete the TPU and queued-resource
Delete your TPU and queued-resource at the end of your session.
gcloud alpha compute tpus tpu-vm delete ${TPU_NAME} --project=${PROJECT_ID} --zone=${ZONE} --quiet
gcloud alpha compute tpus queued-resources delete ${QUEUED_RESOURCE_ID} --project ${PROJECT_ID} --zone ${ZONE} --quiet
Benchmark Result
The following table shows the benchmark throughputs for different accelerator types.
|v5litepod-4
|v5litepod-16
|v5litepod-64
|Epoch
|3
|3
|3
|Global batch size
|32
|128
|512
|Throughput (examples/sec)
|201
|657
|2,844
TensorFlow 2.x
Train Resnet on a single host v5e
This tutorial describes how to train ImageNet on
v5litepod-4 or
v5litepod-8
using a fake dataset. If you want to use a different dataset, refer to
[Preparing the dataset](https://github.com/google/flax/blob/main/examples/imagenet/README.md#preparing-the-dataset).
Set up
Create environment variables:
export PROJECT_ID=your-project-ID export ACCELERATOR_TYPE=v5litepod-4 export ZONE=us-east1-c export RUNTIME_VERSION=tpu-vm-tf-2.15.0-pjrt export TPU_NAME=your-tpu-name export QUEUED_RESOURCE_ID=your-queued-resource-id export QUOTA_TYPE=quota-type
ACCELERATOR_TYPEcan be either
v5litepod-4or
v5litepod-8.
gcloud alpha compute tpus queued-resources create ${QUEUED_RESOURCE_ID} \ --node-id ${TPU_NAME} \ --project ${PROJECT_ID} \ --zone ${ZONE} \ --accelerator-type ${ACCELERATOR_TYPE} \ --runtime-version ${RUNTIME_VERSION} \ --${QUOTA_TYPE}
gcloud alpha compute tpus queued-resources describe ${QUEUED_RESOURCE_ID} \ --project ${PROJECT_ID} \ --zone ${ZONE}
Connect to your TPU using SSH
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \ --project ${PROJECT_ID} \ --zone ${ZONE}
Set some environment variables
export MODELS_REPO=/usr/share/tpu/models export PYTHONPATH=""${MODELS_REPO}:${PYTHONPATH}"" export MODEL_DIR=gcp-directory-to-store-model export DATA_DIR=gs://cloud-tpu-test-datasets/fake_imagenet export NEXT_PLUGGABLE_DEVICE_USE_C_API=true export TF_PLUGGABLE_DEVICE_LIBRARY_PATH=/lib/libtpu.so
Change to the models repository directory and install requirements.
cd ${MODELS_REPO} && git checkout r2.15.0 pip install -r official/requirements.txt
Train the model
Run the training script.
python3 official/vision/train.py \ --tpu=local \ --experiment=resnet_imagenet \ --mode=train_and_eval \ --config_file=official/vision/configs/experiments/image_classification/imagenet_resnet50_tpu.yaml \ --model_dir=${MODEL_DIR} \ --params_override=""runtime.distribution_strategy=tpu,task.train_data.input_path=${DATA_DIR}/train*,task.validation_data.input_path=${DATA_DIR}/validation*,task.train_data.global_batch_size=2048,task.validation_data.global_batch_size=2048,trainer.train_steps=100""
Delete the TPU and queued-resource
Delete your TPU
gcloud alpha compute tpus tpu-vm delete ${TPU_NAME} \ --project=${PROJECT_ID} \ --zone=${ZONE} \ --quiet
Delete your queued resource request
gcloud alpha compute tpus queued-resources delete ${QUEUED_RESOURCE_ID} \ --project ${PROJECT_ID} \ --zone ${ZONE} \ --quiet
Train Resnet on a multi host v5e
This tutorial describes how to train ImageNet on
v5litepod-16 or larger using
a fake dataset. If you want to use a different dataset, see
[Preparing the dataset](https://github.com/google/flax/blob/main/examples/imagenet/README.md#preparing-the-dataset).
Create environment variables:
export PROJECT_ID=your_project_ID export ACCELERATOR_TYPE=v5litepod-16 export ZONE=us-east1-c export RUNTIME_VERSION=tpu-vm-tf-2.15.0-pod-pjrt export TPU_NAME=your_tpu_name export QUEUED_RESOURCE_ID=your-queued-resource-id export QUOTA_TYPE=quota-type
ACCELERATOR_TYPEcan be either
v5litepod-16or larger.
gcloud alpha compute tpus queued-resources create ${QUEUED_RESOURCE_ID} \ --node-id ${TPU_NAME} \ --project ${PROJECT_ID} \ --zone ${ZONE} \ --accelerator-type ${ACCELERATOR_TYPE} \ --runtime-version ${RUNTIME_VERSION} \ --${QUOTA_TYPE}
gcloud alpha compute tpus queued-resources describe ${QUEUED_RESOURCE_ID} \ --project ${PROJECT_ID} \ --zone ${ZONE}
Connect to your TPU (worker zero) using SSH
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \ --project ${PROJECT_ID} \ --zone ${ZONE}
Set some environment variables
export MODELS_REPO=/usr/share/tpu/models export PYTHONPATH=""${MODELS_REPO}:${PYTHONPATH}"" export MODEL_DIR=gcp-directory-to-store-model export DATA_DIR=gs://cloud-tpu-test-datasets/fake_imagenet export TPU_LOAD_LIBRARY=0 export TPU_NAME=your_tpu_name
Change to the models repository directory and install requirements.
cd $MODELS_REPO && git checkout r2.15.0 pip install -r official/requirements.txt
Train the model
Run the training script.
python3 official/vision/train.py \ --tpu=${TPU_NAME} \ --experiment=resnet_imagenet \ --mode=train_and_eval \ --model_dir=${MODEL_DIR} \ --params_override=""runtime.distribution_strategy=tpu,task.train_data.input_path=${DATA_DIR}/train*, task.validation_data.input_path=${DATA_DIR}/validation*""
Delete the TPU and queued-resource
Delete your TPU
gcloud alpha compute tpus tpu-vm delete ${TPU_NAME} \ --project=${PROJECT_ID} \ --zone=${ZONE} \ --quiet
Delete your queued resource request
gcloud alpha compute tpus queued-resources delete ${QUEUED_RESOURCE_ID} \ --project ${PROJECT_ID} \ --zone ${ZONE} \ --quiet",Cloud TPU v5e training | Google Cloud,
id,url,body,title,description
40,https://cloud.google.com/tpu/docs/jax-pods,"Run JAX code on TPU Pod slices
After you have your JAX code running on a single TPU board, you can scale up
your code by running it on a
[TPU Pod slice](https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu_slices).
TPU Pod slices are multiple TPU boards connected to each other over dedicated
high-speed network connections. This document is an introduction to running JAX
code on TPU Pod slices; for more in-depth information, see [Using JAX in multi-host and multi-process environments](https://jax.readthedocs.io/en/latest/multi_process.html). [Using an NFS for data storage](/tpu/docs/training-on-tpu-pods#nfs-data).
Create a TPU Pod slice
Before running the commands in this document, make sure you have followed the
instructions in
[Set up an account and Cloud TPU project](https://cloud.google.com/tpu/docs/setup-gcp-account).
Run the following commands on your local machine.
Create a TPU Pod slice using the
gcloud command. For example, to create a
v4-32 Pod slice use the following command:
$ gcloud compute tpus tpu-vm create tpu-name \
--zone=us-central2-b \
--accelerator-type=v4-32 \
--version=tpu-ubuntu2204-base
Install JAX on the Pod slice
After creating the TPU Pod slice, you must install JAX on all hosts in the TPU
Pod slice. You can install JAX on all hosts with a single command using the
--worker=all option:
gcloud compute tpus tpu-vm ssh tpu-name \ --zone=us-central2-b --worker=all --command=""pip install \ --upgrade 'jax[tpu]>0.3.0' \ -f https://storage.googleapis.com/jax-releases/libtpu_releases.html""
Run JAX code on the Pod slice
To run JAX code on a TPU Pod slice, you must run the code on each host in the
TPU Pod slice. The
jax.device_count() call stops responding until it is
called on each host in the Pod slice. The following example illustrates how to
run a simple JAX calculation on a TPU Pod slice.
Prepare the code
You need
gcloud version >= 344.0.0 (for the
command).
Use
[scp](/sdk/gcloud/reference/alpha/compute/tpus/tpu-vm/scp)
gcloud --version to check your
gcloud version, and
run
gcloud components upgrade, if needed.
Create a file called
example.py with the following code:
# The following code snippet will be run on all TPU hosts
import jax
# The total number of TPU cores in the Pod
device_count = jax.device_count()
# The number of TPU cores attached to this host
local_device_count = jax.local_device_count()
# The psum is performed over all mapped devices across the Pod
xs = jax.numpy.ones(jax.local_device_count())
r = jax.pmap(lambda x: jax.lax.psum(x, 'i'), axis_name='i')(xs)
# Print from a single host to avoid duplicated output
if jax.process_index() == 0:
print('global device count:', jax.device_count())
print('local device count:', jax.local_device_count())
print('pmap result:', r)
Copy
example.py to all TPU worker VMs in the Pod slice
$ gcloud compute tpus tpu-vm scp example.py tpu-name: \
--worker=all \
--zone=us-central2-b
If you have not previously used the
scp command, you might see an
error similar to the following:
ERROR: (gcloud.alpha.compute.tpus.tpu-vm.scp) SSH Key is not present in the SSH agent. Please run `ssh-add /.../.ssh/google_compute_engine` to add it, and try again.
To resolve the error, run the
ssh-add command as displayed in the
error message and rerun the command.
Run the code on the Pod slice
Launch the
example.py program on every VM:
$ gcloud compute tpus tpu-vm ssh tpu-name \
--zone=us-central2-b \
--worker=all \
--command=""python3 example.py""
Output (produced with a v4-32 Pod slice):
global device count: 16
local device count: 4
pmap result: [16. 16. 16. 16.]
Clean up
When you are done, you can release your TPU VM resources using the
gcloud
command:
$ gcloud compute tpus tpu-vm delete tpu-name \
--zone=us-central2-b",Run JAX code on TPU Pod slices | Google Cloud,
id,url,body,title,description
75,https://cloud.google.com/tpu/docs/reference/rest/v1alpha1/projects.locations.acceleratorTypes,"REST Resource: projects.locations.acceleratorTypes
Stay organized with collections
Save and categorize content based on your preferences.
Resource: AcceleratorType
A accelerator type that a Node can be configured with.
|JSON representation
|
{
""name"": string,
""type"": string
}
|Fields
|
name
|
string
The resource name.
|
type
|
string
the accelerator type.
|
Methods
|
|
Gets AcceleratorType.
|
|
Lists accelerator types supported by this API.
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2022-11-28 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",REST Resource: projects.locations.acceleratorTypes | Cloud TPU | Google Cloud,
id,url,body,title,description
96,https://cloud.google.com/tpu/docs/reference/rest/v1alpha1/projects.locations.tensorflowVersions,"REST Resource: projects.locations.tensorflowVersions
Stay organized with collections
Save and categorize content based on your preferences.
Resource: TensorFlowVersion
A tensorflow version that a Node can be configured with.
|JSON representation
|
{
""name"": string,
""version"": string
}
|Fields
|
name
|
string
The resource name.
|
version
|
string
the tensorflow version.
|
Methods
|
|
Gets TensorFlow Version.
|
|
Lists TensorFlow versions supported by this API.
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2022-11-28 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",REST Resource: projects.locations.tensorflowVersions | Cloud TPU | Google Cloud,
id,url,body,title,description
57,https://cloud.google.com/tpu/docs/reference/rest/v1alpha1/projects.locations.nodes/get,"Method: projects.locations.nodes.get
Stay organized with collections
Save and categorize content based on your preferences.
Gets the details of a node.
HTTP request
GET https://tpu.googleapis.com/v1alpha1/{name=projects/*/locations/*/nodes/*}
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
string
Required. The resource name.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[Node](/tpu/docs/reference/rest/v1alpha1/projects.locations.nodes#Node)
Authorization scopes
Requires one of the following OAuth scopes:
-
https://www.googleapis.com/auth/cloud.tpu
-
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Method: projects.locations.nodes.get | Cloud TPU | Google Cloud,
id,url,body,title,description
142,https://cloud.google.com/tpu/docs/reference/rest/v1alpha1/projects.locations.tensorflowVersions/get,"Method: projects.locations.tensorflowVersions.get
Stay organized with collections
Save and categorize content based on your preferences.
HTTP request
GET https://tpu.googleapis.com/v1alpha1/{name=projects/*/locations/*/tensorflowVersions/*}
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
string
Required. The resource name.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[TensorFlowVersion](/tpu/docs/reference/rest/v1alpha1/projects.locations.tensorflowVersions#TensorFlowVersion)
Authorization scopes
Requires one of the following OAuth scopes:
-
https://www.googleapis.com/auth/cloud.tpu
-
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Method: projects.locations.tensorflowVersions.get | Cloud TPU | Google Cloud,
id,url,body,title,description
169,https://cloud.google.com/tpu/docs/reference/rest/v1alpha1/projects.locations.nodes/list,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Query parameters](#body.QUERY_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [Try it!](#try-it)
Lists nodes.
HTTP request
GET https://tpu.googleapis.com/v1alpha1/{parent=projects/*/locations/*}/nodes
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
parent
|
Required. The parent resource name.
Query parameters
|Parameters
|
pageSize
|
The maximum number of items to return.
|
pageToken
|
The nextPageToken value returned from a previous List request, if any.
Request body
The request body must be empty.
Response body
Response for
.
[nodes.list](/tpu/docs/reference/rest/v1alpha1/projects.locations.nodes/list#google.cloud.tpu.v1alpha1.Tpu.ListNodes)
If successful, the response body contains data with the following structure:
|JSON representation
|
{
""nodes"": [
{
object (
|Fields
|
nodes[]
|
The listed nodes.
|
nextPageToken
|
The next page token or empty if none.
|
unreachable[]
|
Locations that could not be reached.
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).",Method: projects.locations.nodes.list | Cloud TPU | Google Cloud,
id,url,body,title,description
183,https://cloud.google.com/tpu/docs/reference/rest/v2alpha1/projects.locations.reservations,"Resource
There is no persistent data associated with this resource.
|
Methods
|
|Retrieves the reservations for the given project in the given location.
There is no persistent data associated with this resource.
|
Methods
|
|Retrieves the reservations for the given project in the given location.
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2024-01-12 UTC.",REST Resource: projects.locations.reservations | Cloud TPU | Google Cloud,
id,url,body,title,description
103,https://cloud.google.com/tpu/docs/reference/rest/v1/projects.locations.tensorflowVersions,"REST Resource: projects.locations.tensorflowVersions
Stay organized with collections
Save and categorize content based on your preferences.
Resource: TensorFlowVersion
A tensorflow version that a Node can be configured with.
|JSON representation
|
{
""name"": string,
""version"": string
}
|Fields
|
name
|
string
The resource name.
|
version
|
string
the tensorflow version.
|
Methods
|
|
Gets TensorFlow Version.
|
|
List TensorFlow versions supported by this API.
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2022-11-28 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",REST Resource: projects.locations.tensorflowVersions | Cloud TPU | Google Cloud,
id,url,body,title,description
106,https://cloud.google.com/tpu/docs/imagenet-setup,"Download, pre-process, and upload the ImageNet dataset
This topic describes how to download, pre-process, and upload the ImageNet dataset to use with Cloud TPU VM architecture.ImageNet is an image database. The images in the database are organized into a hierarchy, with each node of the hierarchy depicted by hundreds and thousands of images.
The size of the ImageNet database means it can take a considerable amount of time to train a model. An alternative is to use a demonstration version of the dataset, referred to as fake_imagenet. This demonstration version lets you test the model, while reducing the storage and time requirements associated with using the full ImageNet database.
Pre-processing the full ImageNet dataset
The ImageNet dataset consists of three parts, training data, validation data, and image labels.
The training data contains 1000 categories and 1.2 million images, packaged for easy downloading. The validation and test data are not contained in the ImageNet training data (duplicates have been removed).
The validation and test data consists of 150,000 photographs, collected from
[Flickr](https://www.flickr.com/) and other search engines, hand labeled with
the presence or absence of 1000 object categories. The 1000 object categories
contain both internal nodes and leaf nodes of ImageNet, but do not overlap with
each other. A random subset of 50,000 of the images with labels has been
released as validation data along with a list of the 1000 categories. The
remaining images are used for evaluation and have been released without labels.
Steps to pre-processing the full ImageNet dataset
There are five steps to preparing the full ImageNet dataset for use by a Machine Learning model:
- Verify that you have space on the download target.
- Set up the target directories.
- Register on the ImageNet site and request download permission.
Download the dataset to local disk or VM instance.
Run the pre-processing and upload script.
Verify space requirements
Whether you download the dataset to your local machine or to a VM instance, you need about 300 GB of space available on the download target.
The default disk allocation for a TPU VM is 100 GB. Since the download to your TPU VM requires
300 GB, if you are going download to your TPU VM instance, you will need
to add a
[persistent disk](https://cloud.google.com/tpu/docs/setup-persistent-disk) and with 200 GB of additional space to complete the download. On a TPU VM, you
can check your available storage with the
df -ha command.
When adding a persistent disk be sure to:
- Set When deleting instance to Delete disk to ensure that the disk is deleted when you delete the VM.
- Make a note of the path to your new disk. For example:
/mnt/disks/mnt-dir.
Set up the target directories
On your local machine or VM instance, set up the directory structure to store the downloaded data.
Create and export a home directory for the ImageNet dataset.
Create a directory, for example,
imagenetunder your home directory on your download target (local machine or TPU VM). Under this directory, create two sub directories:
trainand
validation. Export the home directory as IMAGENET_HOME:
export IMAGENET_HOME=~/imagenet
Register and request permission to download the dataset
- Register on the
[Imagenet website](http://image-net.org/). You cannot download the dataset until ImageNet confirms your registration and sends you a confirmation email. If you do not get the confirmation email within a couple of days, contact [ImageNet support](mailto:support@image-net.org)to see why your registration has not been confirmed. Once your registration is confirmed, you can download the dataset. The Cloud TPU tutorials that use the ImageNet dataset use the images from the ImageNet Large Scale Visual Recognition Challenge 2012 (ILSVRC2012).
Download the ImageNet dataset
From the
[LSRVC 2012 download site](https://image-net.org/challenges/LSVRC/2012/2012-downloads.php), go to the Images section on the page and right-click ""Training images (Task 1 & 2)"". The URL to download the largest part of the training set. Save the URL.
Right-click ""Training images (Task 3)"" to get the URL for the second training set. Save the URL.
Right-click ""Validation images (all tasks)"" to get the URL for the validation dataset. Save the URL.
If you download the ImageNet files to your local machine, you need to copy the directories on your local machine to the corresponding
$IMAGENET_HOMEdirectory on your VM instance. Copying the ImageNet dataset from local host to your VM instance takes approximately 13 hours.
Before copying the ImageNet dataset to your VM, you need to identify the name of your VM instance. To do that, run the following
gcloud describecommand and locate your VM instance name in the output.
gcloud compute tpus tpu-vm describe tpu-name --zone=zone
This generates output containing a line that includes your VM instance name (an example of a VM instance name is shown in bold below):
tpuVmSelflink: https://www.googleapis.com/compute/v1/projects/project-name/zones/zone/instances/t1v-n-2b9b54cd-w-0
Use the following command to copy the files under ~/imagenet on your local machine to
$IMAGENET_HOMEon your VM.
gcloud compute scp --recurse $IMAGENET_HOME username@vm-instance-name:~/imagenet
From $IMAGENET_HOME, use
wgetto download the training and validation files using the saved URLs.
The ""Training images (Task 1 & 2)"" file is the large training set. It is 138 GB and if you are downloading to your VM using the Cloud Shell, the download takes approximately 40 hours. If the Cloud Shell loses its connection to the VM, you can prepend
nohupto the command or use
[screen](https://linuxize.com/post/how-to-use-linux-screen/).
cd `$IMAGENET_HOME` \ nohup wget http://image-net.org/challenges/LSVRC/2012/dd31405981ef5f776aa17412e1f0c112/ILSVRC2012_img_train.tar
This command downloads a large tar file: ILSVRC2012_img_train.tar.
From
$IMAGENET_HOMEon the VM, extract the individual training directories into the
$IMAGENET_HOME/traindirectory using the following command. The extraction takes between 1 - 3 hours.
tar xf $IMAGENET_HOME/ILSVRC2012_img_train.tar -C $IMAGENET_HOME/traintar xf ILSVRC2012_img_train.tar
Extract the individual training tar files located in the
$IMAGENET_HOME/traindirectory, as shown in the following script:
cd `$IMAGENET_HOME/train` for f in *.tar; do d=`basename $f .tar` mkdir $d tar xf $f -C $d done
Delete the tar files after you have extracted them to free up disk space.
The ""Training images (Task 3)"" file is 728 MB and takes just a few minutes to download so you do not need to take precautions against losing the Cloud Shell connection.
When you download this file, it extracts the individual training directories into the existing
$IMAGENET_HOME/traindirectory.
wget http://www.image-net.org/challenges/LSVRC/2012/dd31405981ef5f776aa17412e1f0c112/ILSVRC2012_img_train_t3.tar
When downloading the ""Validation images (all tasks)"" file, your Cloud Shell may disconnect. You can use
nohupor
[screen](https://linuxize.com/post/how-to-use-linux-screen/)to prevent Cloud Shell from disconnecting.
wget http://www.image-net.org/challenges/LSVRC/2012/dd31405981ef5f776aa17412e1f0c112/ILSVRC2012_img_val.tar
This download takes about 30 minutes. When you download this file, it extracts the individual validation directories into the
$IMAGENET_HOME/validationdirectory.
If you downloaded the validation files to your local machine, you need to copy the
$IMAGENET_HOME/validationdirectory on your local machine to the
$IMAGENET_HOME/validationdirectory on your VM instance. This copy operation takes about 30 minutes.
Download the labels file.
wget -O $IMAGENET_HOME/synset_labels.txt \ https://raw.githubusercontent.com/tensorflow/models/master/research/slim/datasets/imagenet_2012_validation_synset_labels.txt
If you downloaded the labels file to your local machine, you need to copy it to the
$IMAGENET_HOMEdirectory on your local machine to
$IMAGENET_HOMEon your VM instance. This copy operation takes a few seconds.
The training subdirectory names (for example, n03062245) are ""WordNet IDs"" (wnid). The
[ImageNet API](https://image-net.org/download-attributes.php)shows the mapping of WordNet IDs to their associated validation labels in the
synset_labels.txtfile. A synset in this context is a visually similar group of images.
Process the Imagenet dataset and, optionally, upload to Cloud Storage
Download the
imagenet_to_gcs.pyscript from GitHub:
wget https://raw.githubusercontent.com/tensorflow/tpu/master/tools/datasets/imagenet_to_gcs.py
If you are uploading the dataset to Cloud Storage, specify the storage bucket location to upload the ImageNet dataset:
export STORAGE_BUCKET=gs://bucket-name
If you are uploading the dataset to your local machine or VM, specify a data directory to hold the dataset:
(vm)$ export DATA_DIR=$IMAGENET_HOME/dataset-directory
Run the script to pre-process the raw dataset as TFRecords and upload it to Cloud Storage using the following command:
python3 imagenet_to_gcs.py \ --project=$PROJECT \ --gcs_output_path=$STORAGE_BUCKET \ --raw_data_dir=$IMAGENET_HOME \ --local_scratch_dir=$IMAGENET_HOME/tf_records
The script generates a set of directories (for both training and validation) of the form:
${DATA_DIR}/train-00000-of-01024
${DATA_DIR}/train-00001-of-01024
...
${DATA_DIR}/train-01023-of-01024
and
${DATA_DIR}/validation-00000-of-00128
S{DATA_DIR}/validation-00001-of-00128
...
${DATA_DIR}/validation-00127-of-00128
After the data has been uploaded to your Cloud bucket, run your model and set
--data_dir=${DATA_DIR}.","Download, pre-process, and upload the ImageNet dataset | Cloud TPU | Google Cloud",
id,url,body,title,description
116,https://cloud.google.com/tpu/docs/reference/rest/v2alpha1/projects.locations.reservations/list,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Query parameters](#body.QUERY_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [Reservation](#Reservation) [Standard](#Standard) [CapacityUnits](#CapacityUnits) [Usage](#Usage) [Try it!](#try-it)
Retrieves the reservations for the given project in the given location.
HTTP request
GET https://tpu.googleapis.com/v2alpha1/{parent=projects/*/locations/*}/reservations
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
parent
|
Required. The parent for reservations.
Query parameters
|Parameters
|
pageSize
|
The maximum number of items to return.
|
pageToken
|
The nextPageToken value returned from a previous List request, if any.
Request body
The request body must be empty.
Response body
Response for
.
[reservations.list](/tpu/docs/reference/rest/v2alpha1/projects.locations.reservations/list#google.cloud.tpu.v2alpha1.Tpu.ListReservations)
If successful, the response body contains data with the following structure:
|JSON representation
|
{
""reservations"": [
{
object (
|Fields
|
reservations[]
|
The listed reservations.
|
nextPageToken
|
The next page token or empty if none.
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
Reservation
A reservation describes the amount of a resource 'allotted' for a defined period of time.
|JSON representation
|
{ ""name"": string, // Union field
|Fields
|
name
|
The reservation name with the format: projects/{projectID}/locations/{location}/reservations/{reservationID}
|
Union field
|
standard
|
Standard
|JSON representation
|
{ ""size"": integer, ""capacityUnits"": enum (
|Fields
|
size
|
The size of the reservation, in the units specified in the 'capacityUnits' field.
|
capacityUnits
|
|
resourceType
|
The resource type of the reservation.
|
interval
|
The start and end time of the reservation.
|
usage
|
CapacityUnits
The units capacity for this reservation is measured in.
|Enums
|
CAPACITY_UNITS_UNSPECIFIED
|The capacity units is not known/set.
|
CORES
|The capacity unit is set to CORES.
|
CHIPS
|The capacity unit is set to CHIPS.
Usage
|JSON representation
|
{ ""total"": string }
|Fields
|
total
|
The real-time value of usage within the reservation, with the unit specified in field capacityUnits.",Method: projects.locations.reservations.list | Cloud TPU | Google Cloud,
id,url,body,title,description
145,https://cloud.google.com/tpu/docs/reference/rest/v1/projects.locations.nodes/create,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Query parameters](#body.QUERY_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [Try it!](#try-it)
Creates a node.
HTTP request
POST https://tpu.googleapis.com/v1/{parent=projects/*/locations/*}/nodes
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
parent
|
Required. The parent resource name.
Query parameters
|Parameters
|
nodeId
|
The unqualified resource name.
Request body
The request body contains an instance of
.
[Node](/tpu/docs/reference/rest/v1/projects.locations.nodes#Node)
Response body
If successful, the response body contains a newly created instance of
.
[Operation](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse#Operation)
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).",Method: projects.locations.nodes.create | Cloud TPU | Google Cloud,
id,url,body,title,description
168,https://cloud.google.com/tpu/docs/kubernetes-engine-setup,"Run Cloud TPU applications on GKE
This guide describes how to:
[Set up a Cloud TPU configuration](#setup-cloud-tpu)in preparation to run it under [Google Kubernetes Engine](/kubernetes-engine/docs) [Create a GKE cluster](#new-cluster)with Cloud TPU support [Use TensorBoard to visualize Cloud TPU metrics](/tpu/docs/cloud-tpu-tools)and analyze performance [Build and containerize your model in Docker](#docker)
For more information about TPU VM architectures, see
[System Architecture](/tpu/docs/system-architecture-tpu-vm).
This guide can only be used with the TPU Nodes architecture.
Benefits of running Cloud TPU applications on GKE
Cloud TPU training applications can be configured to run
in GKE
[containers within GKE Pods](https://kubernetes.io/docs/concepts/workloads/pods/).
When they are, you see the following benefits:
Improved workflow setup and management: GKE manages the TPU lifecycle. Once Cloud TPU initialization and training are set up with GKE, your workloads can be repeated and managed by GKE, including Job failure recovery.
Optimized cost: You only pay for the TPU while the Job is active. GKE automatically creates and deletes TPUs according to a Pod's resource requirements.
Flexible usage: It's a small change in your Pod spec to request a different hardware accelerator (CPU, GPU, or TPU):
kind: Pod metadata: name: example-tpu annotations: # The Cloud TPUs that will be created for this Job will support # TensorFlow 2.6.0. This version MUST match the # TensorFlow version that your model is built on. tf-version.cloud-tpus.google.com: ""2.6.0"" spec: containers: - name: example-container resources: limits: cloud-tpus.google.com/v2: 8 # See the line above for TPU, or below for CPU / GPU. # cpu: 2 # nvidia.com/gpu: 1
Scalability: GKE provides APIs (
[Job](/kubernetes-engine/docs/how-to/scaling-apps)and [Deployment](/kubernetes-engine/docs/how-to/deploying-workloads-overview)) that can scale to hundreds of GKE Pods and TPU Nodes.
Fault tolerance: GKE's
[Job API](https://kubernetes.io/docs/concepts/workloads/controllers/job/), along with the TensorFlow checkpoint mechanism, provide the run-to-completion semantic. Your training Jobs will automatically rerun with the latest state read from the checkpoint if failures occur on the VM instances or Cloud TPU nodes.
Cloud TPU and GKE configuration requirements and limitations
Note the following when defining your GKE configuration:
- Cloud TPU is not supported in Windows Server node pools.
- You must create your GKE cluster and node pools in a zone
where Cloud TPU is available. You must also create the
Cloud Storage buckets to hold your training data and models in the
same region as your GKE cluster.
See the
[types and zones](/tpu/docs/types-zones#types)document for a list of the available zones.
- You must use RFC 1918 compliant IP addresses for your GKE clusters. For more
information, see
[GKE Networking](/kubernetes-engine/docs/concepts/network-overview).
- Each container can request at most one Cloud TPU, but multiple containers in a Pod can request a Cloud TPU each.
Before you begin
-
Sign in to your Google Cloud account. If you're new to
Google Cloud,
[create an account](https://console.cloud.google.com/freetrial)to evaluate how our products perform in real-world scenarios. New customers also get $300 in free credits to run, test, and deploy workloads.
-
In the Google Cloud console, on the project selector page, select or
[create a Google Cloud project](/resource-manager/docs/creating-managing-projects).
-
[Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console).
-
In the Google Cloud console, on the project selector page, select or
[create a Google Cloud project](/resource-manager/docs/creating-managing-projects).
-
[Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console).
-
Enable the following APIs on the Google Cloud console:
When you use Cloud TPU with GKE, your project
uses billable components of Google Cloud. Check
[Cloud TPU pricing](/tpu/docs/pricing) and
[GKE pricing](/kubernetes-engine/pricing) to
estimate your costs, and follow the instructions to [clean
up resources](#cleanup) when you've finished with them.
Create a new cluster with Cloud TPU support
Use the following instructions to set up your environment and create a GKE cluster with Cloud TPU support, using the gcloud CLI:
Install the
gcloudcomponents, which you need for running GKE with Cloud TPU:
$ gcloud components install kubectl
Configure
gcloudwith your Google Cloud project ID:
$ gcloud config set project project-name
Replace
project-namewith the name of your Google Cloud project.
The first time you run this command in a new Cloud Shell VM, an
Authorize Cloud Shellpage is displayed. Click
Authorizeat the bottom of the page to allow
gcloudto make Google Cloud API calls with your credentials.
Configure
gcloudwith the zone where you plan to use a Cloud TPU resource. This example uses
us-central1-b, but you can use a TPU in
[any supported zone](/tpu/docs/types-zones).
$ gcloud config set compute/zone us-central1-b
Use the
[command to create a cluster on GKE with support for Cloud TPU.](/sdk/gcloud/reference/container/clusters/create)
gcloud container clusters create
$ gcloud container clusters create cluster-name \ --release-channel=stable \ --scopes=cloud-platform \ --enable-ip-alias \ --enable-tpu
Command flag descriptions
- release-channel
[Release channels](/kubernetes-engine/docs/concepts/release-channels#channels)provide a way to manage automatic upgrades for your clusters. When you create a new cluster, you can choose its release channel. Your cluster will only be upgraded to versions offered in that channel.
- scopes
- Ensures that all nodes in the cluster have access to your
Cloud Storage bucket. The cluster and the storage bucket must
be in the same project for this to work. Note that the Kubernetes Pods
by default inherit the scopes of the nodes to which they are deployed.
Therefore,
scopes=cloud-platformgives all Kubernetes Pods running in the cluster the
cloud-platformscope. If you want to limit the access on a per Pod basis, see the GKE guide to
[authenticating with Service Accounts](/kubernetes-engine/docs/tutorials/authenticating-to-cloud-platform).
- enable-ip-alias
- Indicates that the cluster uses
[alias IP ranges](/kubernetes-engine/docs/how-to/alias-ips). This is required for using Cloud TPU on GKE.
- enable-tpu
- Indicates that the cluster must support Cloud TPU.
- tpu-ipv4-cidr (optional, not specified above)
- Indicates the CIDR range to use for Cloud TPU. Specify
the
IP_RANGEin the form of
IP/20, such as
10.100.0.0/20. If you don't specify this flag, a
/20size CIDR range is automatically allocated and assigned.
When the cluster has been created, you should see a message similar to the following:
NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS cluster-name us-central1-b 1.16.15-gke.4901 34.71.245.25 n1-standard-1 1.16.15-gke.4901 3 RUNNING
Request a Cloud TPU in your Kubernetes Pod spec
In your Kubernetes Pod spec:
You must build your models in your containers using the same TensorFlow version. See the
[supported versions](/tpu/docs/supported-tpu-configurations#tpu_software_versions).
Specify the Cloud TPU resource in the
limitssection under the
resourcefield in the container spec.
Note that the unit of the Cloud TPU resource is the number of Cloud TPU cores. The following table lists examples of valid resource requests. See
[TPU types and zones](/tpu/docs/types-zones)for a complete list of valid TPU resources.
If the resource intended to be used is a Cloud TPU Pod,
[request quota](/tpu/docs/quota#pod-quota)since the default quota for Cloud TPU Pod is zero. Resource Request Cloud TPU type cloud-tpus.google.com/v2: 8 A Cloud TPU v2 device (8 cores) cloud-tpus.google.com/preemptible-v2: 8 A Preemptible Cloud TPU v2 device (8 cores) cloud-tpus.google.com/v3: 8 A Cloud TPU v3 device (8 cores) cloud-tpus.google.com/preemptible-v3: 8 A Preemptible Cloud TPU v3 device (8 cores) cloud-tpus.google.com/v2: 32 A v2-32 Cloud TPU Pod (32 cores) cloud-tpus.google.com/v3: 32 A v3-32 Cloud TPU Pod (32 cores)
For more information on specifying resources and limits in the Pod spec, see the
[Kubernetes documentation](https://kubernetes.io/docs/home/).
The following sample Pod spec requests one Preemptible Cloud TPU v2-8 TPU with TensorFlow 2.12.0.
The lifetime of Cloud TPU nodes is bound to the Kubernetes Pods that request them. The Cloud TPU is created on demand when the Kubernetes Pod is scheduled, and recycled when the Kubernetes Pod is deleted.
apiVersion: v1
kind: Pod
metadata:
name: gke-tpu-pod
annotations:
# The Cloud TPUs that will be created for this Job will support
# TensorFlow 2.6.0. This version MUST match the
# TensorFlow version that your model is built on.
tf-version.cloud-tpus.google.com: ""2.6.0""
spec:
restartPolicy: Never
containers:
- name: gke-tpu-container
# The official TensorFlow 2.6.0 image.
# https://hub.docker.com/r/tensorflow/tensorflow
image: tensorflow/tensorflow:2.6.0
command:
- python
- -c
- |
import tensorflow as tf
print(""Tensorflow version "" + tf.__version__)
tpu = tf.distribute.cluster_resolver.TPUClusterResolver('$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)')
print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])
tf.config.experimental_connect_to_cluster(tpu)
tf.tpu.experimental.initialize_tpu_system(tpu)
strategy = tf.distribute.TPUStrategy(tpu)
@tf.function
def add_fn(x,y):
z = x + y
return z
x = tf.constant(1.)
y = tf.constant(1.)
z = strategy.run(add_fn, args=(x,y))
print(z)
resources:
limits:
# Request a single Preemptible v2-8 Cloud TPU device to train the model.
cloud-tpus.google.com/preemptible-v2: 8
Creating the Job
Follow these steps to create the Job in the GKE cluster,
and to
[install kubectl](https://kubernetes.io/docs/tasks/tools/)
Using a text editor, create a Pod spec,
example-job.yaml, and copy and paste in the Pod spec shown previously.
Run the Job:
$ kubectl create -f example-job.yaml
pod ""gke-tpu-pod"" created
This command creates the job that automatically schedules the Pod.
Verify that the GKE Pod has been scheduled and Cloud TPU nodes have been provisioned. A GKE Pod requesting Cloud TPU nodes can be pending for 5 minutes before running. You will see output similar to the following until the GKE Pod is scheduled.
$ kubectl get pods -w NAME READY STATUS RESTARTS AGE gke-tpu-pod 0/1 Pending 0 1m
After approximately 5 minutes, you should see something like this:
NAME READY STATUS RESTARTS AGE gke-tpu-pod 0/1 Pending 0 21s gke-tpu-pod 0/1 Pending 0 2m18s gke-tpu-pod 0/1 Pending 0 2m18s gke-tpu-pod 0/1 ContainerCreating 0 2m18s gke-tpu-pod 1/1 Running 0 2m48s gke-tpu-pod 0/1 Completed 0 3m8s
You need to use Ctrl-C to exit 'kubectl get' command.
You can print log information and retrieve more detailed information about each GKE Pod using the following
kubectlcommands. For example, to see the log output for your GKE Pod, use:
$ kubectl logs gke-tpu-pod
You should see output similar to the following:
2021-09-24 18:55:25.400699: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-09-24 18:55:25.405947: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.16.2:8470} 2021-09-24 18:55:25.406058: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:32769} 2021-09-24 18:55:28.091729: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.16.2:8470} 2021-09-24 18:55:28.091896: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:32769} 2021-09-24 18:55:28.092579: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:427] Started server with target: grpc://localhost:32769 Tensorflow version 2.6.0 Running on TPU ['10.0.16.2:8470'] PerReplica:{ 0: tf.Tensor(2.0, shape=(), dtype=float32), 1: tf.Tensor(2.0, shape=(), dtype=float32), 2: tf.Tensor(2.0, shape=(), dtype=float32), 3: tf.Tensor(2.0, shape=(), dtype=float32), 4: tf.Tensor(2.0, shape=(), dtype=float32), 5: tf.Tensor(2.0, shape=(), dtype=float32), 6: tf.Tensor(2.0, shape=(), dtype=float32), 7: tf.Tensor(2.0, shape=(), dtype=float32) }
To see a full description of the GKE Pod, use:
$ kubectl describe pod gke-tpu-pod
See
[Application Introspection and Debugging](https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application-introspection/)for more details.
Build and containerize your model in Docker image
Refer to
[build and containerize your own model](/build/docs/building/build-containers)
for details on this process.
Enable Cloud TPU support on an existing cluster
To enable Cloud TPU support on an existing GKE cluster, perform the following steps in the Google Cloud CLI:
Enable Cloud TPU support:
gcloud beta container clusters update cluster-name --enable-tpu
Replace cluster-name with the name of your cluster.
Update the kubeconfig entry:
gcloud container clusters get-credentials cluster-name
Setting a custom CIDR range
By default, GKE allocates a CIDR block with the size of
/20
for the TPUs provisioned by the cluster. You can specify a custom CIDR range for
the Cloud TPU by running the following command:
gcloud beta container clusters update cluster-name \ --enable-tpu \ --tpu-ipv4-cidr 10.100.0.0/20
Replace the following:
- cluster-name: the name of your existing cluster.
- 10.100.0.0/20: your custom CIDR range.
Using existing CIDR ranges with Shared VPC
Follow the guide on TPU in GKE clusters using a Shared VPC to verify the correct configuration for your Shared VPC.
Disabling Cloud TPU in a cluster
To disable Cloud TPU support on an existing GKE cluster, perform the following steps in the Google Cloud CLI:
Verify that none of your workloads are using Cloud TPU:
$ kubectl get tpu
Disable Cloud TPU support in your cluster:
$ gcloud beta container clusters update cluster-name --no-enable-tpu
Replace cluster-name with the name of your cluster.
For zonal clusters this operation takes around 5 minutes, and for regional clusters, this operation takes roughly 15 minutes, depending on the cluster's region.
Once the operations completes with no errors, you can verify that the TPUs provisioned by the cluster have been removed:
$ gcloud compute tpus list
The names of the TPUs created by Cloud TPU have the following format:
$ gke-cluster-name-cluster-id-tpu-tpu-id
Replace the following:
- cluster-name: the name of your existing cluster.
- cluster-id: the ID of your existing cluster.
- tpu-id: the ID of the Cloud TPU.
If any TPUs appear, you can manually delete them by running:
$ gcloud compute tpus delete gke-cluster-name-cluster-id-tpu-tpu-id
Clean up
When you've finished with Cloud TPU on GKE, clean up the resources to avoid incurring extra charges to your Cloud Billing account.
Run the following command to delete your GKE cluster, replacing
cluster-namewith your cluster name, and
project-namewith your Google Cloud project name:
$ gcloud container clusters delete cluster-name \ --project=project-name --zone=us-central1-b
When you've finished examining the data, use the
gsutilcommand to delete the Cloud Storage bucket that you created. Replace
bucket-namewith the name of your Cloud Storage bucket:
$ gsutil rm -r gs://bucket-name",Run Cloud TPU applications on GKE | Google Cloud,
id,url,body,title,description
136,https://cloud.google.com/tpu/docs/reference/rest/v2alpha1/projects.locations.runtimeVersions,"REST Resource: projects.locations.runtimeVersions
Stay organized with collections
Save and categorize content based on your preferences.
Resource: RuntimeVersion
A runtime version that a Node can be configured with.
|JSON representation
|
{
""name"": string,
""version"": string
}
|Fields
|
name
|
string
The resource name.
|
version
|
string
The runtime version.
|
Methods
|
|
Gets a runtime version.
|
|
Lists runtime versions supported by this API.
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2022-11-28 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",REST Resource: projects.locations.runtimeVersions | Cloud TPU | Google Cloud,
id,url,body,title,description
109,https://cloud.google.com/tpu/docs/reference/rest/v2alpha1/projects.locations.acceleratorTypes/list,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Query parameters](#body.QUERY_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [Try it!](#try-it)
Lists accelerator types supported by this API.
HTTP request
GET https://tpu.googleapis.com/v2alpha1/{parent=projects/*/locations/*}/acceleratorTypes
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
parent
|
Required. The parent resource name.
Query parameters
|Parameters
|
pageSize
|
The maximum number of items to return.
|
pageToken
|
The nextPageToken value returned from a previous List request, if any.
|
filter
|
List filter.
|
orderBy
|
Sort results.
Request body
The request body must be empty.
Response body
Response for
.
[acceleratorTypes.list](/tpu/docs/reference/rest/v2alpha1/projects.locations.acceleratorTypes/list#google.cloud.tpu.v2alpha1.Tpu.ListAcceleratorTypes)
If successful, the response body contains data with the following structure:
|JSON representation
|
{
""acceleratorTypes"": [
{
object (
|Fields
|
acceleratorTypes[]
|
The listed nodes.
|
nextPageToken
|
The next page token or empty if none.
|
unreachable[]
|
Locations that could not be reached.
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).",Method: projects.locations.acceleratorTypes.list | Cloud TPU | Google Cloud,
id,url,body,title,description
92,https://cloud.google.com/tpu/docs/supported-patches,"Stay organized with collections
Save and categorize content based on your preferences.
Supported TensorFlow versions
|TensorFlow version
|Support start
|Support end
|TF 2.15.0
|November 13, 2023
|(End date not yet set)
|TF 2.14.1
|December 4, 2023
|(End date not yet set)
|TF 2.14.0
|September 26, 2023
|(End date not yet set)
|TF 2.13.1
|October 5, 2023
|(End date not yet set)
|TF 2.13.0
|July 10, 2023
|(End date not yet set)
|TF 2.12.1
|July 21, 2023
|(End date not yet set)
|TF 2.12.0
|March 27, 2023
|(End date not yet set)
|TF 2.11.1
|March 31, 2023
|(End date not yet set)
|TF 2.11.0
|December 1, 2022
|(End date not yet set)
|TF 2.10.1
|December 14, 2022
|(End date not yet set)
|TF 2.10.0
|September 19, 2022
|(End date not yet set)
|TF 2.9.3
|December 14, 2022
|(End date not yet set)
|TF 2.9.2
|September 13, 2022
|(End date not yet set)
|TF 2.9.1
|May 27, 2022
|(End date not yet set)
|TF 2.8.4
|Dec 14, 2022
|(End date not yet set)
|TF 2.8.3
|September 13, 2022
|(End date not yet set)
|TF 2.8.2
|May 27, 2022
|(End date not yet set)
|TF 2.8.0
|February 3, 2022
|(End date not yet set)
|TF 2.7.4
|September 13, 2022
|(End date not yet set)
|TF 2.7.1
|March 9, 2022
|(End date not yet set)
|TF 2.7.0
|January 13, 2022
|(End date not yet set)
|TF 2.6.3
|March 21, 2022
|June 30, 2023
|TF 2.6.2
|December 2, 2021
|June 30, 2023
|TF 2.6.0
|August 12, 2021
|June 30, 2023
|TF 2.5.3
|March 9, 2022
|June 30, 2023
|TF 2.5.2
|December 2, 2021
|June 30, 2023
|TF 2.5.1
|August 24, 2021
|June 30, 2023
|TF 2.5.0
|May 17, 2021
|June 30, 2023
|TF 2.4.4
|December 2, 2021
|June 30, 2023
|TF 2.4.3
|August 24, 2021
|June 30, 2023
|TF 2.4.2
|June 22, 2021
|June 30, 2023
|TF 2.4.1
|January 22, 2020
|June 30, 2023
|TF 2.4.0
|January 5, 2021
|June 30, 2023
|TF 2.3.4
|August 24, 2021
|June 30, 2023
|TF 2.3.3
|June 17, 2021
|June 30, 2023
|TF 2.3.2
|January 19, 2021
|June 30, 2023
|TF 2.3.1
|October 1, 2020
|June 30, 2023
|TF 2.3
|July 28, 2020
|June 30, 2023
|TF 2.2.3
|June 17, 2021
|June 30, 2023
|TF 2.2.2
|January 13, 2021
|June 30, 2023
|TF 2.2.1
|October 7, 2020
|June 30, 2023
|TF 2.1.4
|June 17, 2021
|June 30, 2023
|TF 2.1.3
|January 21, 2021
|June 30, 2023
|TF 2.1.2
|October 30, 2020
|June 30, 2023
|TF 2.1
|January 9, 2020
|June 30, 2023
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2024-01-31 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Supported TensorFlow versions | Cloud TPU | Google Cloud,
id,url,body,title,description
89,https://cloud.google.com/tpu/docs/reference/rest/v2/projects.locations/generateServiceIdentity,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [ServiceIdentity](#ServiceIdentity) [Try it!](#try-it)
Generates the Cloud TPU service identity for the project.
HTTP request
POST https://tpu.googleapis.com/v2/{parent=projects/*/locations/*}:generateServiceIdentity
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
parent
|
Required. The parent resource name.
Request body
The request body must be empty.
Response body
Response for
.
[locations.generateServiceIdentity](/tpu/docs/reference/rest/v2/projects.locations/generateServiceIdentity#google.cloud.tpu.v2.Tpu.GenerateServiceIdentity)
If successful, the response body contains data with the following structure:
|JSON representation
|
{
""identity"": {
object (
|Fields
|
identity
|
ServiceIdentity that was created or retrieved.
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
ServiceIdentity
The per-product per-project service identity for Cloud TPU service.
|JSON representation
|
{ ""email"": string }
|Fields
|
email
|
The email address of the service identity.",Method: projects.locations.generateServiceIdentity | Cloud TPU | Google Cloud,
id,url,body,title,description
148,https://cloud.google.com/tpu/docs/intro-to-tpu,"Introduction to Cloud TPU
Tensor Processing Units (TPUs) are Google's custom-developed application-specific
integrated circuits (ASICs) used to accelerate machine learning workloads. For
more detailed information about TPU hardware, see
[System Architecture](/tpu/docs/system-architecture-tpu-vm).
Cloud TPU is a web service that makes TPUs available as scalable computing
resources on Google Cloud.
TPUs train your models more efficiently using hardware designed for performing large matrix operations often found in machine learning algorithms. TPUs have on-chip high-bandwidth memory (HBM) letting you use larger models and batch sizes. TPUs can be connected in groups called Pods that scale up your workloads with little to no code changes.
How does it work?
To understand how TPUs work, it helps to understand how other accelerators address the computational challenges of training ML models.
How a CPU works
A CPU is a general-purpose processor based on the
[von Neumann architecture](https://en.wikipedia.org/wiki/Von_Neumann_architecture).
That means a CPU works with software and memory like this:
The greatest benefit of CPUs is their flexibility. You can load any kind of software on a CPU for many different types of applications. For example, you can use a CPU for word processing on a PC, controlling rocket engines, executing bank transactions, or classifying images with a neural network.
A CPU loads values from memory, performs a calculation on the values and stores
the result back in memory for every calculation. Memory access is slow
when compared to the calculation speed and can limit the total throughput of CPUs.
This is often referred to as the
[von Neumann bottleneck](https://en.wikipedia.org/wiki/Von_Neumann_architecture#Von_Neumann_bottleneck).
How a GPU works
To gain higher throughput, GPUs contain thousands of
[Arithmetic Logic Units](https://en.wikipedia.org/wiki/Arithmetic_logic_unit)
(ALUs) in a single processor. A modern GPU usually contains between 2,5005,000 ALUs.
The large number of processors means you can execute thousands of multiplications
and additions simultaneously.
This GPU architecture works well on applications with massive parallelism, such as matrix operations in a neural network. In fact, on a typical training workload for deep learning, a GPU can provide an order of magnitude higher throughput than a CPU.
But, the GPU is still a general-purpose processor that has to support many different applications and software. Therefore, GPUs have the same problem as CPUs. For every calculation in the thousands of ALUs, a GPU must access registers or shared memory to read operands and store the intermediate calculation results.
How a TPU works
Google designed Cloud TPUs as a matrix processor specialized for neural network workloads. TPUs can't run word processors, control rocket engines, or execute bank transactions, but they can handle massive matrix operations used in neural networks at fast speeds.
The primary task for TPUs is matrix processing, which is a combination
of multiply and accumulate operations. TPUs contain thousands of
multiply-accumulators that are directly connected to each other to form a large
physical matrix. This is called a
[systolic array](https://en.wikipedia.org/wiki/Systolic_array)
architecture. Cloud TPU v3, contain two systolic arrays of
128 x 128 ALUs, on a single processor.
The TPU host streams data into an infeed queue. The TPU loads data from the infeed queue and stores them in HBM memory. When the computation is completed, the TPU loads the results into the outfeed queue. The TPU host then reads the results from the outfeed queue and stores them in the host's memory.
To perform the matrix operations, the TPU loads the parameters from HBM memory into the Matrix Multiplication Unit (MXU).
Then, the TPU loads data from HBM memory. As each multiplication is executed, the result is passed to the next multiply-accumulator. The output is the summation of all multiplication results between the data and parameters. No memory access is required during the matrix multiplication process.
As a result, TPUs can achieve a high-computational throughput on neural network calculations.
XLA compiler
Code that runs on TPUs must be compiled by the accelerator linear algebra (XLA)
compiler.
[XLA](https://www.tensorflow.org/performance/xla/) is a just-in-time
compiler that takes the graph emitted by an ML framework application and
compiles the linear algebra, loss, and gradient components of the graph into
TPU machine code. The rest of the program runs on the TPU host machine. The XLA
compiler is part of the TPU runtime that runs on a TPU host machine.
When to use TPUs
Cloud TPUs are optimized for specific workloads. In some situations, you might want to use GPUs or CPUs on Compute Engine instances to run your machine learning workloads. In general, you can decide what hardware is best for your workload based on the following guidelines:
CPUs
- Quick prototyping that requires maximum flexibility
- Simple models that do not take long to train
- Small models with small, effective batch sizes
- Models that contain many
[custom TensorFlow operations written in C++](https://www.tensorflow.org/guide/create_op)
- Models that are limited by available I/O or the networking bandwidth of the host system
GPUs
- Models with a significant number of custom TensorFlow/PyTorch/JAX operations that must run at least partially on CPUs
- Models with TensorFlow ops that are not available on Cloud TPU
(see the list of
[available TensorFlow ops](/tpu/docs/tensorflow-ops))
- Medium-to-large models with larger effective batch sizes
TPUs
- Models dominated by matrix computations
- Models with no custom TensorFlow/PyTorch/JAX operations inside the main training loop
- Models that train for weeks or months
- Large models with large effective batch sizes
Cloud TPUs are not suited to the following workloads:
- Linear algebra programs that require frequent branching or contain many element-wise algebra operations
- Workloads that access memory in a sparse manner
- Workloads that require high-precision arithmetic
- Neural network workloads that contain custom operations in the main training loop
Best practices for model development
A program whose computation is dominated by non-matrix operations such as add, reshape, or concatenate, will likely not achieve high MXU utilization. The following are some guidelines to help you choose and build models that are suitable for Cloud TPU.
Layout
The XLA compiler performs code transformations, including tiling a matrix
multiply into smaller blocks, to efficiently execute computations on the
matrix unit (MXU). The structure of the MXU hardware, a 128x128
[systolic
array](https://en.wikipedia.org/wiki/Systolic_array), and the design of TPU's
memory subsystem, which prefers dimensions that are multiples of 8, are used by
the XLA compiler for tiling efficiency. Consequently, certain layouts are more
conducive to tiling, while others require reshapes to be performed before they
can be tiled. Reshape operations are often memory bound on the Cloud TPU.
Shapes
The XLA compiler compiles an ML graph just in time for the first batch. If any subsequent batches have different shapes, the model doesn't work. (Re-compiling the graph every time the shape changes is too slow.) Therefore, any model that has tensors with dynamic shapes that change at runtime isn't well suited to TPUs.
Padding
A high performing Cloud TPU program is one where the dense compute can be easily tiled into 128x128 chunks. When a matrix computation cannot occupy an entire MXU, the compiler pads tensors with zeroes. There are two drawbacks to padding:
- Tensors padded with zeroes under-utilize the TPU core.
- Padding increases the amount of on-chip memory storage required for a tensor and can lead to an out-of-memory error in the extreme case.
While padding is automatically performed by the XLA compiler when necessary, one
can determine the amount of padding performed by means of the
[op_profile](/tpu/docs/cloud-tpu-tools#interpreting_the_results_1)
tool. You can avoid padding by picking tensor dimensions that are well suited
to TPUs.
Dimensions
Choosing suitable tensor dimensions goes a long way in extracting maximum performance from the TPU hardware, particularly the MXU. The XLA compiler attempts to use either the batch size or the feature dimension to maximally utilize the MXU. Therefore, one of these must be a multiple of 128. Otherwise, the compiler will pad one of them to 128. Ideally, batch size as well as feature dimensions should be multiples of 8, which enables extracting high performance from the memory subsystem.
VPC Service Controls integration
Cloud TPU VPC Service Controls enables you to define security perimeters around
your Cloud TPU resources and control the movement of data across the perimeter
boundary. To learn more about VPC Service Controls, see
[VPC Service Controls overview](/vpc-service-controls/docs/overview).
To learn about the limitations in using Cloud TPU with VPC Service Controls, see
[supported products and limitations](/vpc-service-controls/docs/supported-products).
Edge TPU
Machine learning models trained in the cloud increasingly need to run inferencing ""at the edge""that is, on devices that operate on the edge of the Internet of Things (IoT). These devices include sensors and other smart devices that gather real-time data, make intelligent decisions, and then take action or communicate their information to other devices or the cloud.
Because such devices must operate on limited power (including battery power), Google designed the Edge TPU coprocessor to accelerate ML inferencing on low-power devices. An individual Edge TPU can perform 4 trillion operations per second (4 TOPS), using only 2 watts of powerin other words, you get 2 TOPS per watt. For example, the Edge TPU can execute state-of-the-art mobile vision models such as MobileNet V2 at almost 400 frames per second, and in a power efficient manner.
This low-power ML accelerator augments Cloud TPU and Cloud IoT to provide an end-to-end (cloud-to-edge, hardware + software) infrastructure that facilitates your AI-based solutions.
The Edge TPU is available for your own prototyping and production devices in
several form-factors, including a single-board computer, a system-on-module, a
PCIe/M.2 card, and a surface-mounted module. For more information about the
Edge TPU and all available products,
[visit coral.ai](https://coral.ai/).
Getting started with Cloud TPU
[Set up a Google Cloud account](/tpu/docs/setup-gcp-account) [Activate the Cloud TPU API](/tpu/docs/activate-apis) [Grant Cloud TPU access to your Cloud Storage buckets](/tpu/docs/storage-buckets) [Run a basic calculation on a TPU](/tpu/docs/quick-starts) [Train a reference model on a TPU](/tpu/docs/tutorials) [Analyze your model](/tpu/docs/cloud-tpu-tools)
Requesting help
Contact
[Cloud TPU support](https://cloud.google.com/tpu/docs/getting-support).
If you have an active Google Cloud project, be prepared to provide
the following information:
- Your Google Cloud project ID
- Your TPU node name, if exists
- Other information you want to provide
What's next?
Looking to learn more about Cloud TPU? The following resources may help:",Introduction to Cloud TPU | Google Cloud,
id,url,body,title,description
199,https://cloud.google.com/tpu/docs/reference/rest/v1/projects.locations.acceleratorTypes/get,"Method: projects.locations.acceleratorTypes.get
Stay organized with collections
Save and categorize content based on your preferences.
HTTP request
GET https://tpu.googleapis.com/v1/{name=projects/*/locations/*/acceleratorTypes/*}
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
string
Required. The resource name.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[AcceleratorType](/tpu/docs/reference/rest/v1/projects.locations.acceleratorTypes#AcceleratorType)
Authorization scopes
Requires one of the following OAuth scopes:
-
https://www.googleapis.com/auth/cloud.tpu
-
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Method: projects.locations.acceleratorTypes.get | Cloud TPU | Google Cloud,
id,url,body,title,description
73,https://cloud.google.com/tpu/docs/reference/rest/v2alpha1/projects.locations.operations/delete,"Send feedback
Method: projects.locations.operations.delete
Stay organized with collections
Save and categorize content based on your preferences.
Deletes a long-running operation. This method indicates that the client is no longer interested in the operation result. It does not cancel the operation. If the server doesn't support this method, it returns
google.rpc.Code.UNIMPLEMENTED.
HTTP request
DELETE https://tpu.googleapis.com/v2alpha1/{name=projects/*/locations/*/operations/*}
The URL uses
gRPC Transcoding syntax.
Path parameters
Parameters
name
string
The name of the operation resource to be deleted.
Request body
The request body must be empty.
Response body
If successful, the response body is empty.
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
Authentication Overview.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]
Need to tell us more?",Method: projects.locations.operations.delete | Cloud TPU | Google Cloud,
id,url,body,title,description
123,https://cloud.google.com/tpu/docs/reference/rest/v2/projects.locations.nodes/create,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Query parameters](#body.QUERY_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [Try it!](#try-it)
Creates a node.
HTTP request
POST https://tpu.googleapis.com/v2/{parent=projects/*/locations/*}/nodes
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
parent
|
Required. The parent resource name.
Query parameters
|Parameters
|
nodeId
|
The unqualified resource name.
Request body
The request body contains an instance of
.
[Node](/tpu/docs/reference/rest/v2/projects.locations.nodes#Node)
Response body
If successful, the response body contains a newly created instance of
.
[Operation](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse#Operation)
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).",Method: projects.locations.nodes.create | Cloud TPU | Google Cloud,
id,url,body,title,description
68,https://cloud.google.com/tpu/docs/reference/rest/v2/projects.locations.operations/get,"Method: projects.locations.operations.get
Stay organized with collections
Save and categorize content based on your preferences.
Gets the latest state of a long-running operation. Clients can use this method to poll the operation result at intervals as recommended by the API service.
HTTP request
GET https://tpu.googleapis.com/v2/{name=projects/*/locations/*/operations/*}
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
string
The name of the operation resource.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[Operation](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse#Operation)
Authorization scopes
Requires one of the following OAuth scopes:
-
https://www.googleapis.com/auth/cloud.tpu
-
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Method: projects.locations.operations.get | Cloud TPU | Google Cloud,
id,url,body,title,description
139,https://cloud.google.com/tpu/docs/reference/rest/v2alpha1/projects.locations.runtimeVersions/list,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Query parameters](#body.QUERY_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [Try it!](#try-it)
Lists runtime versions supported by this API.
HTTP request
GET https://tpu.googleapis.com/v2alpha1/{parent=projects/*/locations/*}/runtimeVersions
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
parent
|
Required. The parent resource name.
Query parameters
|Parameters
|
pageSize
|
The maximum number of items to return.
|
pageToken
|
The nextPageToken value returned from a previous List request, if any.
|
filter
|
List filter.
|
orderBy
|
Sort results.
Request body
The request body must be empty.
Response body
Response for
.
[runtimeVersions.list](/tpu/docs/reference/rest/v2alpha1/projects.locations.runtimeVersions/list#google.cloud.tpu.v2alpha1.Tpu.ListRuntimeVersions)
If successful, the response body contains data with the following structure:
|JSON representation
|
{
""runtimeVersions"": [
{
object (
|Fields
|
runtimeVersions[]
|
The listed nodes.
|
nextPageToken
|
The next page token or empty if none.
|
unreachable[]
|
Locations that could not be reached.
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).",Method: projects.locations.runtimeVersions.list | Cloud TPU | Google Cloud,
id,url,body,title,description
62,https://cloud.google.com/tpu/docs/run-calculation-jax,"Run a calculation on a Cloud TPU VM using JAX
This document provides a brief introduction to working with JAX and Cloud TPU.
Before you follow this quickstart, you must create a Google Cloud Platform
account, install the Google Cloud CLI, and configure the
gcloud command.
For more information, see
[Set up an account and a Cloud TPU project](/tpu/docs/setup-gcp-account).
Install the Google Cloud CLI
The Google Cloud CLI contains tools and libraries for interacting with Google
Cloud products and services. For more information, see
[Installing the Google Cloud CLI](/sdk/docs/install).
Configure the
gcloud command
Run the following commands to configure
gcloud to use your Google Cloud
project and
install components needed for the TPU VM preview.
$ gcloud config set account your-email-account $ gcloud config set project your-project-id
Enable the Cloud TPU API
Enable the Cloud TPU API using the following
gcloudcommand in
[Cloud Shell](https://console.cloud.google.com/?cloudshell=true). (You may also enable it from the [Google Cloud console](https://console.cloud.google.com/)).
$ gcloud services enable tpu.googleapis.com
Run the following command to create a service identity.
$ gcloud beta services identity create --service tpu.googleapis.com
Create a Cloud TPU VM with
gcloud
With Cloud TPU VMs, your model and code run directly on the TPU host machine. You SSH directly into the TPU host. You can run arbitrary code, install packages, view logs, and debug code directly on the TPU Host.
Create your TPU VM by running the following command from a Google Cloud Shell or your computer terminal where the
[Google Cloud CLI](https://cloud.google.com/sdk/docs/install)is installed.
(vm)$ gcloud compute tpus tpu-vm create tpu-name \ --zone=us-central2-b \ --accelerator-type=v4-8 \ --version=tpu-ubuntu2204-base
Connect to your Cloud TPU VM
SSH into your TPU VM by using the following command:
$ gcloud compute tpus tpu-vm ssh tpu-name --zone=us-central2-b
Required fields
tpu_name
- The name of the TPU VM to which you are connecting.
zone
- The
[zone](https://cloud.google.com/tpu/docs/types-zones-tpu-vm)where you created your Cloud TPU.
Install JAX on your Cloud TPU VM
(vm)$ pip install jax[tpu] -f https://storage.googleapis.com/jax-releases/libtpu_releases.html
System check
Verify that JAX can access the TPU and can run basic operations:
Start the Python 3 interpreter:
(vm)$ python3
>>> import jax
Display the number of TPU cores available:
>>> jax.device_count()
The number of TPU cores is displayed. If you are using a v4 TPU, this should be
4. If you are using a v2 or v3 TPU, this should be
8.
Perform a simple calculation:
>>> jax.numpy.add(1, 1)
The result of the numpy add is displayed:
Output from the command:
Array(2, dtype=int32, weak_type=true)
Exit the Python interpreter:
>>> exit()
Running JAX code on a TPU VM
You can now run any JAX code you please. The
[flax examples](https://github.com/google/flax/tree/master/examples)
are a great place to start with running standard ML models in JAX. For instance,
to train a basic MNIST convolutional network:
Install Flax examples dependencies
(vm)$ pip install --upgrade clu (vm)$ pip install tensorflow (vm)$ pip install tensorflow_datasets
Install FLAX
(vm)$ git clone https://github.com/google/flax.git (vm)$ pip install --user flax
Run the FLAX MNIST training script
(vm)$ cd flax/examples/mnist (vm)$ python3 main.py --workdir=/tmp/mnist \ --config=configs/default.py \ --config.learning_rate=0.05 \ --config.num_epochs=5
The script downloads the dataset and starts training. The script output should look like this:
0214 18:00:50.660087 140369022753856 train.py:146] epoch: 1, train_loss: 0.2421, train_accuracy: 92.97, test_loss: 0.0615, test_accuracy: 97.88 I0214 18:00:52.015867 140369022753856 train.py:146] epoch: 2, train_loss: 0.0594, train_accuracy: 98.16, test_loss: 0.0412, test_accuracy: 98.72 I0214 18:00:53.377511 140369022753856 train.py:146] epoch: 3, train_loss: 0.0418, train_accuracy: 98.72, test_loss: 0.0296, test_accuracy: 99.04 I0214 18:00:54.727168 140369022753856 train.py:146] epoch: 4, train_loss: 0.0305, train_accuracy: 99.06, test_loss: 0.0257, test_accuracy: 99.15 I0214 18:00:56.082807 140369022753856 train.py:146] epoch: 5, train_loss: 0.0252, train_accuracy: 99.20, test_loss: 0.0263, test_accuracy: 99.18
Clean up
When you are done with your TPU VM follow these steps to clean up your resources.
Disconnect from the Compute Engine instance, if you have not already done so:
(vm)$ exit
Delete your Cloud TPU.
$ gcloud compute tpus tpu-vm delete tpu-name \ --zone=us-central2-b
Verify the resources have been deleted by running the following command. Make sure your TPU is no longer listed. The deletion might take several minutes.
$ gcloud compute tpus tpu-vm list \ --zone=us-central2-b
Performance Notes
Here are a few important details that are particularly relevant to using TPUs in JAX.
Padding
One of the most common causes for slow performance on TPUs is introducing inadvertent padding:
- Arrays in the Cloud TPU are tiled. This entails padding one of the dimensions to a multiple of 8, and a different dimension to a multiple of 128.
- The matrix multiplication unit performs best with pairs of large matrices that minimize the need for padding.
bfloat16 dtype
By default, matrix multiplication in JAX on TPUs uses
[bfloat16](https://cloud.google.com/tpu/docs/bfloat16)
with float32 accumulation. This can be controlled with the precision argument on
relevant jax.numpy function calls (matmul, dot, einsum, etc). In particular:
precision=jax.lax.Precision.DEFAULT: uses mixed bfloat16 precision (fastest)
precision=jax.lax.Precision.HIGH: uses multiple MXU passes to achieve higher precision
precision=jax.lax.Precision.HIGHEST: uses even more MXU passes to achieve full float32 precision
JAX also adds the bfloat16 dtype, which you can use to explicitly cast arrays to
bfloat16, for example,
jax.numpy.array(x, dtype=jax.numpy.bfloat16).
Running JAX in a Colab
When you run JAX code in a Colab notebook, Colab automatically creates a legacy
TPU node. TPU nodes have a different architecture. For more information, see
[System Architecture](/tpu/docs/system-architecture-tpu-vm).
What's next
For more information about Cloud TPU, see:",Run a calculation on a Cloud TPU VM using JAX | Google Cloud,Learn how to run a calculation on a Cloud TPU VM by using JAX and the Google Cloud CLI.
id,url,body,title,description
127,https://cloud.google.com/tpu/docs/reference/rest/v1/projects.locations.operations/list,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Query parameters](#body.QUERY_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [Try it!](#try-it)
Lists operations that match the specified filter in the request. If the server doesn't support this method, it returns
UNIMPLEMENTED.
HTTP request
GET https://tpu.googleapis.com/v1/{name=projects/*/locations/*}/operations
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
The name of the operation's parent resource.
Query parameters
|Parameters
|
filter
|
The standard list filter.
|
pageSize
|
The standard list page size.
|
pageToken
|
The standard list page token.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[ListOperationsResponse](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse)
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).",Method: projects.locations.operations.list | Cloud TPU | Google Cloud,
id,url,body,title,description
100,https://cloud.google.com/tpu/docs/classification-data-conversion,"This tutorial describes how to use the
[image classification data converter
sample](https://github.com/tensorflow/tpu/tree/master/tools/data_converter/image_classification)
script to convert a raw image classification dataset into the [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord)
format used to train Cloud TPU models. [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord)s make
reading large files from Google Cloud Storage more efficient than reading each
image as an individual file. You can use TFRecord anywhere you are using a
tf.data.Dataset pipeline.
See the following TensorFlow documents for more information on using TFRecord:
[TFRecord and tf.train.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord) [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) [tf.data: Build TensorFlow input pipelines](https://www.tensorflow.org/guide/data) [PyTorch TFRecord reader and writer](https://github.com/vahidk/tfrecord)
If you use the PyTorch or JAX framework, and are not using Google Cloud storage for your dataset storage, you might not get the same advantage from TFRecords.
Conversion overview
The
[image classification folder within the data converter repository](https://github.com/tensorflow/tpu/tree/master/tools/data_converter)
on GitHub contains the
converter script,
image_classification_data.py, and a
sample implementation,
simple_example.py, you can copy and modify to do
your own data conversion.
The image classification data converter sample defines two classes,
ImageClassificationConfig and
ImageClassificationBuilder. These classes are
defined in
tpu/tools/data_converter/image_classification_data.py.
ImageClassificationConfig is an abstract base class. You subclass
ImageClassificationConfig to define the configuration needed to instantiate an
ImageClassificationBuilder.
ImageClassificationBuilder is a
[TensorFlow dataset builder](https://www.tensorflow.org/datasets/api_docs/python/tfds/builder)
for image classification datasets. It is a subclass of [.
It retrieves data examples from your dataset and converts them to TFRecords. The
TFRecords are written to a path specified by the tdfs.core.GeneratorBasedBuilder](https://www.tensorflow.org/datasets/api_docs/python/tfds/core/DatasetBuilder)
data_dir parameter to the
__init__ method of
ImageClassificationBuilder.
In
[simple_example.py](https://github.com/tensorflow/tpu/blob/master/tools/data_converter/image_classification/simple_example.py),
SimpleDatasetConfig subclasses
ImageClassificationConfig, implementing
properties that define the supported modes, number of image classes, and an
example generator that yields a dictionary containing image data and an image
class for each example in the dataset.
The
main() function creates a dataset of randomly generated image data and
instantiates a
SimpleDatasetConfig object specifying the number of classes and
the path to the dataset on disk. Next,
main() instantiates an
ImageClassificationBuilder object, passing in the
SimpleDatasetConfig
instance. Finally,
main() calls
download_and_prepare(). When this method is
called, the
ImageClassificationBuilder instance uses the data example
generator implemented by
SimpleDatasetConfig to load each example and saves
them to a series of TFRecord files.
For a more detailed explanation, please see the
[Classification Converter Notebook](https://github.com/tensorflow/tpu/blob/master/tools/colab/image_classification_converter.ipynb).
Before you begin
Modifying the data conversion sample to load your dataset
To convert your dataset into TFRecord format, subclass the
ImageClassificationConfig class defining the following properties:
- num_labels - returns the number of image classes
- supported_modes - returns a list of modes supported by your dataset (for example: test, train, and validate)
- text_label_map - returns a dictionary that models the mapping between a text class label and an integer class label (SimpleDatasetConfig does not use this property, because it does not require a mapping)
- download_path - the path from which to download your dataset (SimpleDatasetConfig does not use this property, the example_generator loads the data from disk)
Implement the example_generator generator function. This method must yield a
dictionary containing the image data and the image class name for each example.
ImageClassificationBuilder uses the
example_generator() function to retrieve
each example and writes them to disk in TFRecord format.
Running the data conversion sample
Create a Cloud Storage bucket using the following command:
gsutil mb -p ${PROJECT_ID} -c standard -l us-central1 gs://bucket-name
Launch a Compute Engine VM using the
gcloudcommand.
$ gcloud compute tpus execution-groups create \ --vm-only \ --zone=us-central1-b \ --name=imageclassificationconverter \ --tf-version=2.5.0
gcloud compute ssh imageclassificationconverter --zone=us-central1-b
From this point on, a prefix of
(vm)$means you should run the command on the Compute Engine VM instance.
Install required packages.
(vm)$ pip3 install opencv-python-headless pillow
Create the following environment variables used by the script.
(vm)$ export STORAGE_BUCKET=gs://bucket-name
(vm)$ export CONVERTED_DIR=$HOME/tfrecords (vm)$ export GENERATED_DATA=$HOME/data (vm)$ export GCS_CONVERTED=$STORAGE_BUCKET/data_converter/image_classification/tfrecords (vm)$ export GCS_RAW=$STORAGE_BUCKET/image_classification/raw (vm)$ export PYTHONPATH=""$PYTHONPATH:/usr/share/tpu/models""
Change to the
data_converterdirectory.
(vm)$ cd /usr/share/tpu/tools/data_converter
Running the data converter on a fake dataset
The
simple_example.py script is located in the
image_classification
folder of the data converter sample. Running the script with the following
parameters generates a set of fake images and converts them into TFRecords.
(vm)$ python3 image_classification/simple_example.py \
--num_classes=1000 \
--data_path=$GENERATED_DATA \
--generate=True \
--num_examples_per_class_low=10 \
--num_examples_per_class_high=11 \
--save_dir=$CONVERTED_DIR
Running the data converter on one of our raw datasets
Create an environment variable for the location of the raw data.
(vm)$ export GCS_RAW=gs://cloud-tpu-test-datasets/data_converter/raw_image_classification
Run the
simple_example.pyscript.
(vm)$ python3 image_classification/simple_example.py \ --num_classes=1000 \ --data_path=$GCS_RAW \ --generate=False \ --save_dir=$CONVERTED_DIR
The
simple_example.py script takes the following parameters:
num_classesrefers to the number of classes in the dataset. We're using 1000 here to match ImageNet format.
generatedetermines whether or not to generate the raw data.
data_pathrefers to the path where the data is generated if
generate=Trueor the path where the raw data is stored if
generate=False.
num_examples_per_class_lowand
num_examples_per_class_highdetermine how many examples per class to generate. The script generates a random number of examples in this range.
save_dirrefers to where the saved TFRecords are saved. In order to train a model on Cloud TPU, the data must be stored on Cloud Storage. This can be on Cloud Storage or on the VM.
Renaming and moving the TFRecords to Cloud Storage
The following example uses the converted data with the ResNet model.
Rename the TFRecords to the same format as ImageNet TFRecords:
(vm)$ cd $CONVERTED_DIR/image_classification_builder/Simple/0.1.0/ (vm)$ sudo apt install rename
(vm)$ rename -v 's/image_classification_builder-(\w+)\.tfrecord/$1/g' *
Copy the TFRecords to Cloud Storage:
(vm)$ gsutil -m cp train* $GCS_CONVERTED (vm)$ gsutil -m cp validation* $GCS_CONVERTED",Converting an image classification dataset for use with Cloud TPU | Google Cloud,
id,url,body,title,description
115,https://cloud.google.com/tpu/docs/reference/rest/v2/projects.locations.acceleratorTypes/get,"Method: projects.locations.acceleratorTypes.get
Stay organized with collections
Save and categorize content based on your preferences.
HTTP request
GET https://tpu.googleapis.com/v2/{name=projects/*/locations/*/acceleratorTypes/*}
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
string
Required. The resource name.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[AcceleratorType](/tpu/docs/reference/rest/v2/projects.locations.acceleratorTypes#AcceleratorType)
Authorization scopes
Requires one of the following OAuth scopes:
-
https://www.googleapis.com/auth/cloud.tpu
-
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Method: projects.locations.acceleratorTypes.get | Cloud TPU | Google Cloud,
id,url,body,title,description
76,https://cloud.google.com/tpu/docs/reference/rest/v2alpha1/projects.locations/list,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Query parameters](#body.QUERY_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [Try it!](#try-it)
Lists information about the supported locations for this service.
HTTP request
GET https://tpu.googleapis.com/v2alpha1/{name=projects/*}/locations
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
The resource that owns the locations collection, if applicable.
Query parameters
|Parameters
|
filter
|
A filter to narrow down results to a preferred subset. The filtering language accepts strings like
|
pageSize
|
The maximum number of results to return. If not set, the service selects a default.
|
pageToken
|
A page token received from the
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[ListLocationsResponse](/tpu/docs/reference/rest/Shared.Types/ListLocationsResponse)
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).",Method: projects.locations.list | Cloud TPU | Google Cloud,
id,url,body,title,description
65,https://cloud.google.com/tpu/docs/tensorflow-ops,"Available TensorFlow Ops
This page lists the TensorFlow Python APIs and graph operators available on Cloud TPU.
Available Python APIs
The list below is a guide to the set of available TensorFlow Python APIs. This list is not exhaustive. Library functions not on this list may work if they are composed of available primitives.
See the
[performance guide](https://cloud.google.com/tpu/docs/performance-guide)
for recommendations about specific operators.
|Module
|Available Python API
|Comments
|
tf
|
tf.abs
|
tf.acosh
|
tf.add
|
tf.add_n
|
tf.angle
|
tf.arg_max
|
dimension argument must be a compile-time constant.
|
tf.arg_min
|
dimension argument must be a compile-time constant.
|
tf.asinh
|
tf.assign
|Available only for resource variable.
|
tf.assign_add
|Available only for resource variable.
|
tf.assign_sub
|Available only for resource variable.
|
tf.atan
|
tf.atan2
|
tf.atanh
|
tf.batch_to_space
|
crops and
block_shape arguments must be compile-time constant.
|
tf.batch_to_space_nd
|
crops argument must be a compile-time constant.
|
tf.broadcast_dynamic_shape
|
tf.broadcast_static_shape
|
tf.case
|Experimental (control-flow). May not work reliably yet.
|
tf.cast
|
tf.ceil
|
tf.cholesky
|Experimental. May have issues with numerical precision.
|
tf.cholesky_solve
|Experimental. May have issues with numerical precision.
|
tf.clip_by_average_norm
|
tf.clip_by_global_norm
|
tf.clip_by_norm
|
tf.clip_by_value
|
tf.complex
|
tf.concat
|
concat_dim must be a compile-time constant.
|
tf.cond
|Experimental (control-flow). May not work reliably yet.
|
tf.conj
|
tf.constant
|
tf.convert_to_tensor
|
tf.cos
|
tf.cosh
|
tf.cross
|
tf.cumprod
|
axis must be a compile-time constant.
|
tf.cumsum
|
axis must be a compile-time constant.
|
tf.depth_to_space
|
tf.diag
|
tf.diag_part
|
tf.div
|
int32 division is slower than other types.
|
tf.divide
|
int32 division is slower than other types.
|
tf.dynamic_stitch
|
indices must be a compile-time constant.
|
tf.einsum
|
tf.equal
|
tf.erf
|
tf.erfc
|
tf.exp
|
tf.expand_dims
|
dims must be a compile-time constant.
|
tf.expm1
|
tf.extract_image_patches
|
tf.eye
|
tf.fake_quant_with_min_max_args
|
tf.fake_quant_with_min_max_args_gradient
|
tf.fake_quant_with_min_max_vars
|
tf.fake_quant_with_min_max_vars_gradient
|
tf.fft
|
tf.fft2d
|
tf.fft3d
|
tf.fill
|
dims argument must be a compile-time constant.
|
tf.floor
|
tf.floordiv
|
tf.floormod
|
tf.foldl
|Experimental (control-flow).
|
tf.foldr
|Experimental (control-flow).
|
tf.gather
|
axis must be a compile-time constant.
|
tf.gather_nd
|
tf.greater
|
tf.greater_equal
|
tf.hessians
|Experimental (control-flow.
|
tf.identity
|
tf.identity_n
|
tf.ifft
|
tf.ifft2d
|
tf.ifft3d
|
tf.imag
|
tf.invert_permutation
|
x argument must be a compile-time constant.
|
tf.is_finite
|
tf.is_inf
|
tf.is_nan
|
tf.is_non_decreasing
|
tf.is_strictly_increasing
|
tf.less
|
tf.less_equal
|
tf.linspace
|
start,
stop and
num arguments must be compile-time constants.
|
tf.log
|
tf.log1p
|
tf.log_sigmoid
|
tf.logical_and
|
tf.logical_or
|
tf.logical_not
|
tf.logical_xor
|
tf.matmul
|Uses a
bfloat16 matmul with
float32 accumulation.
|
tf.matrix_band_part
|
tf.matrix_diag
|
tf.matrix_diag_part
|
tf.matrix_set_diag
|
tf.matrix_triangular_solve
|Experimental. May have issues with numerical precision.
|
tf.maximum
|
tf.meshgrid
|
tf.minimum
|
tf.mod
|
tf.multinomial
|
num_samples argument must be a compile-time constant.
|
tf.multiply
|
tf.negative
|
tf.no_op
|
tf.norm
|
tf.not_equal
|
tf.one_hot
|
depth must be a compile-time constant.
|
tf.ones
|
tf.ones_like
|
tf.pad
|
paddings argument must be a compile-time constant. Gradient of
REFLECT padding is not yet available.
|
tf.pow
|
tf.random_normal
|
shape must be a compile-time constant.
|
tf.random_uniform
|
shape must be a compile-time constant.
|
tf.range
|
start,
limit and
delta arguments must be compile-time constants.
|
tf.rank
|
tf.real
|
tf.realdiv
|
tf.reciprocal
|
tf.reduce_all
|
axis must be a compile-time constant.
|
tf.reduce_any
|
axis must be a compile-time constant.
|
tf.reduce_logsumexp
|
tf.reduce_max
|
axis must be a compile-time constant.
|
tf.reduce_min
|
axis must be a compile-time constant.
|
tf.reduce_prod
|
axis must be a compile-time constant.
|
tf.reduce_sum
|
axis must be a compile-time constant.
|
tf.reshape
|
shape argument must be a compile-time constant.
|
tf.reverse
|
dims argument must be a compile-time constant.
|
tf.reverse_sequence
|
tf.reverse_v2
|
axis argument must be a compile-time constant.
|
tf.rint
|
tf.round
|
tf.rsqrt
|
tf.saturate_cast
|
tf.scalar_mul
|
tf.scan
|Experimental (control-flow).
|
tf.scatter_nd
|
tf.sequence_mask
|
tf.shape
|
tf.shape_n
|
tf.sigmoid
|
tf.sign
|
tf.sin
|
tf.sinh
|
tf.size
|
tf.slice
|
size must be a compile-time constant. In addition, either
begin must be a compile-time constant or
size must be non-negative. Backpropagation is only supported if
begin and
size are compile-time constants.
|
tf.space_to_batch
|
paddings and
block_shape must be compile-time constants.
|
tf.space_to_batch_nd
|
paddings must be a compile-time constant.
|
tf.space_to_depth
|
tf.split
|
axis must be a compile-time constant.
|
tf.sqrt
|
tf.square
|
tf.squared_difference
|
tf.squeeze
|
tf.stack
|
tf.stop_gradient
|
tf.strided_slice
|
tf.tan
|
tf.tanh
|
tf.tensordot
|
tf.tile
|
multiples argument must be a compile-time constant.
|
tf.to_bfloat16
|
tf.to_float
|
tf.to_int32
|
tf.to_int64
|
int64 support is limited.
|
tf.trace
|
tf.transpose
|
perm argument must be a compile-time constant.
|
tf.truediv
|
tf.truncated_normal
|
shape must be a compile-time constant.
|
tf.truncatediv
|
tf.truncatemod
|
tf.unsorted_segment_sum
|
tf.unstack
|
tf.where
|Both
x and
y must be non-
None. If both
x and
y are
None, the operator would not have a static shape.
|
tf.while_loop
|Computing the gradient of a while loop requires that the
maximum_iterations argument is passed.
|
tf.zeros
|
tf.zeros_like
|
tf.Tensor.__getitem__
|The start, end, and strides of a slice must be compile-time constants.
|
tf.bitwise
|
tf.bitwise_and
|
tf.bitwise_or
|
tf.bitwise_invert
|
tf.contrib.stateless
|
tf.contrib.stateless.stateless_random_normal
|
tf.contrib.stateless.stateless_random_uniform
|
tf.image
|
tf.image.adjust_brightness
|
tf.image.adjust_contrast
|
tf.image.adjust_gamma
|
tf.image.adjust_hue
|
tf.image.adjust_saturation
|
tf.image.central_crop
|Crop factor must be a compile-time constant.
|
tf.image.convert_image_dtype
|
tf.image.flip_left_right
|
tf.image.flip_up_down
|
tf.image.grayscale_to_rgb
|
tf.image.hsv_to_rgb
|
tf.image.resize_bilinear
|Only
align_corners=True is available.
size must be a compile-time constant.
|
tf.image.random_brightness
|
tf.image.random_contrast
|
tf.image.random_flip_left_right
|
tf.image.random_flip_up_down
|
tf.image.random_hue
|
tf.image.random_saturation
|
tf.image.rgb_to_hsv
|
tf.image.rgb_to_grayscale
|
tf.image.rot90
|
tf.image.total_variation
|
tf.image.transpose_image
|
tf.layers
|
tf.layers.average_pooling1d
|
tf.layers.average_pooling2d
|
tf.layers.average_pooling1d
|
tf.layers.batch_normalization
|
tf.layers.conv1d
|
tf.layers.conv2d
|
tf.layers.conv2d_transpose
|
tf.layers.conv3d
|
tf.layers.conv3d_transpose
|
tf.layers.dense
|
tf.layers.dropout
|
tf.layers.flatten
|
tf.layers.max_pooling1d
|
tf.layers.max_pooling2d
|
tf.layers.max_pooling3d
|
tf.layers.separable_conv2d
|
tf.nn
|
tf.nn.atrous_conv2d
|
tf.nn.atrous_conv2d_transpose
|
tf.nn.avg_pool
|
tf.nn.avg_pool3d
|
tf.nn.batch_normalization
|
tf.nn.bias_add
|
tf.nn.conv1d
|
tf.nn.conv2d
|
tf.nn.conv2d_backprop_filter
|
tf.nn.conv2d_backprop_input
|
tf.nn.conv2d_transpose
|
tf.nn.conv3d
|
tf.nn.conv3d_backprop_filter
|
tf.nn.conv3d_backprop_input
|
tf.nn.conv3d_transpose
|
tf.nn.convolution
|
tf.nn.crelu
|
tf.nn.depthwise_conv2d
|
tf.nn.depthwise_conv2d_native
|
tf.nn.depthwise_conv2d_native_backprop_filter
|
tf.nn.depthwise_conv2d_native_backprop_input
|
tf.nn.dropout
|
tf.nn.dynamic_rnn
|Experimental.
|
tf.nn.elu
|
tf.nn.fused_batch_norm
|
tf.nn.l2_loss
|
tf.nn.l2_normalize
|
tf.nn.leaky_relu
|
tf.nn.local_response_normalization
|
tf.nn.log_poisson_loss
|
tf.nn.log_softmax
|
tf.nn.max_pool
|
tf.nn.max_pool3d
|
tf.nn.moments
|
tf.nn.normalize_moments
|
tf.nn.pool
|
tf.nn.relu
|
tf.nn.relu6
|
tf.nn.relu_layer
|
tf.nn.selu
|
tf.nn.separable_conv2d
|
tf.nn.sigmoid_cross_entropy_with_logits
|
tf.nn.softmax
|
tf.nn.softmax_cross_entropy_with_logits
|
tf.nn.softplus
|
tf.nn.softsign
|
tf.nn.sparse_softmax_cross_entropy_with_logits
|
tf.nn.static_bidirectional_rnn
|Experimental.
|
tf.nn.static_rnn
|Experimental.
|
tf.nn.weighted_cross_entropy_with_logits
|Experimental.
|
tf.nn.weighted_moments
|
tf.nn.with_space_to_batch
|
tf.nn.xw_plus_b
|
tf.nn.zero_fraction
|
tf.spectral
|
tf.spectral.fft
|
tf.spectral.fft2d
|
tf.spectral.fft3d
|
tf.spectral.ifft
|
tf.spectral.ifft2d
|
tf.spectral.ifft3d
|
tf.spectral.irfft
|
fft_length must be a compile-time constant.
|
tf.spectral.irfft2d
|
fft_length must be a compile-time constant.
|
tf.spectral.irfft3d
|
fft_length must be a compile-time constant.
|
tf.spectral.rfft
|
fft_length must be a compile-time constant.
|
tf.spectral.rfft2d
|
fft_length must be a compile-time constant.
|
tf.spectral.rfft3d
|
fft_length must be a compile-time constant.
Unavailable Python APIs
This list is not exhaustive. Ops that are not available on Cloud TPU include the following:
|Module
|Unavailable Python API
|Comments
|
tf
|
tf.accumulate_n
|Uses Ref variables.
|
tf.acos
|
tf.asin
|
tf.betainc
|
tf.bitcast
|
tf.add_check_numerics_ops
|Programs containing check numerics operators should run, but the check numerics operator is currently ignored.
|
tf.assert_...
|Programs containing assertions should run, but the assertions are ignored.
|
tf.check_numerics
|Programs containing check numerics operators should run, but the check numerics operator is currently ignored.
|
tf.confusion_matrix
|
tf.count_nonzero
|Uses
int64 reduction.
|
tf.count_up_to
|
tf.create_partitioned_variables
|
tf.dequantize
|
tf.digamma
|
tf.dynamic_partition
|
tf.edit_distance
|
tf.fake_quant_with_min_max_vars_per_channel
|
tf.fake_quant_with_min_max_vars_per_channel_gradient
|
tf.histogram_fixed_width
|
tf.igamma
|
tf.igammac
|
tf.lbeta
|
tf.lgamma
|
tf.matrix_determinant
|
tf.matrix_inverse
|
tf.matrix_solve
|
tf.matrix_solve_ls
|
tf.polygamma
|
tf.py_func
|
tf.qr
|
tf.quantize_v2
|
tf.quantized_concat
|
tf.random_crop
|
tf.random_gamma
|
tf.random_poisson
|
tf.random_shuffle
|
tf.scatter_add
|
tf.scatter_div
|
tf.scatter_mul
|
tf.scatter_nd_add
|
tf.scatter_nd_sub
|
tf.scatter_nd_update
|
tf.segment_mean
|
tf.segment_max
|
tf.segment_min
|
tf.segment_prod
|
tf.segment_sum
|
tf.self_adjoint_eig
|
tf.self_adjoint_eigvals
|
tf.setdiff1d
|
tf.sparse_...
|
tf.string_...
|
tf.substr
|
tf.svd
|
tf.to_double
|
tf.unique
|
tf.unsorted_segment_max
|
tf.zeta
|
tf.bitwise.bitwise_xor
|
tf.contrib.stateless.stateless_truncated_normal
Available graph operators
|Operator
|Type Constraint
|
Abs
|
T={bfloat16,float,int32,int64}
|
Acos
|
T={bfloat16,complex64,float,int32,int64}
|
Acosh
|
T={bfloat16,complex64,float}
|
Add
|
T={bfloat16,complex64,float,int32,int64}
|
AddN
|
T={bfloat16,complex64,float,int32,int64,uint32,uint64}
|
AdjustContrastv2
|
T={float}
|
AdjustHue
|
T={float}
|
AdjustSaturation
|
T={float}
|
All
|
Tidx={int32,int64}
|
AllToAll
|
T={bfloat16,float}
|
Angle
|
Tout={float}
T={complex64}
|
Any
|
Tidx={int32,int64}
|
ApproximateEqual
|
T={bfloat16,complex64,float,int32,int64,uint32,uint64}
|
ArgMax
|
Tidx={int32,int64}
output_type={int32,int64}
T={bfloat16,complex64,float,int32,int64,uint32,uint64}
|
ArgMin
|
Tidx={int32,int64}
output_type={int32,int64}
T={bfloat16,complex64,float,int32,int64,uint32,uint64}
|
Asin
|
T={bfloat16,complex64,float,int32,int64}
|
Asinh
|
T={bfloat16,complex64,float}
|
Assert
|
T={bfloat16,bool,complex64,float,int32,int64,string,uint32,uint64}
|
AssignAddVariableOp
|
dtype={bfloat16,complex64,float,int32,int64,uint32,uint64}
|
AssignSubVariableOp
|
dtype={bfloat16,complex64,float,int32,int64,uint32,uint64}
|
AssignVariableOp
|
dtype={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
Atan
|
T={bfloat16,complex64,float,int32,int64}
|
Atan2
|
T={bfloat16,float}
|
Atanh
|
T={bfloat16,complex64,float}
|
AvgPool
|
T={bfloat16,float}
|
AvgPool3D
|
T={bfloat16,float}
|
AvgPool3DGrad
|
T={bfloat16,float}
|
AvgPoolGrad
|
T={bfloat16,float}
|
BatchMatMul
|
T={bfloat16,complex64,float,int32,int64}
|
BatchToSpace
|
Tidx={int32,int64}
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
BatchToSpaceND
|
Tcrops={int32,int64}
Tblock_shape={int32,int64}
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
BiasAdd
|
T={bfloat16,complex64,float,int32,int64,uint32,uint64}
|
BiasAddGrad
|
T={bfloat16,complex64,float,int32,int64,uint32,uint64}
|
BiasAddV1
|
T={bfloat16,complex64,float,int32,int64,uint32,uint64}
|
Bitcast
|
type={bfloat16,complex64,float,int32,int64,uint32,uint64}
T={bfloat16,complex64,float,int32,int64,uint32,uint64}
|
BitwiseAnd
|
T={int32,int64,uint32,uint64}
|
BitwiseOr
|
T={int32,int64,uint32,uint64}
|
BitwiseXor
|
T={int32,int64,uint32,uint64}
|
BroadcastArgs
|
T={int32,int64}
|
BroadcastGradientArgs
|
T={int32,int64}
|
BroadcastTo
|
Tidx={int32,int64}
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
Bucketize
|
T={float,int32,int64}
|
Cast
|
DstT={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
SrcT={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
Ceil
|
T={bfloat16,float}
|
CheckNumerics
|
T={bfloat16,float}
|
Cholesky
|
T={float}
|
ClipByValue
|
T={bfloat16,complex64,float,int32,int64,uint32,uint64}
|
CollectivePermute
|
T={bfloat16,float}
|
Complex
|
Tout={complex64}
T={float}
|
ComplexAbs
|
Tout={float}
T={complex64}
|
Concat
|
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
ConcatOffset
|
ConcatV2
|
Tidx={int32}
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
Conj
|
T={complex64}
|
ConjugateTranspose
|
Tperm={int32,int64}
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
Const
|
dtype={bfloat16,bool,complex64,float,int32,int64,string,uint32,uint64}
|
ControlTrigger
|
Conv2D
|
T={bfloat16,float}
|
Conv2DBackpropFilter
|
T={bfloat16,float}
|
Conv2DBackpropInput
|
T={bfloat16,float}
|
Conv3D
|
T={bfloat16,float}
|
Conv3DBackpropFilterV2
|
T={bfloat16,float}
|
Conv3DBackpropInputV2
|
Tshape={int32,int64}
T={bfloat16,float}
|
Cos
|
T={bfloat16,complex64,float}
|
Cosh
|
T={bfloat16,complex64,float}
|
Cross
|
T={bfloat16,float,int32,int64,uint32,uint64}
|
CrossReplicaSum
|
T={bfloat16,float}
|
Cumprod
|
Tidx={int32,int64}
T={bfloat16,float,int32}
|
Cumsum
|
Tidx={int32,int64}
T={bfloat16,float,int32}
|
DataFormatVecPermute
|
T={int32,int64}
|
DepthToSpace
|
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
DepthwiseConv2dNative
|
T={bfloat16,float}
|
DepthwiseConv2dNativeBackpropFilter
|
T={bfloat16,float}
|
DepthwiseConv2dNativeBackpropInput
|
T={bfloat16,float}
|
Diag
|
T={bfloat16,complex64,float,int32,int64}
|
DiagPart
|
T={bfloat16,complex64,float,int32,int64}
|
Digamma
|
T={bfloat16,float}
|
Div
|
T={bfloat16,complex64,float,int32,int64}
|
DivNoNan
|
T={float}
|
DynamicStitch
|
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
Elu
|
T={bfloat16,float}
|
EluGrad
|
T={bfloat16,float}
|
Empty
|
dtype={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
EmptyTensorList
|
shape_type={int32,int64}
element_dtype={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
Equal
|
T={bfloat16,bool,complex64,float,int32,int64}
|
Erf
|
T={bfloat16,float}
|
Erfc
|
T={bfloat16,float}
|
Exp
|
T={bfloat16,complex64,float}
|
ExpandDims
|
Tdim={int32,int64}
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
Expm1
|
T={bfloat16,complex64,float}
|
ExtractImagePatches
|
T={bfloat16,float,int32,int64,uint32,uint64}
|
FFT
|
Tcomplex={complex64}
|
FFT2D
|
Tcomplex={complex64}
|
FFT3D
|
Tcomplex={complex64}
|
FakeParam
|
dtype={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
FakeQuantWithMinMaxArgs
|
FakeQuantWithMinMaxArgsGradient
|
FakeQuantWithMinMaxVars
|
FakeQuantWithMinMaxVarsGradient
|
Fill
|
index_type={int32,int64}
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
Floor
|
T={bfloat16,float}
|
FloorDiv
|
T={bfloat16,complex64,float,int32,int64}
|
FloorMod
|
T={bfloat16,float,int32,int64}
|
FusedBatchNorm
|
T={float}
|
FusedBatchNormGrad
|
T={float}
|
FusedBatchNormGradV2
|
U={float}
T={bfloat16,float}
|
FusedBatchNormV2
|
U={float}
T={bfloat16,float}
|
Gather
|
Tindices={int32,int64}
Tparams={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
GatherNd
|
Tindices={int32,int64}
Tparams={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
GatherV2
|
Taxis={int32,int64}
Tindices={int32,int64}
Tparams={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
GetItem
|
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
Greater
|
T={bfloat16,float,int32,int64,uint32,uint64}
|
GreaterEqual
|
T={bfloat16,float,int32,int64,uint32,uint64}
|
HSVToRGB
|
T={bfloat16,float}
|
IFFT
|
Tcomplex={complex64}
|
IFFT2D
|
Tcomplex={complex64}
|
IFFT3D
|
Tcomplex={complex64}
|
IRFFT
|
IRFFT2D
|
IRFFT3D
|
Identity
|
T={bfloat16,bool,complex64,float,int32,int64,resource,uint32,uint64}
|
IdentityN
|
T={bfloat16,bool,complex64,float,int32,int64,resource,uint32,uint64}
|
If
|
Tout={bfloat16,bool,complex64,float,int32,int64,resource,uint32,uint64}
Tin={bfloat16,bool,complex64,float,int32,int64,resource,uint32,uint64}
Tcond={bfloat16,bool,complex64,float,int32,int64,resource,uint32,uint64}
|
Imag
|
Tout={float}
T={complex64}
|
InfeedDequeue
|
dtype={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
InfeedDequeueTuple
|
dtypes={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
InplaceAdd
|
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
InplaceUpdate
|
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
Inv
|
T={bfloat16,complex64,float,int32,int64}
|
Invert
|
T={int32,int64,uint32,uint64}
|
InvertPermutation
|
T={int32}
|
IsFinite
|
T={bfloat16,float}
|
IsInf
|
T={bfloat16,float}
|
IsNan
|
T={bfloat16,float}
|
L2Loss
|
T={bfloat16,float}
|
LRN
|
T={bfloat16,float}
|
LRNGrad
|
T={bfloat16,float}
|
LeakyRelu
|
T={bfloat16,float}
|
LeakyReluGrad
|
T={bfloat16,float}
|
LeftShift
|
T={int32,int64,uint32,uint64}
|
Less
|
T={bfloat16,float,int32,int64,uint32,uint64}
|
LessEqual
|
T={bfloat16,float,int32,int64,uint32,uint64}
|
Lgamma
|
T={bfloat16,float}
|
LinSpace
|
Tidx={int32,int64}
T={bfloat16,float}
|
ListDiff
|
out_idx={int32,int64}
T={int32,int64}
|
Log
|
T={bfloat16,complex64,float}
|
Log1p
|
T={bfloat16,complex64,float}
|
LogSoftmax
|
T={bfloat16,float}
|
LogicalAnd
|
LogicalNot
|
LogicalOr
|
MatMul
|
T={bfloat16,complex64,float}
|
MatrixBandPart
|
Tindex={int32,int64}
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
MatrixDiag
|
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
MatrixDiagPart
|
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
MatrixSetDiag
|
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
MatrixTriangularSolve
|
T={complex64,float}
|
Max
|
Tidx={int32,int64}
T={bfloat16,complex64,float,int32,int64,uint32,uint64}
|
MaxPool
|
T={bfloat16,float,int32,int64}
|
MaxPool3D
|
T={bfloat16,float}
|
MaxPool3DGrad
|
TInput={bfloat16,float}
T={bfloat16,float}
|
MaxPool3DGradGrad
|
T={float}
|
MaxPoolGrad
|
T={bfloat16,float,int32,int64,uint32,uint64}
|
MaxPoolGradGrad
|
T={float}
|
MaxPoolGradGradV2
|
T={float}
|
MaxPoolGradV2
|
T={bfloat16,float,int32,int64,uint32,uint64}
|
MaxPoolV2
|
T={bfloat16,float,int32,int64}
|
Maximum
|
T={bfloat16,float,int32,int64}
|
Mean
|
Tidx={int32,int64}
T={bfloat16,complex64,float,int32,int64,uint32,uint64}
|
Min
|
Tidx={int32,int64}
T={bfloat16,complex64,float,int32,int64,uint32,uint64}
|
Minimum
|
T={bfloat16,float,int32,int64}
|
MirrorPad
|
Tpaddings={int32,int64}
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
Mod
|
T={bfloat16,float,int32,int64}
|
Mul
|
T={bfloat16,complex64,float,int32,int64}
|
Multinomial
|
output_dtype={int32,int64}
T={bfloat16,float,int32,int64,uint32,uint64}
|
Neg
|
T={bfloat16,complex64,float,int32,int64}
|
NoOp
|
NonMaxSuppressionV4
|
T={float}
|
NotEqual
|
T={bfloat16,bool,complex64,float,int32,int64}
|
OneHot
|
TI={int32,int64}
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
OnesLike
|
T={bfloat16,bool,complex64,float,int32,int64}
|
OutfeedEnqueue
|
dtype={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
OutfeedEnqueueTuple
|
dtypes={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
Pack
|
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
Pad
|
Tpaddings={int32,int64}
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
PadV2
|
Tpaddings={int32,int64}
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
ParallelDynamicStitch
|
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
PlaceholderWithDefault
|
dtype={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
Pow
|
T={bfloat16,complex64,float,int32,int64}
|
PreventGradient
|
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
Prod
|
Tidx={int32,int64}
T={bfloat16,complex64,float,int32,int64,uint32,uint64}
|
Qr
|
T={float}
|
QuantizeAndDequantizeV2
|
T={bfloat16,float}
|
QuantizeAndDequantizeV3
|
T={bfloat16,float}
|
RFFT
|
RFFT2D
|
RFFT3D
|
RGBToHSV
|
T={bfloat16,float}
|
RandomShuffle
|
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
RandomStandardNormal
|
T={int32,int64}
dtype={bfloat16,float}
|
RandomUniform
|
T={int32,int64}
dtype={bfloat16,float}
|
RandomUniformInt
|
T={int32,int64}
Tout={int32,int64}
|
Range
|
Tidx={bfloat16,float,int32,int64}
|
Rank
|
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
ReadVariableOp
|
dtype={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
Real
|
Tout={float}
T={complex64}
|
RealDiv
|
T={bfloat16,complex64,float,int32,int64}
|
Reciprocal
|
T={bfloat16,complex64,float,int32,int64}
|
ReciprocalGrad
|
T={bfloat16,complex64,float}
|
RecvTPUEmbeddingActivations
|
Relu
|
T={bfloat16,float,int32,int64,uint32,uint64}
|
Relu6
|
T={bfloat16,float,int32,int64,uint32,uint64}
|
Relu6Grad
|
T={bfloat16,float,int32,int64,uint32,uint64}
|
ReluGrad
|
T={bfloat16,float,int32,int64,uint32,uint64}
|
Reshape
|
Tshape={int32,int64}
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
ResizeBilinear
|
T={bfloat16,float,int32,int64}
|
ResizeBilinearGrad
|
T={bfloat16,float}
|
ResizeNearestNeighbor
|
T={float,int32,int64}
|
ResourceApplyAdaMax
|
T={bfloat16,float}
|
ResourceApplyAdadelta
|
T={bfloat16,float}
|
ResourceApplyAdagrad
|
T={bfloat16,float}
|
ResourceApplyAdagradDA
|
T={bfloat16,float}
|
ResourceApplyAdam
|
T={bfloat16,float}
|
ResourceApplyAddSign
|
T={bfloat16,float}
|
ResourceApplyCenteredRMSProp
|
T={bfloat16,float}
|
ResourceApplyFtrl
|
T={bfloat16,float}
|
ResourceApplyFtrlV2
|
T={bfloat16,float}
|
ResourceApplyGradientDescent
|
T={bfloat16,float}
|
ResourceApplyKerasMomentum
|
T={bfloat16,float}
|
ResourceApplyMomentum
|
T={bfloat16,float}
|
ResourceApplyPowerSign
|
T={bfloat16,float}
|
ResourceApplyProximalAdagrad
|
T={bfloat16,float}
|
ResourceApplyProximalGradientDescent
|
T={bfloat16,float}
|
ResourceApplyRMSProp
|
T={bfloat16,float}
|
ResourceGather
|
Tindices={int32,int64}
dtype={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
ResourceScatterAdd
|
Tindices={int32,int64}
dtype={bfloat16,complex64,float,int32,int64,uint32,uint64}
|
ResourceScatterDiv
|
Tindices={int32,int64}
dtype={bfloat16,complex64,float,int32,int64,uint32,uint64}
|
ResourceScatterMax
|
Tindices={int32,int64}
dtype={bfloat16,complex64,float,int32,int64,uint32,uint64}
|
ResourceScatterMin
|
Tindices={int32,int64}
dtype={bfloat16,complex64,float,int32,int64,uint32,uint64}
|
ResourceScatterMul
|
Tindices={int32,int64}
dtype={bfloat16,complex64,float,int32,int64,uint32,uint64}
|
ResourceScatterNdAdd
|
Tindices={int32,int64}
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
ResourceScatterNdSub
|
Tindices={int32,int64}
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
ResourceScatterNdUpdate
|
Tindices={int32,int64}
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
ResourceScatterSub
|
Tindices={int32,int64}
dtype={bfloat16,complex64,float,int32,int64,uint32,uint64}
|
ResourceScatterUpdate
|
Tindices={int32,int64}
dtype={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
ResourceStridedSliceAssign
|
Index={int32,int64}
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
Reverse
|
T={bool,complex64,float,int32,int64}
|
ReverseSequence
|
Tlen={int32,int64}
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
ReverseV2
|
T={bfloat16,bool,complex64,float,int32,int64}
Tidx={int32,int64}
|
RightShift
|
T={int32,int64,uint32,uint64}
|
Rint
|
T={bfloat16,float}
|
Round
|
T={bfloat16,complex64,float,int32,int64}
|
Rsqrt
|
T={bfloat16,complex64,float}
|
RsqrtGrad
|
T={bfloat16,complex64,float}
|
ScatterNd
|
Tindices={int32,int64}
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
Select
|
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
Selu
|
T={bfloat16,float}
|
SeluGrad
|
T={bfloat16,float}
|
SendTPUEmbeddingGradients
|
Shape
|
out_type={int32,int64}
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
ShapeN
|
out_type={int32,int64}
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
Sigmoid
|
T={bfloat16,complex64,float}
|
SigmoidGrad
|
T={bfloat16,complex64,float}
|
Sign
|
T={bfloat16,complex64,float,int32,int64}
|
Sin
|
T={bfloat16,complex64,float}
|
Sinh
|
T={bfloat16,complex64,float}
|
Size
|
out_type={int32,int64}
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
Slice
|
Index={int32,int64}
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
Snapshot
|
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
Softmax
|
T={bfloat16,float}
|
SoftmaxCrossEntropyWithLogits
|
T={bfloat16,float}
|
Softplus
|
T={bfloat16,float}
|
SoftplusGrad
|
T={bfloat16,float}
|
Softsign
|
T={bfloat16,float}
|
SoftsignGrad
|
T={bfloat16,float}
|
SpaceToBatch
|
Tpaddings={int32,int64}
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
SpaceToBatchND
|
Tblock_shape={int32,int64}
Tpaddings={int32,int64}
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
SpaceToDepth
|
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
SparseMatMul
|
Tb={bfloat16,float}
Ta={bfloat16,float}
|
SparseSoftmaxCrossEntropyWithLogits
|
Tlabels={int32,int64}
T={bfloat16,float}
|
SparseToDense
|
Tindices={int32,int64}
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
Split
|
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
SplitV
|
Tlen={int32,int64}
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
Sqrt
|
T={bfloat16,complex64,float}
|
SqrtGrad
|
T={bfloat16,complex64,float}
|
Square
|
T={bfloat16,complex64,float,int32,int64}
|
SquaredDifference
|
T={bfloat16,complex64,float,int32,int64}
|
Squeeze
|
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
StackCloseV2
|
StackPopV2
|
elem_type={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
StackPushV2
|
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
StackV2
|
elem_type={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
StatelessIf
|
Tout={bfloat16,bool,complex64,float,int32,int64,resource,uint32,uint64}
Tin={bfloat16,bool,complex64,float,int32,int64,resource,uint32,uint64}
Tcond={bfloat16,bool,complex64,float,int32,int64,resource,uint32,uint64}
|
StatelessMultinomial
|
output_dtype={int32,int64}
Tseed={int32}
T={bfloat16,float}
|
StatelessRandomNormal
|
Tseed={int32}
T={int32,int64}
dtype={bfloat16,float}
|
StatelessRandomUniform
|
Tseed={int32}
T={int32,int64}
dtype={bfloat16,float}
|
StatelessRandomUniformInt
|
Tseed={int32}
T={int32,int64}
dtype={int32,int64}
|
StatelessTruncatedNormal
|
Tseed={int32}
T={int32,int64}
dtype={bfloat16,float}
|
StatelessWhile
|
T={bfloat16,bool,complex64,float,int32,int64,resource,uint32,uint64}
|
StopGradient
|
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
StridedSlice
|
Index={int32,int64}
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
StridedSliceGrad
|
Index={int32,int64}
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
Sub
|
T={bfloat16,complex64,float,int32,int64}
|
Sum
|
Tidx={int32,int64}
T={bfloat16,complex64,float,int32,int64,uint32,uint64}
|
SymbolicGradient
|
Tout={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
Tin={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
TPUEmbeddingActivations
|
Tan
|
T={bfloat16,complex64,float,int32,int64}
|
Tanh
|
T={bfloat16,complex64,float}
|
TanhGrad
|
T={bfloat16,complex64,float}
|
TensorArrayCloseV3
|
TensorArrayConcatV3
|
dtype={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
TensorArrayGatherV3
|
dtype={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
TensorArrayGradV3
|
TensorArrayReadV3
|
dtype={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
TensorArrayScatterV3
|
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
TensorArraySizeV3
|
TensorArraySplitV3
|
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
TensorArrayV3
|
dtype={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
TensorArrayWriteV3
|
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
TensorListElementShape
|
shape_type={int32,int64}
|
TensorListPopBack
|
element_dtype={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
TensorListPushBack
|
element_dtype={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
TensorListReserve
|
shape_type={int32,int64}
element_dtype={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
Tile
|
Tmultiples={int32,int64}
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
TopKV2
|
T={bfloat16,float,int32,uint32}
|
Transpose
|
Tperm={int32,int64}
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
TruncateDiv
|
T={bfloat16,complex64,float,int32,int64}
|
TruncateMod
|
T={bfloat16,float,int32,int64}
|
TruncatedNormal
|
T={int32,int64}
dtype={float}
|
Unpack
|
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
UnsortedSegmentMax
|
Tnumsegments={int32,int64}
Tindices={int32,int64}
T={bfloat16,float,int32,int64,uint32,uint64}
|
UnsortedSegmentMin
|
Tnumsegments={int32,int64}
Tindices={int32,int64}
T={bfloat16,float,int32,int64,uint32,uint64}
|
UnsortedSegmentProd
|
Tnumsegments={int32,int64}
Tindices={int32,int64}
T={bfloat16,complex64,float,int32,int64,uint32,uint64}
|
UnsortedSegmentSum
|
Tnumsegments={int32,int64}
Tindices={int32,int64}
T={bfloat16,complex64,float,int32,int64,uint32,uint64}
|
VarIsInitializedOp
|
VariableShape
|
out_type={int32,int64}
|
While
|
T={bfloat16,bool,complex64,float,int32,int64,resource,uint32,uint64}
|
Xdivy
|
T={complex64,float}
|
XlaBroadcastHelper
|
Tindices={int32,int64}
T={bfloat16,complex64,float,int32,int64,uint32,uint64}
|
XlaConv
|
Tindices={int32,int64}
T={bfloat16,complex64,float,int32,int64,uint32,uint64}
|
XlaDequantize
|
XlaDot
|
T={bfloat16,complex64,float,int32,int64,uint32,uint64}
|
XlaDynamicSlice
|
Tindices={int32,int64}
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
XlaDynamicUpdateSlice
|
Tindices={int32,int64}
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
XlaHostCompute
|
Toutputs={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
Tinputs={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
XlaIf
|
Tout={bfloat16,bool,complex64,float,int32,int64,resource,uint32,uint64}
Tin={bfloat16,bool,complex64,float,int32,int64,resource,uint32,uint64}
Tcond={bfloat16,bool,complex64,float,int32,int64,resource,uint32,uint64}
|
XlaKeyValueSort
|
V={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
K={bfloat16,float,int32,int64,uint32,uint64}
|
XlaPad
|
Tindices={int32,int64}
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
XlaRecv
|
dtype={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
XlaRecvFromHost
|
Toutput={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
XlaReduce
|
T={bfloat16,complex64,float,int32,int64,uint32,uint64}
|
XlaReduceWindow
|
Tindices={int32,int64}
T={bfloat16,complex64,float,int32,int64,uint32,uint64}
|
XlaSelectAndScatter
|
Tindices={int32,int64}
T={bfloat16,complex64,float,int32,int64,uint32,uint64}
|
XlaSend
|
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
XlaSendToHost
|
Tinput={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
XlaSort
|
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
XlaWhile
|
T={bfloat16,bool,complex64,float,int32,int64,resource,uint32,uint64}
|
Xlogy
|
T={complex64,float}
|
ZerosLike
|
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
_Arg
|
T={bfloat16,bool,complex64,float,int32,int64,resource,uint32,uint64}
|
_ArrayToList
|
out_types={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
_ListToArray
|
T={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
Tin={bfloat16,bool,complex64,float,int32,int64,uint32,uint64}
|
_Retval
|
T={bfloat16,bool,complex64,float,int32,int64,resource,uint32,uint64}",Available TensorFlow Ops | Cloud TPU | Google Cloud,
id,url,body,title,description
4,https://cloud.google.com/tpu/docs/reference/rest/v2alpha1/projects.locations.acceleratorTypes/get,"Method: projects.locations.acceleratorTypes.get
Stay organized with collections
Save and categorize content based on your preferences.
HTTP request
GET https://tpu.googleapis.com/v2alpha1/{name=projects/*/locations/*/acceleratorTypes/*}
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
string
Required. The resource name.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[AcceleratorType](/tpu/docs/reference/rest/v2alpha1/projects.locations.acceleratorTypes#AcceleratorType)
Authorization scopes
Requires one of the following OAuth scopes:
-
https://www.googleapis.com/auth/cloud.tpu
-
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Method: projects.locations.acceleratorTypes.get | Cloud TPU | Google Cloud,
id,url,body,title,description
126,https://cloud.google.com/tpu/docs/reference/rest/v1/projects.locations.operations/cancel,"Starts asynchronous cancellation on a long-running operation. The server makes a best effort to cancel the operation, but success is not guaranteed. If the server doesn't support this method, it returns
google.rpc.Code.UNIMPLEMENTED. Clients can use
or other methods to check whether the cancellation succeeded or whether the operation completed despite cancellation. On successful cancellation, the operation is not deleted; instead, it becomes an operation with an
[Operations.GetOperation](/tpu/docs/reference/rest/v1alpha1/projects.locations.operations/get#google.longrunning.Operations.GetOperation)
value with a
[Operation.error](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse#Operation.FIELDS.error)
of 1, corresponding to
[google.rpc.Status.code](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse#Status.FIELDS.code)
Code.CANCELLED.
HTTP request
POST https://tpu.googleapis.com/v1/{name=projects/*/locations/*/operations/*}:cancel
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
The name of the operation resource to be cancelled.
Request body
The request body must be empty.
Response body
If successful, the response body is empty.
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).",Method: projects.locations.operations.cancel | Cloud TPU | Google Cloud,
id,url,body,title,description
104,https://cloud.google.com/tpu/docs/tpu-gke-multislice,"Multislice Cloud TPUs in GKE Overview
Summary
This document explains how to manage Cloud TPUs in a Multislice
configuration using GKE. We assume familiarity with using
[single Cloud TPU slices on GKE](/tpu/docs/tpus-in-gke)
and general experience using [Multislice Cloud TPUs](/tpu/docs/tpu-gke-multislice).
We recommend that you model each Cloud TPU slice as a node pool and
submit Jobs that involve multiple node pools using the [JobSet API](https://github.com/kubernetes-sigs/jobset).
Concepts
- Auto-repair
- When a slice encounters a maintenance event, preemption or hardware failure, Google Kubernetes Engine will create a new slice. If there is insufficient capacity to create a new slice, the creation won't complete until resources becomes available. When resources become available, the slice that has encountered the event is restarted and training continues. When your training program is started, it must check for checkpoints and load them before restarting training.
- Data Center Networking (DCN)
- A higher latency, lower-throughput network that connects Cloud TPU slices in a Multislice configuration.
- Dataset
- The data that is used by a model for training or inference.
- Host
- A host is a physical computer that runs VMs. A host can run at most one VM at one time. Each VM has a dedicated Cloud TPU.
- Inference
- Load a pre-trained machine learning model onto a host and make predictions on data.
- Interchip Interconnect (ICI)
- High speed, low latency internal links that connect Cloud TPUs within a Cloud TPU Pod.
- Kubernetes Pod
- A group of one or more containers, with shared storage and network resources, and a specification for how to run the containers.
- Multislice
- Two or more Cloud TPU slices that can communicate over Data Center Network (DCN).
- Node pools
- In the GKE context, a node pool is a group of nodes (VMs)
that can be scheduled with workloads (for example, a training job) by the
Kubernetes scheduler. For more information about GKE cluster
architecture and concepts, see
[Cluster Architecture](/kubernetes-engine/docs/concepts/cluster-architecture).
- Tensor
- A data structure that is used to represent multidimensional data in a machine learning model.
- Tensor Processing Unit (TPU)
- Google's internally developed ML acceleration chip. They are designed to offer the fastest and most power-efficient compute for key machine learning tasks like matrix multiplication.
- Cloud TPU Pod
- A collection of Cloud TPU chips connected by dedicated ICI network interfaces. A Cloud TPU Pod lets you distribute the processing load across multiple Cloud TPUs.
- Cloud TPU slice
- A logical subsection of a Cloud TPU Pod consisting of Cloud TPU chips. All the chips in a slice communicate with each other using the ICI network.
- Cloud TPU VM
- A virtual machine running Linux that has access to the underlying Cloud TPU's. For v4 Cloud TPUs, each Cloud TPU VM has direct access to 4 chips. Cloud TPU VMs are sometimes called workers.
Prerequisites
This document assumes that you are familiar with
[Cloud TPU Multislice overview](/tpu/docs/multislice-overview)
and [Cloud TPUs in GKE introduction](/tpu/docs/tpus-in-gke).
Managing Multislice Cloud TPUs in GKE
Provisioning Cloud TPU Capacity for use with GKE Multislice
[Deploy Cloud TPU workloads in GKE](/kubernetes-engine/docs/how-to/tpus#plan_tpu)
details how you can reserve Cloud TPU. However, if you're using Cloud TPU
capacity directly through Cloud TPU API and would like to use the
same capacity for GKE Multislice, contact
[gke-tpu-support@google.com](mailto:gke-tpu-support@google.com) to move part of
your capacity to GKE.
Creating node pools in a Multislice environment
To create a Multislice environment you need to group multiple GKE
node pools into a Multislice group. You need to create a separate node
pool for each slice that is part of your Multislice environment. You
must add each node pool into a Multislice group. Multislice
groups are a way to group and manage multiple node pools together. Multislice
groups are defined by applying labels (name/value pairs) to each node pool that
will be part of your Multislice environment. In this tutorial, we are
using labels named
MultisliceGroup and
MultisliceGroupSize, but you can use
any name you want. All node pools in a Multislice group must have the
same labels (name/value pairs) and the same Cloud TPU topology.
You can add the Multislice label at creation time. In this example we
use
MultisliceGroup and
MultisliceGroupSize for the label names:
$ gcloud beta container node-pools create pool-name \
--region=cluster-region \
--cluster=cluster-name \
--node-locations=node-zone \
--machine-type=machine-type \
--tpu-topology=tpu-topology \
--node-labels=MultisliceGroup=multislice-group-name,MultisliceGroupSize=num-slices \
--num-nodes=num-nodes \
[--reservation-affinity=specific \
--reservation=reservation-name]
Notes:
Only one node pool operation can be performed at a time, so you will need to wait for each node pool to be created or deleted before you can start creating or deleting the next one.
Multislice environments composed of single-host slices are not supported. For example, you cannot create a Multislice with multiple
v4-16,
v5ewith (1,4) topology, or
v5e-8slices.
You can also add the
MultisliceGroupand
MultisliceGroupSize labels to on
existing node pool:
$ gcloud beta container node-pools update pool-name \
--region=cluster-region \
--cluster=cluster-name \
--node-labels=MultisliceGroup=multislice-group-name,MultisliceGroupSize=num-slices
Checking node pool status
You can check the status of your node pools with
kubectl:
$ kubectl get nodes -l MultisliceGroup=multislice-group-1 | grep ""Ready"" | wc -l
Notes:
- You need to connect to the GKE cluster before running the
command. For more information, see
[Accessing Clusters](https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/).
When running
get nodescommand, you might see an error saying:
E0608 13:25:01.586645 21293 memcache.go:255] couldn't get resource list for http://metrics.k8s.io/v1beta1 : the server is currently unable to handle the request
This error typically means you don't have a non-Cloud TPU node pool in your GKE cluster. Add a non-Cloud TPU node pool to your cluster and try the command again.
Running Multislice workloads
JobSet API
[JobSet API](https://github.com/kubernetes-sigs/jobset) is an open source
API for managing a group of Jobs together. It is a great fit for Multislice
workloads; as described in the [Cloud TPUs in GKE Introduction](/tpu/docs/tpus-in-gke)
for single-slice workloads we use a single Kubernetes
IndexedJob to run on a
single Cloud TPU slice. Similarly for Multislice environments,
we need multiple
[Kubernetes IndexedJobs](https://kubernetes.io/blog/2021/04/19/introducing-indexed-jobs/)
working together, JobSet helps achieve this.
JobSet provides:
- Failure handling: Automatic recreation of all the child Jobs in-case any of the child Job fails.
- Slice indexing: JobSet indexes each Job with an ID and adds that as part of annotations or labels on the Job and Pod.
- Workload placement: Automatically takes care of assigning a single Job to a specific domain (in this case it is every Job on a single node pool).
- Headless-Service creation: Managed lifecycle of a
[headless-svc](https://kubernetes.io/docs/concepts/services-networking/service/)for all the Jobs in the JobSet.
Installation
[Install](https://github.com/kubernetes-sigs/jobset/blob/main/docs/setup/install.md)the JobSet API on your cluster:
$ kubectl apply --server-side -f https://github.com/kubernetes-sigs/jobset/releases/download/latest-version/manifests.yaml
Verify that the JobSet Controller is running:
$ kubectl get pods -n jobset-system
Running a Multislice workload
The following YAML manifest shows how to run a Multislice workload on
four slices of v4-16s. Copy the following YAML file, save it in a file and then
run
kubectl apply -f <file-name.yaml>.:
apiVersion: jobset.x-k8s.io/v1alpha2
kind: JobSet
metadata:
name: multislice-job # JobSet name (${JOBSET_NAME})
annotations:
alpha.jobset.sigs.k8s.io/exclusive-topology: cloud.google.com/gke-nodepool # 1:1 job replica to node pool assignment
spec:
failurePolicy:
maxRestarts: 4 # The set will be restarted on failures up to 4 times.
replicatedJobs:
- name: slice # Part of the name of the child Jobs (<replicateJobName>)
replicas: 4 # Number of slices
template:
spec:
parallelism: 2 # Must be set to number of nodes in each node pool
completions: 2 # Must be set to number of nodes in each node pool
backoffLimit: 0 # Must be set to 0. Fail the job when any pod fails.
template:
spec:
affinity: # The affinity section is to make sure there is only one Multislice job running in a single Multislice Group. More on this below.
podAffinity:
requiredDuringSchedulingIgnoredDuringExecution:
- labelSelector:
matchExpressions:
- key: jobset.sigs.k8s.io/jobset-name
operator: In
values:
- multislice-job # JobSet name
topologyKey: MultisliceGroup
podAntiAffinity:
requiredDuringSchedulingIgnoredDuringExecution:
- labelSelector:
matchExpressions:
- key: jobset.sigs.k8s.io/jobset-name
operator: NotIn
values:
- multislice-job # JobSet name
topologyKey: MultisliceGroup
namespaceSelector:
matchExpressions:
- key: jobset.sigs.k8s.io/jobset-name
operator: Exists
hostNetwork: true
dnsPolicy: ClusterFirstWithHostNet
nodeSelector:
cloud.google.com/gke-tpu-accelerator: tpu-v4-podslice
cloud.google.com/gke-tpu-topology: 2x2x2
MultisliceGroupSize: ""4""
containers:
- name: jax-tpu
image: python:3.8
ports:
- containerPort: 8471
- containerPort: 8080 # Port for MXLA coordinator
securityContext:
privileged: true
command:
- bash
- -c
- |
pip install ""jax[tpu]"" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html
python -c 'import jax; print(""Global device count:"", jax.device_count())'
resources:
limits:
google.com/tpu: 4 # Number of Cloud TPU VMs per worker
The JobSet will create 4 (number of slices) Kubernetes Indexed Jobs. The Kubernetes Indexed Jobs will follow the naming pattern:
<jobSetName>-<replicateJobName>-<job-index>(in this case it is
multislice-job-slice-{0,1,2,3}) The created jobs will show up in the Google Cloud CLI under
Workloads`.
The JobSet is a Headless Service with same name as the JobSet name (in this case it is
multislice-job).
Each Indexed Job creates 2 (# of workers) Kubernetes Pods. Jobs follow the naming pattern:
<jobSetName>-<replicateJobName>-<job-index>-<worker-index>-<5 letter suffix>(in this case it is
multislice-job-slice-{0,1,2,3}-{1,2}-{5 letter suffix}).
Multislice on Google Kubernetes Engine supports synchronous multi-controller training. To achieve this,
parallelism, and
completionsneed to be set to the number of nodes in each node pool, and
backoffneeds to be set to 0. The number of nodes is number of cores divided by the number of cores. For v4 TPUs this is eight so for
v4-16, set
parallelismand
completionsto
2.
The minimum JAX version is:
[v0.4.9](https://github.com/google/jax/releases).
Injection of environment variables in GKE
The GKE webhook automatically injects environment variables into the Pod spec of the Indexed Job which is then subsequently inherited by the Kubernetes Pod running the Multislice workload. GKE injects the following environment variables:
|Environment Variable
|Value
|
TPU_WORKER_ID
|
metadata.annotations['batch.kubernetes.io/job-completion-index']
|
TPU_WORKER_HOSTNAMES
|Comma separated list of pod indexes of the job as follows:
|
MEGASCALE_NUM_SLICES
|
metadata.annotations['jobset.sigs.k8s.io/replicatedjob-replicas']
|
MEGASCALE_SLICE_ID
|
metadata.annotations['jobset.sigs.k8s.io/job-index']
|
MEGASCALE_COORDINATOR_ADDRESS
|The value is composed of the jobSetName and the replicatedJobName:
Optional: Turn off hostNetwork on your GKE Pods
Use
hostNetwork: true in your Pod spec to skip all the Kubernetes networking
stack and let your Kubernetes Pods use the host network directly for VM to VM
communication. This improves network performance between slices. To keep using
the
podHostnames for worker discovery with
hostNetwork, set
dnsPolicy: ClusterFirstWithHostNet. This is important when auto-resuming training
jobs and you need to have the same names for reloading the same checkpoints.
To turn off
hostNetworking remove the following two lines from your Pod spec:
hostNetwork: true dnsPolicy: ClusterFirstWithHostNet
Orchestrating Multislice Jobs
Avoid Scheduling Deadlock
If you submit multiple Multislice Jobs (JobSet A and B) of the same
MultisliceGroupSize, you could have a scenario where both JobSets are partially
scheduled (for example, 5 out of 8 child jobs are scheduled from JobSet A and 3
out of 8 child jobs are scheduled from JobSet B). This will cause your node pools
to sit idle and both JobSet A and B to be in a blocked state.
To avoid this you can use
podAffinity and
podAntiAffinity. For Multislice
workloads, change the
topologyKey to be the label (
MultisliceGroup)
representing a Multislice group instead of the label
cloud.google.com/gke-nodepool.
For example:
affinity:
podAffinity:
requiredDuringSchedulingIgnoredDuringExecution:
- labelSelector:
matchExpressions:
- key: jobset.sigs.k8s.io/jobset-name
operator: In
values:
- multislice-job # JobSet name
topologyKey: MultisliceGroup
podAntiAffinity:
requiredDuringSchedulingIgnoredDuringExecution:
- labelSelector:
matchExpressions:
- key: jobset.sigs.k8s.io/jobset-name
operator: NotIn
values:
- multislice-job # JobSet name
topologyKey: MultisliceGroup
namespaceSelector:
matchExpressions:
- key: jobset.sigs.k8s.io/jobset-name
operator: Exists
Observability
Cloud Logging Logs
The GKE Pods of the JobSet are named using the following pattern:
<jobSetName>-<replicateJobName>-<job-index>-<worker-index>-<5 letter suffix>
You can view your logs using the
[Cloud Logging Logs Explorer](/logging/docs/view/logs-explorer-interface)
with the following filter to view the container logs for your workload:
resource.type=""k8s_container""
resource.labels.cluster_name=<cluster-name>
labels.""k8s-pod/jobset_sigs_k8s_io/jobset-name""=<jobSetName>
To filter logs for slice
x and worker
y, use the following filter:
resource.type=""k8s_container""
resource.labels.cluster_name=<cluster-name>
labels.""k8s-pod/jobset_sigs_k8s_io/jobset-name""=<jobSetName>
resource.labels.pod_name:<jobSetName>-<replicateJobName>-<job-index>-<worker-index>",Multislice Cloud TPUs in GKE Overview | Google Cloud,
id,url,body,title,description
90,https://cloud.google.com/tpu/docs/queued-resources,"Queued resources user guide
Queued resources enable you to request Cloud TPU resources in a queued manner. When you request queued resources, the request is added to a queue maintained by the Cloud TPU service. When the requested resource becomes available, it's assigned to your Google Cloud project for your immediate exclusive use. It will remain assigned to your project unless you delete it or it's preempted. Only preemptible TPUs are eligible for preemption.
You can specify an optional
[start and end time](#request-queued-specified-time) in a queued
resource request. The start time specifies the earliest time in which to fill
the request. If a request has not been filled by the specified end time, the
request expires. The request remains in the queue after it has expired.
Queued resource requests can be in one the following states:
WAITING_FOR_RESOURCES
-
The request has passed initial validation and has been added to the queue.
It remains in this state until there are sufficient free resources to begin
provisioning your request or the
[allocation interval](#request-queued-specified-time)elapses. When demand is high, not all requests can be immediately provisioned. If you need more reliable obtainability of TPUs, consider purchasing a reservation.
PROVISIONING
- The request has been selected from the queue and its resources are currently being allocated.
ACTIVE
- The request has been allocated. When queued resource requests are in the
ACTIVEstate, you can manage your TPU VMs as described in
[Manage TPUs](/tpu/docs/managing-tpus-tpu-vm).
FAILED
- The request couldn't be completed, either because there is a problem with the request or the requested resources were not available within the allocation interval. The request remains in the queue until it is explicitly deleted.
SUSPENDING
- The resources associated with the request are currently being deleted.
SUSPENDED
- The resources specified in the request have been deleted. When a request
is in the
SUSPENDEDstate, it's no longer eligible for further allocation.
Prerequisites
Before running the commands in this guide, make sure you:
- Install the
[Google Cloud CLI alpha components](https://cloud.google.com/sdk/gcloud/reference/components/install)
- Enable the
[Cloud TPU API](https://console.cloud.google.com/apis/library/tpu.googleapis.com?_ga=2.174913505.2116185568.1664815833-671377895.1662479582&_gac=1.184538452.1663874535.Cj0KCQjwj7CZBhDHARIsAPPWv3diPfi1d-VcluDdaKlKf4CZnZ1K9DUABPC1VeRRurakgONsiLWUVMoaAmANEALw_wcB)
Request an on-demand queued resource
You can request an on-demand queued resource using the
gcloud alpha compute tpus queued-resources create command. For more information about
on-demand resources, see
[Quota types](/tpu/docs/quota#quota_types).
gcloud
gcloud alpha compute tpus queued-resources create your-queued-resource-id \ --node-id your-node-id \ --project your-project \ --zone us-central2-b \ --accelerator-type v4-8 \ --runtime-version tpu-vm-tf-2.15.0-pjrt
curl
curl -X POST -H ""Authorization: Bearer $(gcloud auth print-access-token)"" \ -H ""Content-Type: application/json"" \ -d ""{ 'tpu': { 'node_spec': { 'parent': 'projects/your-project-number/locations/us-central2-b', 'node_id': 'your-node-id', 'node': { 'accelerator_type': 'v4-8', 'runtime_version': 'tpu-vm-tf-2.15.0-pjrt', } } } }"" \ https://tpu.googleapis.com/v2alpha1/projects/your-project-id/locations/us-central2-b/queuedResources?queued_resource_id=your-queued-resource-id
Command parameter descriptions
queued-resource-id
- The user-assigned ID of the queued resource request.
node-id
- The user-assigned ID of the TPU which is created when the queued resource request is allocated.
project
- Your Google Cloud project.
zone
- The
[zone](/tpu/docs/types-zones-tpu-vm)where you plan to create your Cloud TPU.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
runtime-version
- The Cloud TPU software
[version.](/tpu/docs/supported-tpu-versions#tpu_software_versions)
Default slice sizes for on-demand queued resources
When you use on-demand quota, you must request a slice size less than the default limit for the accelerator type you are using. Requests that exceed the default limits are declined by the system.
The following table shows the TPU types and their associated default limits.
|Accelerator type
|Default limit (in number of TensorCores)
|v2
|128
|v3
|128
|v4
|384
|v5
|32
If you require larger slice sizes, contact Cloud TPU support for additional information.
Request a queued resource using reserved quota
You can request a queued resource using reserved quota by specifying the
--reserved flag in your
gcloud command or
guaranteed.reserved=true in your
curl request. For more information about reserved quota, see
[Quota
types](/tpu/docs/quota#quota_types).
gcloud
gcloud alpha compute tpus queued-resources create your-queued-resource-id \ --node-id your-node-id \ --project your-project \ --zone us-central2-b \ --accelerator-type v4-8 \ --runtime-version tpu-vm-tf-2.15.0-pjrt \ --reserved
curl
curl -X POST -H ""Authorization: Bearer $(gcloud auth print-access-token)"" \ -H ""Content-Type: application/json"" \ -d ""{ 'tpu': { 'node_spec': { 'parent': 'projects/your-project-number/locations/us-central2-b', 'node_id': 'your-node-id', 'node': { 'accelerator_type': 'v4-8', 'runtime_version': 'tpu-vm-tf-2.15.0-pjrt', } } }, 'guaranteed': { 'reserved': true, } }"" \ https://tpu.googleapis.com/v2alpha1/projects/your-project-id/locations/us-central2-b/queuedResources?queued_resource_id=your-queued-resource-id
Command parameter descriptions
queued-resource-id
- The user-assigned ID of the queued resource request.
node-id
- The user-assigned ID of the TPU which is created when the queued resource request is allocated.
project
- Your Google Cloud project.
zone
- The
[zone](/tpu/docs/types-zones-tpu-vm)where you plan to create your Cloud TPU.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
runtime-version
- The Cloud TPU software
[version.](/tpu/docs/supported-tpu-versions#tpu_software_versions)
reserved
- Use this flag when requesting queued resources as part of a Cloud TPU reservation.
Request a preemptible queued resource
You can request a preemptible queued resource. A
[preemptible
resource](/tpu/docs/preemptible) is a resource that may
be assigned to another workload if extra resources are needed by other
workloads. Preemptible resources cost less and you may get access to resources
sooner compared to a non-preemptible request. For more information about
preemptible quota, see [Quota types](/tpu/docs/quota#quota_types).
gcloud
gcloud alpha compute tpus queued-resources create your-queued-resource-id \ --node-id your-node-id \ --project your-project-id \ --zone us-central2-b \ --accelerator-type v4-8 \ --runtime-version tpu-vm-tf-2.15.0-pjrt \ --best-effort
curl
curl -X POST -H ""Authorization: Bearer $(gcloud auth print-access-token)"" \ -H ""Content-Type: application/json"" \ -d ""{ 'tpu': { 'node_spec': { 'parent': 'projects/your-project-number/locations/us-central2-b', 'node_id': 'your-node-id', 'node': { 'accelerator_type': 'v4-8', 'runtime_version': 'tpu-vm-tf-2.15.0-pjrt', } } }, 'best_effort': {} }"" \ https://tpu.googleapis.com/v2alpha1/projects/your-project-id/locations/us-central2-b/queuedResources?queued_resource_id=your-queued-resource-id
Command parameter descriptions
queued-resource-request-id
- The user-assigned ID of the queued resource request.
node-id
- The user-defined ID of the TPU created in response to the request.
project
- The ID of the project where the queued resource is allocated.
zone
- The
[zone](/tpu/docs/types-zones-tpu-vm)where you plan to create your Cloud TPU.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
runtime-version
- The Cloud TPU software
[version.](/tpu/docs/supported-tpu-versions#tpu_software_versions)
best-effort
- A boolean flag specifying that the queued resource is preemptible.
Request a queued resource to be allocated before or after a specified time
You can specify an optional
[start time](#request-queued-after-time), [end
time](#request-queued-until-time), [start
duration](#request-queued-after-duration), or [end
duration](#request-queued-until-duration) in a queued resource request. The
start time or start duration specifies the earliest time in which to fill the
request. If a request has not been filled by the specified end time or within
the specified duration, the request expires. After the request has expired, it
remains in the queue but is no longer eligible for allocation.
You can also specify an
[allocation interval](#request-queued-interval) by
specifying a start time or duration and an end time or duration.
See
[Datetime](https://cloud.google.com/sdk/gcloud/reference/topic/datetimes) for a
list of supported timestamp and duration formats.
Request a queued resource after a specified duration
You can specify a duration after which a resource should be allocated using
the
--valid-after-duration flag. The following example requests a v4-32 to be
allocated after six hours.
gcloud
gcloud alpha compute tpus queued-resources create your-queued-resource-id \ --node-id your-node-id \ --project your-project-id \ --zone us-central2-b \ --accelerator-type v4-32 \ --runtime-version tpu-vm-tf-2.15.0-pod-pjrt \ --valid-after-duration 6h
curl
curl -X POST -H ""Authorization: Bearer $(gcloud auth print-access-token)"" \ -H ""Content-Type: application/json"" \ -d ""{ 'tpu': { 'node_spec': { 'parent': 'projects/your-project-number/locations/us-central2-b', 'node_id': 'your-node-id', 'node': { 'accelerator_type': 'v4-32', 'runtime_version': 'tpu-vm-tf-2.15.0-pod-pjrt', } } }, 'queueing_policy': { 'valid_after_duration': { 'seconds': 21600 } }"" \ https://tpu.googleapis.com/v2alpha1/projects/your-project-id/locations/us-central2-b/queuedResources?queued_resource_id=your-queued-resource-id
Command parameter descriptions
queued-resource-request-id
- The user-assigned ID of the queued resource request.
node-id
- The user-defined ID of the TPU created in response to the request.
project
- The Google Cloud project where the queued resource is allocated.
zone
- The
[zone](/tpu/docs/types-zones-tpu-vm)where you plan to create your Cloud TPU.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
runtime-version
- The Cloud TPU software
[version.](/tpu/docs/supported-tpu-versions#tpu_software_versions)
valid-after-duration
- The duration before which the TPU must not be provisioned. For more information on duration formats, see
[Google Cloud CLI topic datetime](/sdk/gcloud/reference/topic/datetimes)
Request a queued resource that expires after a specified duration
You can specify how long a queued resource request remains valid using
the
--valid-until-duration flag. The following example requests a v4-32 that
expires if not filled in six hours.
gcloud
gcloud alpha compute tpus queued-resources create your-queued-resource-id \ --node-id your-node-id \ --project your-project-id \ --zone us-central2-b \ --accelerator-type v4-32 \ --runtime-version tpu-vm-tf-2.15.0-pod-pjrt \ --valid-until-duration 6h
curl
curl -X POST -H ""Authorization: Bearer $(gcloud auth print-access-token)"" \ -H ""Content-Type: application/json"" \ -d ""{ 'tpu': { 'node_spec': { 'parent': 'projects/your-project-number/locations/us-central2-b', 'node_id': 'your-node-id', 'node': { 'accelerator_type': 'v4-32', 'runtime_version': 'tpu-vm-tf-2.15.0-pod-pjrt', } } }, 'queueing_policy': { 'valid_until_duration': { 'seconds': 21600 } }"" \ https://tpu.googleapis.com/v2alpha1/projects/your-project-id/locations/us-central2-b/queuedResources?queued_resource_id=your-queued-resource-id
Command parameter descriptions
queued-resource-request-id
- The user-assigned ID of the queued resource request.
node-id
- The user-defined ID of the TPU created in response to the request.
project
- The Google Cloud project where the queued resource is allocated.
zone
- The
[zone](/tpu/docs/types-zones-tpu-vm)where you plan to create your Cloud TPU.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
runtime-version
- The Cloud TPU software
[version.](/tpu/docs/supported-tpu-versions#tpu_software_versions)
valid-until-duration
- The duration for which the request is valid. For more information on duration formats, see
[Google Cloud CLI topic datetime](/sdk/gcloud/reference/topic/datetimes)
Request a queued resource after a specified time
You can specify a time after which a resource should be allocated using the
--valid-after-time flag.
The following command requests a v4-4096 TPU with
runtime version
tpu-vm-tf-2.15.0-pjrt to be allocated after 9AM on December 14, 2022.
gcloud
gcloud alpha compute tpus queued-resources create your-queued-resource-id \ --node-id your-node-id \ --project your-project-id \ --zone us-central2-b \ --accelerator-type v4-4096 \ --runtime-version tpu-vm-tf-2.15.0-pod-pjrt \ --valid-after-time 2022-12-14T09:00:00Z
curl
curl -X POST -H ""Authorization: Bearer $(gcloud auth print-access-token)"" \ -H ""Content-Type: application/json"" \ -d ""{ 'tpu': { 'node_spec': { 'parent': 'projects/your-project-number/locations/us-central2-b', 'node_id': 'your-node-id', 'node': { 'accelerator_type': 'v4-4096', 'runtime_version': 'tpu-vm-tf-2.15.0-pod-pjrt', } } }, 'queueing_policy': { 'valid_after_time': { 'seconds': 2022-12-14T09:00:00Z } } }"" \ https://tpu.googleapis.com/v2alpha1/projects/your-project-id/locations/us-central2-b/queuedResources?queued_resource_id=your-queued-resource-id
Command parameter descriptions
queued-resource-request-id
- The user-assigned ID of the queued resource request.
node-id
- The user-defined ID of the TPU created in response to the request.
project
- The Google Cloud project where the queued resource is allocated.
zone
- The
[zone](/tpu/docs/types-zones-tpu-vm)where you plan to create your Cloud TPU.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
runtime-version
- The Cloud TPU software
[version.](/tpu/docs/supported-tpu-versions#tpu_software_versions)
valid-after-time
- The time, after which, the resource should be allocated For more information on duration formats, see
[Google Cloud CLI topic datetime](/sdk/gcloud/reference/topic/datetimes).
Request a queued resource before a specified time
You can specify a time before which the resource should be allocated using the
--valid-until-time flag.
The following command requests a v4-4096 TPU node with
runtime version
tpu-vm-tf-2.10.0-pod to be created no later than December 14, 2022 at 9:00 AM.
gcloud
gcloud alpha compute tpus queued-resources create your-queued-resource-id \ --node-id your-node-id \ --project your-project-id \ --zone us-central2-b \ --accelerator-type v4-4096 \ --runtime-version tpu-vm-tf-2.15.0-pod-pjrt \ --valid-until-time 2022-12-14T09:00:00Z
curl
curl -X POST -H ""Authorization: Bearer $(gcloud auth print-access-token)"" \ -H ""Content-Type: application/json"" \ -d ""{ 'tpu': { 'node_spec': { 'parent': 'projects/your-project-number/locations/us-central2-b', 'node_id': 'your-node-id', 'node': { 'accelerator_type': 'v4-4096', 'runtime_version': 'tpu-vm-tf-2.15.0-pod-pjrt', } } }, 'queueing_policy': { 'valid_until_time': { 'seconds': 1655197200 } } }"" \ https://tpu.googleapis.com/v2alpha1/projects/your-project-id/locations/us-central2-b/queuedResources?queued_resource_id=your-queued-resource-id
Command parameter descriptions
queued-resource-request-id
- The user-assigned ID of the queued resource request.
node-id
- The user-defined ID of the TPU created in response to the request.
project
- The ID of the project where the queued resource is allocated.
zone
- The
[zone](/tpu/docs/types-zones-tpu-vm)where you plan to create your Cloud TPU.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
runtime-version
- The Cloud TPU software
[version.](/tpu/docs/supported-tpu-versions#tpu_software_versions)
valid-until-time
- The time after which the request is canceled. For more information on duration formats, see
[Google Cloud CLI topic datetime](/sdk/gcloud/reference/topic/datetimes).
Request a queued resource to be allocated within a specified interval
You can specify an allocation interval using any pair of the
--valid-after-time,
--valid-after-duration,
--valid-until-duration, and
--valid-until-time flags,
provided one flag specifies the beginning of the allocation interval and the
other specifies the end of the allocation interval.
The following command requests a v4-32 in 5 hours and 30 minutes from the current time, to be created no later than December 14, 2022 at 9:00 AM.
gcloud
gcloud alpha compute tpus queued-resources create your-queued-resource-id \ --node-id your-node-id \ --project your-project-id \ --zone us-central2-b \ --accelerator-type v4-32 \ --runtime-version tpu-vm-tf-2.15.0-pod-pjrt \ --valid-after-duration 5h30m \ --valid-until-time 2022-12-14T09:00:00Z
curl
curl -X POST -H ""Authorization: Bearer $(gcloud auth print-access-token)"" \ -H ""Content-Type: application/json"" \ -d ""{ 'tpu': { 'node_spec': { 'parent': 'projects/your-project-number/locations/us-central2-b', 'node_id': 'your-node-id', 'node': { 'accelerator_type': 'v4-32', 'runtime_version': 'tpu-vm-tf-2.15.0-pod-pjrt', } } }, 'queueing_policy': { 'validInterval': { 'startTime': '2022-12-10T14:30:00Z', 'endTime': '2022-12-14T09:00:00Z' } }, }"" \ https://tpu.googleapis.com/v2alpha1/projects/your-project-id/locations/us-central2-b/queuedResources?queued_resource_id=your-queued-resource-id
Command flag descriptions
queued-resource-request-id
- The user-assigned ID of the queued resource request.
node-id
- The user-defined ID of the TPU created in response to the request.
project
- The ID of the project where the queued resource is allocated.
zone
- The
[zone](/tpu/docs/types-zones-tpu-vm)where you plan to create your Cloud TPU.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
runtime-version
- The Cloud TPU software
[version.](/tpu/docs/supported-tpu-versions#tpu_software_versions)
valid-until-timw
- The time after which the request is canceled. For more information on duration formats, see
[Google Cloud CLI topic datetime](/sdk/gcloud/reference/topic/datetimes).
valid-until-duration
- The duration for which the request is valid. For more information on duration formats, see
[Google Cloud CLI topic datetime](/sdk/gcloud/reference/topic/datetimes).
Request a queued resource with a startup script
You can specify a script to be run on a queued resource after it has been
provisioned. When using the
gcloud command, you can use either the
--metadata
or
--metadata-from-file flag to specify a script command or a file containing
the script code, respectively. When using
curl, you must include the script
code in the JSON content. The following example creates a queued resource
request that will run the script contained in
startup-script.sh. The
curl
example shows an inline script in the JSON body.
gcloud
gcloud alpha compute tpus queued-resources create your-queued-resource-id \ --node-id your-node-id \ --project your-project \ --zone us-central2-b \ --accelerator-type v4-8 \ --runtime-version tpu-vm-tf-2.12.0 \ --reserved \ --metadata-from-file='startup-script=startup-script.sh'
curl
curl -X POST -H ""Authorization: Bearer $(gcloud auth print-access-token)"" \ -H ""Content-Type: application/json"" \ -d ""{ tpu: { node_spec: { parent: 'projects/your-project-number/locations/us-central2-b', node_id: 'your-node-id', node: { accelerator_type: 'v2-8', runtime_version: 'tpu-vm-tf-2.15.0-pjrt', metadata: { ""startup-script"": ""#! /bin/bash\npwd > /tmp/out.txt\nwhoami >> /tmp/out.txt"" } } } }, 'queueing_policy': { 'validInterval': { 'startTime': '2022-12-10T14:30:00Z', 'endTime': '2022-12-14T09:00:00Z' } }, }"" \ https://tpu.googleapis.com/v2alpha1/projects/your-project-id/locations/us-central2-b/queuedResources?queued_resource_id=your-queued-resource-id
Command flag descriptions
queued-resource-request-id
- The user-assigned ID of the queued resource request.
node-id
- The user-defined ID of the TPU created in response to the request.
project
- The ID of the project where the queued resource is allocated.
zone
- The
[zone](/tpu/docs/types-zones-tpu-vm)where you plan to create your Cloud TPU.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
runtime-version
- The Cloud TPU software
[version.](/tpu/docs/supported-tpu-versions#tpu_software_versions)
validInterval
- The time during which the request is valid after which the request is
canceled. For more information on duration formats, see
[Google Cloud CLI topic datetime](/sdk/gcloud/reference/topic/datetimes).
metadata-from-file
- Specifies a file that contains metadata. If you don't specify a fully qualified path to the metadata file, the command assumes it is located in the current directory. In this example the file contains a startup script that is run when the queued resource is provisioned.
metadata
- Specifies metadata for the request. In this example the metadata is a startup script command run when the queued resource is provisioned.
Request a queued resources with a specified network and subnetwork
You can request a queued resource specifying the network and subnetwork that you want to connect your TPU to.
gcloud
gcloud alpha compute tpus queued-resources create your-queued-resource-id \ --node-id your-node-id \ --project your-project \ --zone us-central2-b \ --accelerator-type v4-8 \ --runtime-version tpu-vm-tf-2.15.0-pjrt \ --network network-name \ --subnetwork subnetwork-name
curl
curl -X POST -H ""Authorization: Bearer $(gcloud auth print-access-token)"" \ -H ""Content-Type: application/json"" \ -d ""{ 'tpu': { 'node_spec': { 'parent': 'projects/your-project-number/locations/us-central2-b', 'node_id': 'your-node-id', 'node': { 'accelerator_type': 'v4-8', 'runtime_version': 'tpu-vm-tf-2.15.0-pjrt', 'network_config': { 'network': 'network-name', 'subnetwork': 'subnetwork-name', 'enable_external_ips': true } } }, 'guaranteed': { 'reserved': true, } }"" \ https://tpu.googleapis.com/v2alpha1/projects/your-project-id/locations/us-central2-b/queuedResources?queued_resource_id=your-queued-resource-id
Command parameter descriptions
queued-resource-id
- The user-assigned ID of the queued resource request.
node-id
- The user-assigned ID of the TPU which is created when the queued resource request is allocated.
project
- Your Google Cloud project.
zone
- The
[zone](/tpu/docs/types-zones-tpu-vm)where you plan to create your Cloud TPU.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
runtime-version
- The Cloud TPU software
[version.](/tpu/docs/supported-tpu-versions#tpu_software_versions)
reserved
- Use this flag when requesting queued resources as part of a Cloud TPU reservation.
network
- A network that the queued resource will be a part of.
subnetwork
- A subnetwork that the queued resource will be a part of.
Delete a queued resource request
You can delete a queued resource request and the TPU VM created by the request
by passing the
--force flag to the
queued-resource delete command. Otherwise,
you must delete the TPU VM before deleting the queued resource request. When you
delete the TPU VM, the queued resource request transitions to the
SUSPENDED state, after which the queued resource request may be
deleted.
The following commands delete the queued resource request named
""my-queued-resource"" in the ""my-project"" project in zone ""us-central2-b"". It
uses the
--force flag to delete both the TPU VM and the queued resource
request.
gcloud
gcloud alpha compute tpus queued-resources delete my-queued-resource \ --project my-project \ --zone us-central2-b \ --force \ --async
curl
curl -X DELETE -H ""Authorization: Bearer $(gcloud auth print-access-token)"" \ -H ""Content-Type: application/json"" \ https://tpu.googleapis.com/v2alpha1/projects/my-project/locations/us-central2-b/queuedResources/my-queued-resource?force=true
Command flag descriptions
queued-resource-request-id
- The user-assigned ID of the queued resource request.
project
- The Google Cloud project where the queued resource is allocated.
zone
- The
[zone](/tpu/docs/types-zones-tpu-vm)of the Cloud TPU to delete.
force
- Delete both the TPU VM and the queued resource request.
The following commands delete the queued resource request named ""my-queued-resource"" in the ""my-project"" project in zone ""us-central2-b"".
gcloud
gcloud alpha compute tpus queued-resources delete your-queued-resource-id \ --project your-project-id \ --zone us-central2-b
curl
curl -X DELETE -H ""Authorization: Bearer $(gcloud auth print-access-token)"" \ -H ""Content-Type: application/json"" \ https://tpu.googleapis.com/v2alpha1/projects/your-project-id/locations/us-central2-b/queuedResources/your-queued-resource-id
Command flag descriptions
queued-resource-request-id
- The user-assigned ID of the queued resource request.
project
- The Google Cloud project where the queued resource is allocated.
zone
- The
[zone](/tpu/docs/types-zones-tpu-vm)where you plan to create your Cloud TPU.
Retrieve state and diagnostic information about a queued resource request
Retrieve the state and diagnostic information about a queued resource request:
gcloud
gcloud alpha compute tpus queued-resources describe queued-resource-request-id \ --project your-project-id \ --zone us-central2-b
curl
curl -X GET -H ""Authorization: Bearer $(gcloud auth print-access-token)"" \ -H ""Content-Type: application/json"" \ https://tpu.googleapis.com/v2alpha1/projects/your-project-id/locations/us-central2-b/queuedResources/your-queued-resource-id
Command flag descriptions
queued-resource-request-id
- The user-assigned ID of the queued resource request.
project
- The ID of the project where the queued resource is allocated.
zone
- The
[zone](/tpu/docs/types-zones-tpu-vm)where you plan to create your Cloud TPU.
If the request fails, the response will contain error information. For a request that is waiting for resources, the output will look similar to the following:
name: projects/your-project-id/locations/us-central2-b/queuedResources/your-queued-resource-id state: state: WAITING_FOR_RESOURCES tpu: nodeSpec: - node: acceleratorType: v4-8 bootDisk: {} networkConfig: enableExternalIps: true queuedResource: projects/your-project-number/locations/us-central2-b/queuedResources/your-queued-resource-id runtimeVersion: tpu-vm-tf-2.10.0 schedulingConfig: {} serviceAccount: {} shieldedInstanceConfig: {} useTpuVm: true nodeId: your-node-id parent: projects/your-project-number/locations/us-central2-b
List queued resource requests in your project
The following command lists the queued resource requests in project ""your-project-id"":
gcloud
gcloud alpha compute tpus queued-resources list --project your-project-id \ --zone us-central2-b
curl
curl -X GET -H ""Authorization: Bearer $(gcloud auth print-access-token)"" \ -H ""Content-Type: application/json"" \ https://tpu.googleapis.com/v2alpha1/projects/your-project-id/locations/us-central2-b/queuedResources
Command flag descriptions
project
- The Google Cloud project where the queued resource is allocated.
zone
- The
[zone](/tpu/docs/types-zones-tpu-vm)where you plan to create your Cloud TPU.",Queued resources user guide | Cloud TPU | Google Cloud,
id,url,body,title,description
165,https://cloud.google.com/tpu/docs/release-notes,"This page documents production updates to Cloud TPU. You can periodically check this page for announcements about new or updated features, bug fixes, known issues, and deprecated functionality.
You can see the latest product updates for all of Google Cloud on the
[
Google Cloud](/release-notes) page, browse and filter all release notes in the
[Google Cloud console](https://console.cloud.google.com/release-notes),
or programmatically access release notes in
[BigQuery](https://console.cloud.google.com/bigquery?p=bigquery-public-data&d=google_cloud_release_notes&t=release_notes&page=table).
To get the latest product updates delivered to you, add the URL of this page to your
[feed
reader](https://wikipedia.org/wiki/Comparison_of_feed_aggregators), or add the feed URL directly:
https://cloud.google.com/feeds/tpu-release-notes.xml
December 04, 2023
Cloud TPU now supports TensorFlow 2.14.1. For more information see the
[TensorFlow 2.14.1 release notes](https://github.com/tensorflow/tensorflow/releases/tag/v2.14.1).
November 13, 2023
Cloud TPU now supports TensorFlow 2.15.0, which adds support for PJRT. For more information see the
[TensorFlow 2.15.0 release notes](https://github.com/tensorflow/tensorflow/releases/tag/v2.15.0).
October 05, 2023
Cloud TPU now supports TensorFlow 2.13.1. For more information see the
[TensorFlow 2.13.1 release notes](https://github.com/tensorflow/tensorflow/releases/tag/v2.13.1).
September 27, 2023
Cloud TPU now supports TensorFlow 2.14.0. For more information see the
[TensorFlow 2.14.0 release notes](https://github.com/tensorflow/tensorflow/releases/tag/v2.14.0).
August 29, 2023
You can now create Cloud Tensor Processing Unit (TPU) nodes in Google Kubernetes Engine (GKE) to run AI workloads, from training to inference models. GKE manages your cluster by automating TPU resource provisioning, scaling, scheduling, repairing, and upgrading. GKE provides TPU infrastructure metrics in Cloud Monitoring, TPU logs, and error reports for better visibility and monitoring of TPU node pools in GKE clusters. TPUs are available with GKE Standard clusters. GKE supports TPU v4 in version 1.26.1.gke-1500 and later, and supports TPU v5e in version 1.27.2-gke.1500 and later. To learn more, see
[TPUs in GKE introduction](https://cloud.google.com/tpu/docs/tpus-in-gke).
July 21, 2023
Cloud TPU now supports TensorFlow 2.12.1. For more information see the
[TensorFlow 2.12.1 release notes](https://github.com/tensorflow/tensorflow/releases/tag/v2.12.1).
July 10, 2023
Cloud TPU now supports TensorFlow 2.13.0. For more information see the
[TensorFlow 2.13.0 Release Notes](https://github.com/tensorflow/tensorflow/releases/tag/v2.13.0).
June 07, 2023
You can now view historical logs of maintenance events on your TPU in
[system
event audit logs](https://cloud.google.com/tpu/docs/audit-logs#audited_operations). For additional
information see the [maintenance events documentation](https://cloud.google.com/tpu/docs/maintenance-events).
March 31, 2023
Cloud TPU now supports Tensorflow 2.11.1. For more information see the
[TensorFlow 2.11.1 release notes](https://github.com/tensorflow/tensorflow/releases/tag/v2.11.1).
March 27, 2023
Cloud TPU now supports Tensorflow 2.12.0. For more information see the
[TensorFlow 2.12 release notes](https://github.com/tensorflow/tensorflow/releases/tag/v2.12.0).
March 24, 2023
Cloud TPUs now support the
[PyTorch 2.0 release](https://github.com/pytorch/pytorch/releases), via PyTorch/XLA integration. On top of the underlying improvements and bug fixes in PyTorch's 2.0 release, this release introduces several features, and PyTorch/XLA specific bug fixes.
Beta Features
PJRT runtime
- Checkout our newest
[document](https://github.com/pytorch/xla/blob/r2.0/docs/pjrt.md); PjRt is the default runtime in 2.0.
- New Implementation of
xm.rendezvouswith XLA collective communication which scales better (
[#4181](https://github.com/pytorch/xla/pull/4181))
- New PJRT TPU backend through the C-API (
[#4077](https://github.com/pytorch/xla/pull/4077))
- Use PJRT to default if no runtime is configured (
[#4599](https://github.com/pytorch/xla/pull/4599))
- Experimental support for torch.distributed and DDP on TPU v2 and v3 (
)
[#4520](https://github.com/pytorch/xla/pull/4520)
FSDP
- Add
auto_wrap_policyinto XLA FSDP for automatic wrapping (
[#4318](https://github.com/pytorch/xla/pull/4318))
Stable Features
Lazy Tensor Core Migration
- Migration is completed, checkout this
[dev discussion](https://dev-discuss.pytorch.org/t/pytorch-xla-2022-q4-dev-update/961)for more detail.
- Naively inherits LazyTensor (
[#4271](https://github.com/pytorch/xla/pull/4271))
- Adopt even more LazyTensor interfaces (
[#4317](https://github.com/pytorch/xla/pull/4317))
- Introduce XLAGraphExecutor (
[#4270](https://github.com/pytorch/xla/pull/4270))
- Inherits LazyGraphExecutor (
[#4296](https://github.com/pytorch/xla/pull/4296))
- Adopt more LazyGraphExecutor virtual interfaces (
[#4314](https://github.com/pytorch/xla/pull/4314))
- Rollback to use
xla::Shapeinstead of
torch::lazy::Shape(
[#4111](https://github.com/pytorch/xla/pull/4111))
- Use TORCH_LAZY_COUNTER/METRIC (
[#4208](https://github.com/pytorch/xla/pull/4208))
Improvements & Additions
- Add an option to increase the worker thread efficiency for data loading (
[#4727](https://github.com/pytorch/xla/pull/4727))
- Improve numerical stability of torch.sigmoid (
[#4311](https://github.com/pytorch/xla/pull/4311))
- Add an api to clear counter and metrics (
[#4109](https://github.com/pytorch/xla/pull/4109))
- Add
met.short_metrics_reportto display more concise metrics report (
[#4148](https://github.com/pytorch/xla/pull/4148))
- Document environment variables (
[#4273](https://github.com/pytorch/xla/pull/4273))
- Op Lowering
Experimental Features
TorchDynamo (
torch.compile) support
- Checkout our newest
[doc](https://github.com/pytorch/xla/blob/r2.0/docs/dynamo.md).
- Dynamo bridge python binding (
[#4119](https://github.com/pytorch/xla/pull/4119))
- Dynamo bridge backend implementation (
[#4523](https://github.com/pytorch/xla/pull/4523))
- Training optimization: make execution async (
[#4425](https://github.com/pytorch/xla/pull/4425))
- Training optimization: reduce graph execution per step (
[#4523](https://github.com/pytorch/xla/pull/4523))
PyTorch/XLA GSPMD on single host
- Preserve parameter sharding with sharded data placeholder (
[#4721)](https://github.com/pytorch/xla/pull/4721)
- Transfer shards from server to host (
[#4508](https://github.com/pytorch/xla/pull/4508))
- Store the sharding annotation within XLATensor(#
[4390](https://github.com/pytorch/xla/pull/4390))
- Use d2d replication for more efficient input sharding (
[#4336](https://github.com/pytorch/xla/pull/4336))
- Mesh to support custom device order. (
[#4162](https://github.com/pytorch/xla/pull/4162))
- Introduce virtual SPMD device to avoid unpartitioned data transfer (
[#4091](https://github.com/pytorch/xla/pull/4091))
Ongoing development
- Ongoing Dynamic Shape implementation
- Ongoing SPMD multi host execution (
[#4573](https://github.com/pytorch/xla/pull/4573))
Bug fixes & improvements
December 19, 2022
Cloud TPU now supports TensorFlow patches:
[2.8.4](https://pypi.org/project/tensorflow/2.8.4/), [2.9.3](https://pypi.org/project/tensorflow/2.9.3/), and [2.10.1](https://pypi.org/project/tensorflow/2.10.1/). See the TensorFlow release notes for details:
December 01, 2022
Cloud TPU now supports Tensorflow 2.11.0. For more information see
[TensorFlow 2.11 release notes](https://github.com/tensorflow/tensorflow/releases/tag/v2.11.0).
September 13, 2022
Cloud TPU now supports Tensorflow 2.10.0. For more information see
[TensorFlow 2.10 release notes](https://github.com/tensorflow/tensorflow/releases/tag/v2.10.0).
June 24, 2022
Cloud TPU now supports
[TensorFlow 2.6.5](https://pypi.org/project/tensorflow/2.6.5/) and [TensorFlow 2.7.3](https://pypi.org/project/tensorflow/2.7.3/).
For more information see
[TensorFlow 2.6.5](https://github.com/tensorflow/tensorflow/releases/tag/v2.6.5) and [TensorFlow 2.7.3](https://github.com/tensorflow/tensorflow/releases/tag/v2.7.3) release notes.
May 27, 2022
Cloud TPU now supports Tensorflow 2.8.2 and 2.9.1. For more information see
[TensorFlow 2.8.2 release notes](https://github.com/tensorflow/tensorflow/releases/tag/v2.8.2) and [TensorFlow 2.9.1 release notes](https://github.com/tensorflow/tensorflow/releases/tag/v2.9.1).
March 18, 2022
Cloud TPU now supports Tensorflow 2.6.3. For more information see
[TensorFlow 2.6.3 release notes](https://github.com/tensorflow/tensorflow/releases/tag/v2.6.3).
March 09, 2022
Cloud TPU now supports Tensorflow 2.5.3 and 2.7.1. For more information see
[TensorFlow 2.5.3 release notes ](https://github.com/tensorflow/tensorflow/releases/tag/v2.5.3) and [TensorFlow 2.7.1 release notes](https://github.com/tensorflow/tensorflow/releases/tag/v2.7.1).
February 03, 2022
Cloud TPU now supports Tensorflow 2.8.0. For more information, see
[TensorFlow 2.8.0 Release Notes](https://github.com/tensorflow/tensorflow/releases/tag/v2.8.0).
December 02, 2021
Cloud TPU team just released TF-2.4.4, TF-2.5.2 and TF-2.6.2 on Cloud TPUs. The TensorFlow release notes for these releases are shown below.
November 05, 2021
Cloud TPU now supports Tensorflow 2.7.0. For more information, see
[Tensorflow 2.7.0 Release Notes](https://github.com/tensorflow/tensorflow/releases/tag/v2.7.0)
August 24, 2021
Cloud TPU team just released TF-2.3.4, TF-2.4.3 and TF-2.5.1 on Cloud TPUs. The TensorFlow release notes for these releases are shown below.
August 12, 2021
Cloud TPU now supports Tensorflow 2.6.0. For more information, see
[Tensorflow 2.6.0 Release Notes](https://github.com/tensorflow/tensorflow/releases/tag/v2.6.0).
In TF 2.6.0, TensorFlow has introduced a new version of the TF/XLA bridge using the MLIR compiler infrastructure. The MLIR bridge is enabled by default. To explicitly disable it at runtime, add the following code snippet to your model's code:
tf.config.experimental.disable_mlir_bridge()
June 22, 2021
The Cloud TPU team has released support for TensorFlow 2.4.2. The corresponding Tensorflow release notes are:
June 17, 2021
Cloud TPU team just released TF-2.1.4, TF-2.2.3 and TF-2.3.3 on Cloud TPUs. The TensorFlow release notes for these releases are shown below.
June 07, 2021
Cloud TPU now supports Tensorflow 2.5.0. For more information, see
[Tensorflow 2.5.0 Release Notes](https://github.com/tensorflow/tensorflow/releases/tag/v2.5.0)
June 01, 2021
New Cloud TPU VMs make training your ML models on TPUs easier than ever
The new Cloud TPU VM architecture makes it easier than ever before to use our industry-leading TPU hardware. The Cloud TPU VMs provide direct access to TPU host machines, offering a new and improved user experience for developing and deploying TensorFlow, PyTorch, and JAX on Cloud TPUs. Instead of accessing Cloud TPUs remotely over the network, Cloud TPU VMs let you set up your own interactive development environment on each TPU host machine. Now you can write and debug an ML model line-by-line using a single TPU VM, and then scale it up on a Cloud TPU Pod slice to take advantage of the super-fast TPU interconnects. You have root access to every TPU VM you create, so you can install and run any code you wish in a tight loop with your TPU accelerators. You can use local storage, execute custom code in your input pipelines, and more easily integrate Cloud TPUs into your research and production workflows. Google supports Cloud TPU integrations with TensorFlow, PyTorch, and JAX, and you can even write your own integrations via a new libtpu shared library on the VM. For more information, see https://cloud.google.com/blog/products/compute/introducing-cloud-tpu-vms.
December 14, 2020
Cloud TPU now supports Shared VPC
Shared VPC allows an organization to connect resources from multiple projects to a common VPC network to communicate with each other securely and efficiently using internal IPs from that network. This release enables connecting to Cloud TPU Nodes from Shared VPC networks.
September 11, 2020
Compute Engine TPU Metrics and Logs In-Context
New Monitoring tab for TPUs provides key TPU Metrics and access to logs at a glance. You can see a variety of key TPU metrics including MXU utilization, CPU, memory, sent/received traffic, and more. In addition, it provides instant access to TPU logs which give insight into important events.
August 20, 2020
PyTorch/XLA 1.6 Release (GA)
Highlights
Cloud TPUs now support the
[PyTorch 1.6 release](https://github.com/pytorch/pytorch/releases/tag/v1.6.0), via PyTorch/XLA integration. With this release we mark our general availability (GA) with the models such as ResNet, FairSeq Transformer and RoBERTa, and HuggingFace GLUE task models that have been rigorously tested and optimized.
In addition, with our PyTorch/XLA 1.6 release, you no longer need to run the
[env-setup.py](https://github.com/pytorch/xla/blob/master/contrib/scripts/env-setup.py) script on Colab/Kaggle as those are now compatible with native
torch wheels. You can still continue to use that script if you would like to run with our latest unstable releases.
New Features
- XLA RNG state checkpointing/loading (https://github.com/pytorch/xla/pull/2096)
- Device Memory XRT API (https://github.com/pytorch/xla/pull/2295)
- [Kaggle/Colab] Small host VM memory environment utility (https://github.com/pytorch/xla/pull/2025)
- [Advanced User] XLA Builder Support (https://github.com/pytorch/xla/pull/2125)
- New op supported on PyTorch/XLA
- Hardsigmoid (https://github.com/pytorch/xla/pull/1940)
- true_divide (https://github.com/pytorch/xla/pull/1782)
- max_unpool2d (https://github.com/pytorch/xla/pull/2188)
- max_unpool3d (https://github.com/pytorch/xla/pull/2188)
- Replication_pad1d (https://github.com/pytorch/xla/pull/2188)
- Replication_pad2d (https://github.com/pytorch/xla/pull/2188)
- Dynamic shape support on XLA:CPU and XLA:GPU (experimental)
Bug Fixes
- RNG Fix (proper dropout)
- Manual all-reduce in backward pass (https://github.com/pytorch/xla/pull/2325)
August 19, 2020
Cloud TPU now supports Shared VPC in Beta.
Shared VPC allows an organization to connect resources from multiple projects to a common VPC network to communicate with each other securely and efficiently using internal IPs from that network. This release enables connecting to Cloud TPU Nodes from Shared VPC networks.
May 29, 2020
Cloud TPU now supports TensorFlow version 1.15.3. See the
[TensorFlow 1.15.3 Release Notes](https://github.com/tensorflow/tensorflow/releases/tag/v1.15.3).
May 21, 2020
Cloud TPU now supports TensorFlow 2.1.1 with Keras support. See the
[TensorFlow 2.1.1 Release Notes](https://github.com/tensorflow/tensorflow/releases/tag/v2.1.1) for a complete list of features included in this release.
May 12, 2020
Cloud TPU currently supports TensorFlow version 1.15.2. See the
[Release Notes](https://github.com/tensorflow/tensorflow/releases/tag/v1.15.2).
TensorFlow 1.15 supported Python 2, but that support has been discontinued with TensorFlow 1.15.2.
May 08, 2020
Cloud TPU now supports TensorFlow 2.2. See the
[TensorFlow 2.2 Release Notes](https://github.com/tensorflow/tensorflow/releases/tag/v2.2.0) for a complete list of features included with this release. New models for Image segmentation and Image classification have been added to the official cloud [TPU supported models list](https://cloud.google.com/tpu/docs/tutorials/support-matrix).
April 21, 2020
Cloud TPUs and Cloud TPU Pods now support PyTorch 1.5 via the PyTorch/XLA integration. This integration makes it possible for PyTorch users to do everything they can do on GPUs on Cloud TPUs, while minimizing changes to the user experience. You can try out PyTorch on an 8-core Cloud TPU device for free via Google Colab, and you can use PyTorch on Cloud TPUs at a much larger scale on Google Cloud (all the way up to full Cloud TPU Pods).
See the
[PyTorch/XLA 1.5 Release Notes](https://github.com/pytorch/xla/releases/tag/v1.5.0) for a complete list of features included in this release.
January 09, 2020
Cloud TPU now supports TensorFlow 2.1 with Keras support. See the
[TensorFlow 2.1 Release Notes](https://github.com/tensorflow/tensorflow/releases/tag/v2.1.0) for a complete list of features included in this release.
December 05, 2019
Cloud TPU v2 and v3 Pods are now Generally Available for TensorFlow version 1.x. Supported models can be found
[here](https://cloud.google.com/tpu/docs/tutorials/supported-models).
Since TPU resources can scale from a single Cloud TPU to a Cloud TPU Pod, you don't need to choose between a single Cloud TPU and a Cloud TPU Pod. You can request portions of Cloud TPU Pods in slices or sets of cores, so that you purchase only the processing power you need.
Cloud TPU v2 and v3 Pod advantages over a single v2 or v3 Cloud TPU device:
- Increased training speeds for fast iteration in R&D *Increased human productivity by providing automatically scalable machine learning (ML) compute
- Ability to train much larger models
Cloud TPU v3 Pod advantages over Cloud TPU v2 Pod:
- Faster processing and larger memory:
- v2 Pod: 11.5 petaflops and 4 TB on-chip memory (HBM)
- v3 Pod: 100 petaflops and 32 TB HBM, with liquid cooling* Can train even larger models
October 22, 2019
Cloud TPU now supports TensorFlow version 1.15 (
[Release Notes](https://github.com/tensorflow/tensorflow/releases/tag/v1.15.0), [API Documentation](https://www.tensorflow.org/versions/r1.15/api_docs/)). See the current supported TensorFlow versions in the [Cloud TPU supported versions document](https://cloud.google.com/tpu/docs/supported-versions).
Cloud TPU support for TensorFlow 1.15 includes the following changes:
- The
[official Cloud TPU supported models list](https://cloud.google.com/tpu/docs/tutorials/support-matrix)has been updated.
July 29, 2019
Cloud TPU now supports
[TensorFlow version 1.14](https://www.tensorflow.org/versions/r1.14/api_docs/). Support for Tensorflow versions 1.11 is removed. See the current supported TensorFlow versions in the [Cloud TPU versioning policy](https://cloud.google.com/tpu/docs/supported-versions).
Cloud TPU support for TensorFlow 1.14 includes the following changes:
- Improved Error Messages: Cloud TPU errors in TensorFlow 1.14 are aggregated across multiple TPU cores and across multiple workers. This change makes error messages more comprehensible for user code.
- Redesigned object detection codebase: The
[object detection codebase](https://github.com/tensorflow/tpu/tree/master/models/official/detection)provides optimized training performance, clean and configurable parameter management, and advanced features such as spatial partition, [NAS-FPN](https://arxiv.org/abs/1904.07392)and [AutoAugment](https://arxiv.org/abs/1805.09501).
May 07, 2019
Cloud TPU v2 Pod is available in Beta release.
Since TPU resources can scale from a single Cloud TPU to a Cloud TPU Pod, you don't need to choose between a single Cloud TPU and a Cloud TPU Pod. You can request portions of Cloud TPU Pods in slices or sets of cores, so that you purchase only the processing power you need.
[Cloud TPU Pod (beta) advantages over a single Cloud TPU v2 device:](https://cloud.google.com/tpu/docs/deciding-pod-versus-tpu)
- Increased training speeds for fast iteration in R&D
- Increased human productivity by providing automatically scalable machine learning (ML) compute
- Ability to train much larger models
Cloud TPU v3 Pod is available in Beta release.
Since TPU resources can scale from a single Cloud TPU to a Cloud TPU Pod, you don't need to choose between a single Cloud TPU and a Cloud TPU Pod. You can request portions of Cloud TPU Pods in slices or sets of cores, so that you purchase only the processing power you need.
[Cloud TPU Pod (beta) advantages over a single v3 Cloud TPU device:](https://cloud.google.com/tpu/docs/deciding-pod-versus-tpu)
- Increased training speeds for fast iteration in R&D
- Increased human productivity by providing automatically scalable machine learning (ML) compute
- Ability to train much larger models
Cloud TPU v3 Pod (beta) advantages over Cloud TPU v2 Pod (beta):
- Faster processing and larger memory:
- v2 Pod: 11.5 petaflops and 4 TB on-chip memory (HBM)
- v3 Pod: 100 petaflops and 32 TB HBM, with liquid cooling* Can train even larger models
March 11, 2019
Cloud TPU now supports
[TensorFlow version 1.13](https://www.tensorflow.org/versions/r1.13/api_docs/). Support for Tensorflow versions 1.8 and 1.9 have been removed.
See the current supported TensorFlow versions in the
[Cloud TPU versioning policy](https://cloud.google.com/tpu/docs/supported-versions).
January 31, 2019
Cloud TPU v3 is now GA (generally available). Cloud TPU v3 has double the memory of v2. This gives improved performance and enables support for more classes of models, for example deeper ResNets and larger images with RetinaNet. Existing models that run on Cloud TPU v2 will continue to work. Refer to the
[Cloud TPU versions guide](https://cloud.google.com/tpu/docs/deciding-tpu-version) for more information.
November 08, 2018
Cloud TPU now supports
[TensorFlow version 1.12](https://www.tensorflow.org/versions/r1.12/api_docs/). This release includes improvements for Keras on Cloud TPUs, performance optimizations throughout the software stack, and improved APIs, error messages, and reliability.
See the current supported TensorFlow versions in the
[Cloud TPU versioning policy](https://cloud.google.com/tpu/docs/supported-versions).
November 07, 2018
Cloud TPU v2 Pod is available in Alpha release.
Since TPU resources can scale from a single Cloud TPU to a Cloud TPU Pod, you don't need to choose between a single Cloud TPU and a Cloud TPU Pod. You can request portions of Cloud TPU Pods in slices or sets of cores, so that you purchase only the processing power you need.
[Cloud TPU Pod (alpha) advantages:](https://cloud.google.com/tpu/docs/deciding-pod-versus-tpu)
- Increased training speeds for fast iteration in R&D
- Increased human productivity by providing automatically scalable machine learning (ML) compute
- Ability to train much larger models than on a single ML accelerator
October 10, 2018
Cloud TPU v3 is available in Beta release. You now have a choice between v2 and v3 in your configuration.
- Cloud TPU v3 has double the memory of v2. This gives improved performance and enables support for more classes of models, for example deeper ResNets and larger images with RetinaNet.
- Existing models that run on Cloud TPU v2 will continue to work.
- Refer to the
[Cloud TPU versions guide for more information.](https://cloud.google.com/tpu/docs/deciding-tpu-version)
Preemptible TPUs are now GA (generally available). A preemptible TPU is a Cloud TPU node that you can create and run at a much lower price than normal nodes. However, Cloud TPU may terminate (preempt) these nodes if it requires access to the resources for another purpose.
- See how to
[use a preemptible TPU](https://cloud.google.com/tpu/docs/preemptible).
- Review the
[pricing](https://cloud.google.com/tpu/docs/pricing)for preemptible and normal Cloud TPU nodes.
September 27, 2018
Cloud TPU now supports
[TensorFlow version 1.11](https://www.tensorflow.org/versions/r1.11/api_docs/). TensorFlow 1.11 introduces experimental support for all of the following on Cloud TPU: Keras, Colab, eager execution, LARS, RNNs, and [Mesh TensorFlow](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/mesh_tensorflow/README.md). This release also introduces a high-performance [Cloud Bigtable](https://cloud.google.com/bigtable/) integration, new XLA compiler optimizations, other performance optimizations throughout the software stack, and it provides improved APIs, error messages, and reliability.
See the current supported TensorFlow versions in the
[Cloud TPU versioning policy](https://cloud.google.com/tpu/docs/supported-versions).
September 07, 2018
Support for TensorFlow version 1.7 ended on September 7, 2018. See the current supported versions in the
[Cloud TPU versioning policy](https://cloud.google.com/tpu/docs/supported-versions).
July 24, 2018
We're delighted to announce promotional pricing for Cloud TPU, resulting in significant price reductions. The following table shows the previous pricing and the new pricing available from today:
US
|Previous price per TPU per hour
|New price per TPU per hour
|Cloud TPU
|$6.50 USD
|$4.50 USD
|Preemptible TPU
|$1.95 USD
|$1.35 USD
Europe
|Previous price per TPU per hour
|New price per TPU per hour
|Cloud TPU
|$7.15 USD
|$4.95 USD
|Preemptible TPU
|$2.15 USD
|$1.485 USD
Asia Pacific
|Previous price per TPU per hour
|New price per TPU per hour
|Cloud TPU
|$7.54 USD
|$5.22 USD
|Preemptible TPU
|$2.26 USD
|$1.566 USD
See the
[pricing guide](https://cloud.google.com/tpu/docs/pricing) for details.
July 12, 2018
Cloud TPU is now available in Google Kubernetes Engine as a Beta feature. Run your machine learning workload in a Kubernetes cluster on Google Cloud, and let GKE manage and scale the Cloud TPU resources for you.
- Follow the
[tutorial](https://cloud.google.com/tpu/docs/tutorials/kubernetes-engine-resnet)to train the Tensorflow ResNet-50 model on Cloud TPU and GKE.
- Refer to the
[GKE setup guide](https://cloud.google.com/tpu/docs/kubernetes-engine-setup)for quick instructions on running Cloud TPU with GKE.
July 02, 2018
Cloud TPU now supports
[TensorFlow version 1.9](https://www.tensorflow.org/versions/r1.9/api_docs/). TensorFlow 1.9 brings increases in Cloud TPU performance as well as improved APIs, error messages, and reliability.
June 27, 2018
Cloud TPU is now GA (generally available). Google's revolutionary TPUs are designed to accelerate machine learning workloads with TensorFlow. Each Cloud TPU provides up to 180 teraflops of performance, providing the computational power to train and run cutting-edge machine learning models.
- Follow the
[quickstart guide](https://cloud.google.com/tpu/docs/quickstart)to set up your Cloud TPU.
- Choose a
[tutorial](https://cloud.google.com/tpu/docs/tutorials)to run a specific model on your Cloud TPU.
June 18, 2018
Preemptible TPUs are now available in Beta. A preemptible TPU is a Cloud TPU node that you can create and run at a much lower price than normal nodes. However, Cloud TPU may terminate (preempt) these nodes if it requires access to the resources for another purpose.
- See how to
[use a preemptible TPU](https://cloud.google.com/tpu/docs/preemptible).
- Review the
[pricing](https://cloud.google.com/tpu/docs/pricing)for preemptible and normal Cloud TPU nodes.
Cloud TPU is now available in the European (EU) and Asia Pacific (APAC) regions as well as the United States (US). See the the
[pricing details](https://cloud.google.com/tpu/docs/pricing) per region. The following zones are available:
- US
us-central1-b
us-central1-c
us-central1-f(
[TFRC program](https://www.tensorflow.org/tfrc/)only)
-
- EU
europe-west4-a
-
- APAC
asia-east1-c
-
June 12, 2018
Support for TensorFlow version 1.6 ended on June 12, 2018. See the current supported versions in the
[Cloud TPU versioning policy](https://cloud.google.com/tpu/docs/supported-versions).
April 20, 2018
Cloud TPU now supports
[TensorFlow version 1.8](https://www.tensorflow.org/versions/r1.8/api_docs/). TensorFlow 1.8 brings increases in Cloud TPU performance as well as improved APIs, error messages, and reliability.
Support for TensorFlow version 1.7 ends on June 20, 2018. See the details in the
[Cloud TPU versioning policy](https://cloud.google.com/tpu/docs/supported-versions).
April 02, 2018
Cloud TPU now supports
[TensorFlow version 1.7](https://www.tensorflow.org/versions/r1.7/api_docs/). Support for TensorFlow version 1.6 ends on June 2, 2018. See the details in the [Cloud TPU versioning policy](https://cloud.google.com/tpu/docs/supported-versions).
February 12, 2018
Cloud TPU is available in Beta release. Google's revolutionary TPUs are designed to accelerate machine learning workloads with TensorFlow. Each Cloud TPU provides up to 180 teraflops of performance, providing the computational power to train and run cutting-edge machine learning models.
- See how to
[request TPU quota](https://cloud.google.com/tpu/docs/quota).
- Follow the
[quickstart guide](https://cloud.google.com/tpu/docs/quickstart)to set up your Cloud TPU.
- Choose a
[tutorial](https://cloud.google.com/tpu/docs/tutorials)to run a specific model on your Cloud TPU.",Cloud TPU release notes | Google Cloud,
id,url,body,title,description
147,https://cloud.google.com/tpu/docs/reference/rest/v2alpha1/projects.locations.nodes/getGuestAttributes,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [GuestAttributes](#GuestAttributes) [GuestAttributesValue](#GuestAttributesValue) [GuestAttributesEntry](#GuestAttributesEntry) [Try it!](#try-it)
Retrieves the guest attributes for the node.
HTTP request
POST https://tpu.googleapis.com/v2alpha1/{name=projects/*/locations/*/nodes/*}:getGuestAttributes
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
Required. The resource name.
Request body
The request body contains data with the following structure:
|JSON representation
|
{ ""queryPath"": string, ""workerIds"": [ string ] }
|Fields
|
queryPath
|
The guest attributes path to be queried.
|
workerIds[]
|
The 0-based worker ID. If it is empty, all workers' GuestAttributes will be returned.
Response body
Response for
.
[nodes.getGuestAttributes](/tpu/docs/reference/rest/v2alpha1/projects.locations.nodes/getGuestAttributes#google.cloud.tpu.v2alpha1.Tpu.GetGuestAttributes)
If successful, the response body contains data with the following structure:
|JSON representation
|
{
""guestAttributes"": [
{
object (
|Fields
|
guestAttributes[]
|
The guest attributes for the TPU workers.
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
GuestAttributes
A guest attributes.
|JSON representation
|
{
""queryPath"": string,
""queryValue"": {
object (
|Fields
|
queryPath
|
The path to be queried. This can be the default namespace ('/') or a nested namespace ('/<namespace>/') or a specified key ('/<namespace>/<key>')
|
queryValue
|
The value of the requested queried path.
GuestAttributesValue
Array of guest attribute namespace/key/value tuples.
|JSON representation
|
{
""items"": [
{
object (
|Fields
|
items[]
|
The list of guest attributes entries.
GuestAttributesEntry
A guest attributes namespace/key/value entry.
|JSON representation
|
{ ""namespace"": string, ""key"": string, ""value"": string }
|Fields
|
namespace
|
Namespace for the guest attribute entry.
|
key
|
Key for the guest attribute entry.
|
value
|
Value for the guest attribute entry.",Method: projects.locations.nodes.getGuestAttributes | Cloud TPU | Google Cloud,
id,url,body,title,description
72,https://cloud.google.com/tpu/docs/reference/rest/v1/projects.locations,"Resource: Location
A resource that represents a Google Cloud location.
|JSON representation
|
{ ""name"": string, ""locationId"": string, ""displayName"": string, ""labels"": { string: string, ... }, ""metadata"": { ""@type"": string, field1: ..., ... } }
|Fields
|
name
|
Resource name for the location, which may vary between implementations. For example:
|
locationId
|
The canonical id for this location. For example:
|
displayName
|
The friendly name for this location, typically a nearby city name. For example, ""Tokyo"".
|
labels
|
Cross-service attributes for the location. For example
An object containing a list of
|
metadata
|
Service-specific metadata. For example the available capacity at the given location.
An object containing fields of an arbitrary type. An additional field
|
Methods
|
|Gets information about a location.
|
|Lists information about the supported locations for this service.",REST Resource: projects.locations | Cloud TPU | Google Cloud,
id,url,body,title,description
119,https://cloud.google.com/tpu/docs/troubleshooting/error_glossary,"Cloud TPU Error Glossary
This document provides a glossary of common errors with solutions from the Cloud TPU service.
Invalid Accelerator Type
Error Message
generic::invalid_argument: Accelerator type v2-512 as preemptible (false) and reserved (false) is not available in zone us-central1-a, please contact support.
Solution
An invalid parameter has been provided to the create command. The availability of an accelerator in a zone depends on 3 parameters: the type, the preemptible flag, and the reserved flag. The preemptible and reserved flags can be changed by including/excluding them in the create command.
A TPU created with the reserved flag will use reserved
capacity. Including the preemptible flag will allow the TPU to be preempted by
higher priority TPUs. If neither is provided, the TPU will be on demand. It is
not a valid configuration to enable both flags. See the
[create command documentation](https://cloud.google.com/sdk/gcloud/reference/alpha/compute/tpus/create) for more information.
The accelerator types available in each zone can be found in the
[TPU regions and zones documentation](https://cloud.google.com/tpu/docs/regions-zones) or they can be
queried using the [accelerator-types list command](https://cloud.google.com/sdk/gcloud/reference/alpha/compute/tpus/accelerator-types/list). Change the create command to use one
of these accelerator types and try again or contact support if the problem persists.
Network Not Found
Error Message
Cloud TPU received a bad request. The field ""Network"" cannot be ""xxxx"": requested resource not found
Solution
The Network xxxx was not found. Ensure that the Network was created
and set up properly. See
[https://cloud.google.com/vpc/docs/create-modify-vpc-networks](https://cloud.google.com/vpc/docs/create-modify-vpc-networks) for more information.
Service Account Permission Denied
Error Message
generic::permission_denied: Cloud TPU got permissions denied when trying to access the customer project. Make sure that the IAM account 'service-[project number]@cloud-tpu.iam.gserviceaccount.com' has the 'Cloud TPU API Service Agent' role by following https://cloud.google.com/iam/docs/manage-access-service-accounts
Solution
This error occurs when a user attempts to create or list nodes in a project
without IAM authorization. A likely cause of this issue is that the Cloud TPU API service
account does not have the required role for the project. The
[Manage access accounts documentation](https://cloud.google.com/iam/docs/manage-access-service-accounts)
gives an overview of how to manage access. Follow the [Grant or revoke a single role](https://cloud.google.com/iam/docs/manage-access-service-accounts#single-role) steps
and give the account 'service-
PROJECT_NUMBER@cloud-tpu.iam.gserviceaccount.com' the role of 'Cloud TPU API Service Agent' (be sure to replace
PROJECT_NUMBER
with your project number, which can be found in the project settings in the Google Cloud console). For more
information on service agents, see the
[Service agents documentation](https://cloud.google.com/iam/docs/service-agents).
Quota Exceeded
Error Message
You have reached XXXX limit. Please request an increase for the 'YYYY' quota for Compute Engine API by following https://cloud.google.com/docs/quota#requesting_higher_quota.
Solution
Your project has reached a quota limit. To learn more about working with quotas, see
[https://cloud.google.com/docs/quota](https://cloud.google.com/docs/quota). This should not be confused with the [TPU quota](https://cloud.google.com/tpu/docs/quota), which governs the usage of TPU pods.
You may request an increase to the appropriate limit by following the steps listed at
[https://cloud.google.com/docs/quota_detail/view_manage#requesting_higher_quota](https://cloud.google.com/docs/quota_detail/view_manage#requesting_higher_quota). On the quotas page, you may search for the quota specified by the 'YYYY' part of this message. Some quotas are split across different regions or services. The error message will indicate which one needs to be increased.
The 'XXXX' and 'YYYY' parts of the message may be one of the following: * HEALTH_CHECKS - 'Health checks' quota * FIREWALLS - 'Firewall rules' quota * NETWORK_ENDPOINT_GROUPS - 'Network endpoint groups' quota for this region * READ_REQUESTS - 'Read requests per minute' quota for the Compute Engine API service * OPERATION_READ_REQUESTS - 'Operation read requests per minute' quota
This request is typically processed within 2-3 business days. If the request is urgent, please reach out to a customer engineer or technical account manager.",Cloud TPU Error Glossary | Google Cloud,
id,url,body,title,description
45,https://cloud.google.com/tpu/docs/tutorials/LLM/single-device-pax,"This document provides a brief introduction to working with Pax on a single-host TPU (v2-8, v3-8, v4-8).
[Pax](https://github.com/google/paxml) is a framework to configure
and run machine learning experiments on top of JAX. Pax focuses
on simplifying ML at scale by sharing infrastructure components
with existing ML frameworks and utilizing the
[Praxis](https://github.com/google/praxis) modeling library for modularity.
Objectives
- Set up TPU resources for training
- Install Pax on a single-host TPU
- Train a transformer based SPMD model using Pax
Before you begin
Run the following commands to configure
gcloud to use
your Cloud TPU project and install components needed to train
a model running Pax on a single-host TPU.
Install the Google Cloud CLI
The Google Cloud CLI contains tools and libraries for interacting
with Google Cloud CLI products and services. If you haven't installed
it previously, install it now using the instructions in
[Installing
the Google Cloud CLI](https://cloud.google.com/sdk/docs/install).
Configure the
gcloud command
(Run
gcloud auth list to see your currently available accounts).
$ gcloud config set account account
$ gcloud config set project project-id
Enable the Cloud TPU API
Enable the Cloud TPU API using the following
gcloud command
in
[Cloud Shell](https://console.cloud.google.com/?cloudshell=true).
(You may also enable it from the
[Google Cloud console](https://console.cloud.google.com/)).
$ gcloud services enable tpu.googleapis.com
Run the following command to create a service identity (a service account).
$ gcloud beta services identity create --service tpu.googleapis.com
Create a TPU VM
With Cloud TPU VMs, your model and code run directly on the TPU VM. You SSH directly into the TPU VM. You can run arbitrary code, install packages, view logs, and debug code directly on the TPU VM.
Create your TPU VM by running the following command from a
Cloud Shell or your computer terminal where the
[Google Cloud CLI](https://cloud.google.com/sdk/docs/install) is installed.
Set the
zone based on availability in your contract,
reference
[TPU Regions and Zones](https://cloud.google.com/tpu/docs/regions-zones)
if needed.
Set the
accelerator-type variable to v2-8, v3-8, or v4-8.
Set the
version variable to
tpu-vm-base
for v2 and v3 TPU versions or
tpu-vm-v4-base for v4 TPUs.
$ gcloud compute tpus tpu-vm create tpu-name \
--zone zone \
--accelerator-type accelerator-type \
--version version
Connect to your Google Cloud TPU VM
SSH into your TPU VM by using the following command:
$ gcloud compute tpus tpu-vm ssh tpu-name --zone zone
When you are logged into the VM, your shell prompt changes from
username@projectname to
username@vm-name:
Install Pax on the Google Cloud TPU VM
Install Pax, JAX and
libtpu on your TPU VM using the following commands:
(vm)$ python3 -m pip install -U pip \
python3 -m pip install paxml jax[tpu]
-f https://storage.googleapis.com/jax-releases/libtpu_releases.html
System check
Test that everything is installed correctly by checking that JAX sees the TPU cores:
(vm)$ python3 -c ""import jax; print(jax.device_count())""
The number of TPU cores is displayed, this should be 8 if you are using a v2-8 or v3-8, or 4 if you are using a v4-8.
Running Pax code on a TPU VM
You can now run any Pax code you wish. The lm_cloud
[examples](https://github.com/google/paxml/blob/main/paxml/tasks/lm/params/lm_cloud.py)
are a great place to start running models in Pax. For example, the
following commands train a
[2B parameter](https://github.com/google/paxml/blob/main/paxml/tasks/lm/params/lm_cloud.py#L155) transformer based SPMD language model on synthetic data.
The following commands show training output for a SPMD language model. It trains for 300 step in approximately 20 minutes.
(vm)$ python3 .local/lib/python3.8/site-packages/paxml/main.py --exp=tasks.lm.params.lm_cloud.LmCloudSpmd2BLimitSteps --job_log_dir=job_log_dir
On v4-8 slice, the output should include:
Losses and step times
summary tensor at step=step_#
loss = loss
summary tensor at step=step_# Steps/sec x
Clean up
To avoid incurring charges to your Google Cloud account for the resources used in this tutorial, either delete the project that contains the resources, or keep the project and delete the individual resources.
When you are done with your TPU VM follow these steps to clean up your resources.
Disconnect from the Compute Engine instance, if you have not already done so:
(vm)$ exit
Delete your Cloud TPU.
$ gcloud compute tpus tpu-vm delete tpu-name --zone zone
What's next
For more information about Cloud TPU, see:",Train on a single-host TPU using Pax | Google Cloud,
id,url,body,title,description
121,https://cloud.google.com/tpu/docs/reference/rest/v1/projects.locations.nodes/start,"Method: projects.locations.nodes.start
Stay organized with collections
Save and categorize content based on your preferences.
HTTP request
POST https://tpu.googleapis.com/v1/{name=projects/*/locations/*/nodes/*}:start
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
string
The resource name.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[Operation](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse#Operation)
Authorization scopes
Requires one of the following OAuth scopes:
-
https://www.googleapis.com/auth/cloud.tpu
-
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Method: projects.locations.nodes.start | Cloud TPU | Google Cloud,
id,url,body,title,description
141,https://cloud.google.com/tpu/docs/reference/rest/v1alpha1/projects.locations.operations/get,"Method: projects.locations.operations.get
Stay organized with collections
Save and categorize content based on your preferences.
Gets the latest state of a long-running operation. Clients can use this method to poll the operation result at intervals as recommended by the API service.
HTTP request
GET https://tpu.googleapis.com/v1alpha1/{name=projects/*/locations/*/operations/*}
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
string
The name of the operation resource.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[Operation](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse#Operation)
Authorization scopes
Requires one of the following OAuth scopes:
-
https://www.googleapis.com/auth/cloud.tpu
-
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Method: projects.locations.operations.get | Cloud TPU | Google Cloud,
id,url,body,title,description
133,https://cloud.google.com/tpu/docs/tutorials/bert-2.x,"This tutorial shows you how to train the Bidirectional Encoder Representations from Transformers (BERT) model on Cloud TPU.
BERT is a method of pre-training language representations. Pre-training refers
to how BERT is first trained on a large source of text, such as Wikipedia. You can
then apply the training results to other Natural Language Processing (NLP) tasks, such as
[question answering](https://en.wikipedia.org/wiki/Question_answering) and
[sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis). With
BERT and Cloud TPU, you can train a variety of NLP models
in about 30 minutes.
For more information about BERT, see the following resources:
[Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html) [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
Objectives
- Create a Cloud Storage bucket to hold your dataset and model output.
- Run the training job.
- Verify the output results.
Costs
In this document, you use the following billable components of Google Cloud:
- Compute Engine
- Cloud TPU
- Cloud Storage
To generate a cost estimate based on your projected usage,
use the
[pricing calculator](/products/calculator).
[free trial](/free-trial).
Before you begin
This section provides information on setting up Cloud Storage bucket and a Compute Engine VM.
Open a Cloud Shell window.
Create a variable for your project's ID.
export PROJECT_ID=project-id
Configure Google Cloud CLI to use the project where you want to create Cloud TPU.
gcloud config set project ${PROJECT_ID}
Create a Service Account for the Cloud TPU project.
gcloud beta services identity create --service tpu.googleapis.com --project $PROJECT_ID
The command returns a Cloud TPU Service Account with following format:
service-PROJECT_NUMBER@cloud-tpu.iam.gserviceaccount.com
Create a Cloud Storage bucket using the following command:
gsutil mb -p ${PROJECT_ID} -c standard -l us-central1 gs://bucket-name
This Cloud Storage bucket stores the data you use to train your model and the training results. The command you use to create a TPU (
gcloud compute tpus execution-groups createfor the TPU Node architecture or
gcloud compute tpus tpu-vm createfor the TPU VM architecture) sets up default permissions for the Cloud TPU Service Account you set up in the previous step. If you want finer-grain permissions, review the
[access level permissions](/tpu/docs/storage-buckets).
The bucket location must be in the same region as your Compute Engine (VM) and your Cloud TPU node.
Launch a Compute Engine VM and Cloud TPU using the
gcloudcommand. The command you use depends on whether you are using a TPU VM or a TPU node. For more information on the two VM architecture, see
[System Architecture](/tpu/docs/system-architecture-tpu-vm). For more information on the
gcloudcommand, see the
[gcloud Reference](/sdk/gcloud/reference).
TPU VM
$ gcloud compute tpus tpu-vm create bert-tutorial \ --zone=us-central1-b \ --accelerator-type=v3-8 \ --version=tpu-vm-tf-2.15.0-se
Command flag descriptions
zone
- The
[zone](/tpu/docs/types-zones)where you plan to create your Cloud TPU.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
version
- The Cloud TPU
[software version](/tpu/docs/supported-tpu-versions#tpu_software_versions).
TPU Node
$ gcloud compute tpus execution-groups create \ --name=bert-tutorial \ --zone=us-central1-b \ --tf-version=2.12.0 \ --machine-type=n1-standard-1 \ --accelerator-type=v3-8
Command flag descriptions
name
- The name of the Cloud TPU to create.
zone
- The
[zone](/tpu/docs/types-zones)where you plan to create your Cloud TPU.
tf-version
- The version of Tensorflow
ctpuinstalls on the VM.
machine-type
- The
[machine type](/compute/docs/machine-types)of the Compute Engine VM to create.
accelerator type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
-
If you are not automatically logged in to the Compute Engine instance, log in by running the following
sshcommand. When you are logged into the VM, your shell prompt changes from
username@projectnameto
username@vm-name:
TPU VM
gcloud compute tpus tpu-vm ssh bert-tutorial --zone=us-central1-b
TPU Node
gcloud compute ssh bert-tutorial --zone=us-central1-b
As you continue these instructions, run each command that begins with
(vm)$in your VM session window.
Create an environment variable for the TPU name.
TPU VM
(vm)$ export TPU_NAME=local
TPU Node
(vm)$ export TPU_NAME=bert-tutorial
Prepare the dataset
Define the storage bucket needed to store the model and the dataset:
(vm)$ export STORAGE_BUCKET=gs://bucket-name
Copy the pretrained checkpoint and vocab files to your storage bucket:
(vm)$ curl https://storage.googleapis.com/tf_model_garden/nlp/bert/v3/uncased_L-12_H-768_A-12.tar.gz -o uncased_L-12_H-768_A-12.tar.gz (vm)$ mkdir -p uncased_L-12_H-768_A-12 (vm)$ tar -xvf uncased_L-12_H-768_A-12.tar.gz (vm)$ gsutil -m cp -R uncased_L-12_H-768_A-12 ${STORAGE_BUCKET}
Train the model
Define several parameter values that are required when you train and evaluate the model:
(vm)$ export INIT_CHECKPOINT=${STORAGE_BUCKET}/uncased_L-12_H-768_A-12/bert_model.ckpt (vm)$ export TFDS_DIR=${STORAGE_BUCKET}/tfds (vm)$ export VOCAB_FILE=${STORAGE_BUCKET}/uncased_L-12_H-768_A-12/vocab.txt (vm)$ export MODEL_DIR=${STORAGE_BUCKET}/bert-output (vm)$ export TASK=mnli
Install TensorFlow requirements.
The command you use depends on whether you are using a TPU VM or a TPU Node.
TPU VM
(vm)$ pip3 install -r /usr/share/tpu/models/official/requirements.txt
TPU Node
(vm)$ pip3 install --user -r /usr/share/models/official/requirements.txt (vm)$ pip3 install tensorflow-datasets==4.6.0
Set the
PYTHONPATHenvironment variable
TPU VM
(vm)$ export PYTHONPATH=/usr/share/tpu/models
TPU Node
(vm)$ export PYTHONPATH=""${PYTHONPATH}:/usr/share/models""
Change to directory that stores the model:
TPU VM
(vm)$ cd /usr/share/tpu/models
TPU Node
(vm)$ cd /usr/share/models
Run the training script:
(vm)$ python3 official/nlp/train.py \ --tpu=${TPU_NAME} \ --experiment=bert/sentence_prediction_text \ --mode=train_and_eval \ --model_dir=${MODEL_DIR} \ --config_file=official/nlp/configs/experiments/glue_mnli_text.yaml \ --params_override=""runtime.distribution_strategy=tpu, task.init_checkpoint=${INIT_CHECKPOINT}, task.train_data.tfds_data_dir=${TFDS_DIR}, task.train_data.vocab_file=${VOCAB_FILE}, task.validation_data.tfds_data_dir=${TFDS_DIR}, task.validation_data.vocab_file=${VOCAB_FILE}, trainer.train_steps=2000""
Command flag descriptions
tpu
- The name of the Cloud TPU to use for training.
mode
- One of
train,
eval,
train_and_eval, or
predict.
model_dir
- The Cloud Storage path where checkpoints and summaries are stored during model training. You can reuse an existing folder to load previously generated checkpoints and to store additional checkpoints as long as the previous checkpoints were created using a Cloud TPU of the same size and TensorFlow version.
The script trains for 2000 steps and then runs 307 steps of evaluation. On a v3-8 TPU, after approximately 5 minutes the training script should complete and display results similar to this:
I0719 00:47:52.683979 140297079573568 controller.py:457] train | step: 2000 | steps/sec: 26.3 | output: {'cls_accuracy': 0.7249375, 'learning_rate': 1.4670059e-05, 'training_loss': 0.6740678} train | step: 2000 | steps/sec: 26.3 | output: {'cls_accuracy': 0.7249375, 'learning_rate': 1.4670059e-05, 'training_loss': 0.6740678} I0719 00:47:53.184051 140297079573568 controller.py:277] eval | step: 2000 | running 307 steps of evaluation... eval | step: 2000 | running 307 steps of evaluation...
-
Clean up
Disconnect from the Compute Engine instance, if you have not already done so:
(vm)$ exit
Your prompt should now be
username@projectname, showing you are in the Cloud Shell.
Delete your Cloud TPU and Compute Engine resources. The command you use to delete your resources depends upon whether you are using TPU VMs or TPU Nodes. For more information, see
[System Architecture](/tpu/docs/system-architecture-tpu-vm).
TPU VM
$ gcloud compute tpus tpu-vm delete bert-tutorial \ --zone=us-central1-b
TPU Node
$ gcloud compute tpus execution-groups delete bert-tutorial \ --zone=us-central1-b
Verify the resources have been deleted by running
gcloud compute tpus execution-groups list. The deletion might take several minutes. The output from the following command should not include any of the resources created in this tutorial:
TPU VM
$ gcloud compute tpus tpu-vm list --zone=us-central1-b
TPU Node
$ gcloud compute tpus execution-groups list --zone=us-central1-b
Delete your Cloud Storage bucket using
gsutilas shown below. Replace bucket-name with the name of your Cloud Storage bucket.
$ gsutil rm -r gs://bucket-name
What's next
The TensorFlow Cloud TPU tutorials generally train the model using a
sample dataset. The results of this training are not usable
for inference. To use a model for inference, you can train the data on a
publicly available dataset or your own data set. TensorFlow models
trained on Cloud TPUs generally require datasets to be in
[TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) format.
You can use the
[dataset conversion tool sample](https://cloud.google.com/tpu/docs/classification-data-conversion) to convert an image classification dataset into TFRecord format. If you are not using an image classification model, you will have to convert your dataset to [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) format yourself. For more information, see [TFRecord and tf.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord).
Hyperparameter tuning
To improve the model's performance with your dataset, you can tune
the model's hyperparameters. You can find information about hyperparameters
common to all TPU supported models on
[GitHub](https://github.com/tensorflow/tpu/tree/master/models/hyperparameters).
Information about model-specific hyperparameters can be found in the
[source code](https://github.com/tensorflow/tpu/tree/master/models/official)
for each model. For more information on hyperparameter tuning, see
[Overview of hyperparameter tuning](https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview),
[Using the Hyperparameter tuning service](https://cloud.google.com/ai-platform/training/docs/using-hyperparameter-tuning),
and [Tune hyperparameters](https://developers.google.com/machine-learning/guides/text-classification/step-5).
Inference
Once you have trained your model you can use it for inference
(also called prediction).
[AI Platform](https://cloud.google.com/ai-platform/docs/technical-overview)
is a cloud-based solution for developing,
[training](https://cloud.google.com/ai-platform/training/docs),
and [deploying](https://cloud.google.com/ai-platform/prediction/docs/deploying-models)
machine learning models. Once a model is deployed, you can use the
[AI Platform Prediction service](https://cloud.google.com/ai-platform/prediction/docs).
- Explore the
[TPU tools in TensorBoard](/tpu/docs/cloud-tpu-tools).",BERT Fine Tuning with Cloud TPU: Sentence and Sentence-Pair Classification Tasks (TF 2.x) | Google Cloud,
id,url,body,title,description
187,https://cloud.google.com/tpu/docs/reference/rest/v2alpha1/projects.locations.queuedResources/get,"Method: projects.locations.queuedResources.get
Stay organized with collections
Save and categorize content based on your preferences.
Gets details of a queued resource.
HTTP request
GET https://tpu.googleapis.com/v2alpha1/{name=projects/*/locations/*/queuedResources/*}
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
string
Required. The resource name.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[QueuedResource](/tpu/docs/reference/rest/v2alpha1/projects.locations.queuedResources#QueuedResource)
Authorization scopes
Requires one of the following OAuth scopes:
-
https://www.googleapis.com/auth/cloud.tpu
-
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Method: projects.locations.queuedResources.get | Cloud TPU | Google Cloud,
id,url,body,title,description
107,https://cloud.google.com/tpu/docs/reference/rest/v2/projects.locations.operations/delete,"Send feedback
Method: projects.locations.operations.delete
Stay organized with collections
Save and categorize content based on your preferences.
Deletes a long-running operation. This method indicates that the client is no longer interested in the operation result. It does not cancel the operation. If the server doesn't support this method, it returns
google.rpc.Code.UNIMPLEMENTED.
HTTP request
DELETE https://tpu.googleapis.com/v2/{name=projects/*/locations/*/operations/*}
The URL uses
gRPC Transcoding syntax.
Path parameters
Parameters
name
string
The name of the operation resource to be deleted.
Request body
The request body must be empty.
Response body
If successful, the response body is empty.
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
Authentication Overview.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]
Need to tell us more?",Method: projects.locations.operations.delete | Cloud TPU | Google Cloud,
id,url,body,title,description
93,https://cloud.google.com/tpu/docs/reference/rest/v2alpha1/projects.locations.acceleratorTypes,"REST Resource: projects.locations.acceleratorTypes
Stay organized with collections
Save and categorize content based on your preferences.
Resource: AcceleratorType
A accelerator type that a Node can be configured with.
|JSON representation
|
{
""name"": string,
""type"": string,
""acceleratorConfigs"": [
{
object (
)
}
]
}
[AcceleratorConfig](/tpu/docs/reference/rest/v2alpha1/AcceleratorConfig)
|Fields
|
name
|
string
The resource name.
|
type
|
string
The accelerator type.
|
acceleratorConfigs[]
|
object (
)
[AcceleratorConfig](/tpu/docs/reference/rest/v2alpha1/AcceleratorConfig)
The accelerator config.
|
Methods
|
|
Gets AcceleratorType.
|
|
Lists accelerator types supported by this API.
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",REST Resource: projects.locations.acceleratorTypes | Cloud TPU | Google Cloud,
id,url,body,title,description
84,https://cloud.google.com/tpu/docs/reference/rest/v1/projects.locations.operations/get,"Method: projects.locations.operations.get
Stay organized with collections
Save and categorize content based on your preferences.
Gets the latest state of a long-running operation. Clients can use this method to poll the operation result at intervals as recommended by the API service.
HTTP request
GET https://tpu.googleapis.com/v1/{name=projects/*/locations/*/operations/*}
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
string
The name of the operation resource.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[Operation](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse#Operation)
Authorization scopes
Requires one of the following OAuth scopes:
-
https://www.googleapis.com/auth/cloud.tpu
-
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Method: projects.locations.operations.get | Cloud TPU | Google Cloud,
id,url,body,title,description
60,https://cloud.google.com/tpu/docs/training-on-tpu-pods,"Training on TPU Pods
Overview
TPUs were designed to be scaled out to a TPU Pod. A TPU Pod is a collection of TPU devices connected by dedicated high-speed network interfaces. A TPU Pod allows you to distribute the processing load across multiple TPUs. Each TPU board is connected to a high-performance CPU-based host machine for things like loading and preprocessing data. To take full advantage of larger numbers of TPUs, you must tune several training task parameters.
The setup for training with TPU Pods is different for each framework. Use the following links to see detailed information about training on Pods with each framework:
The following sections explain some common issues, changes you need to make in your models, and best practices to reduce or avoid Pod failures.
Scaling batch size and train steps
To achieve linear scaling on larger TPU types, keep the per-core batch size the same.
For example if you use a batch size of 1024 on a v2-8, use a batch size of 4096 (4 * 1024) on a v2-32. This fully utilizes the TPU hardware. You can use smaller batch sizes, but your training will not scale linearly if you do so.
Some models include a
train_steps flag where one step corresponds to
processing a single batch of data. When you increase the batch size, scale down
the number of training steps so that the total number of training examples
remains the same.
For example, if you have a batch size of 1000 for 100 steps,
100,000 examples are processed during training. If you now have 4 workers and an
effective batch size of 4000, you would have to adjust the number of steps to 25
to process that same 100,000 examples. If your model uses an
epochs flag, you
do not need to scale the number of steps.
Larger batch sizes can change convergence behavior of the model, so you might also tune some hyperparameters, like learning rate.
Using regional Google Cloud Storage buckets in the same region as the TPU Pod
In general, the best practice for TPU training is to always use resources in the same region. Resource region is particularly important when using TPU Pods because the data transfer rates are higher when your Google Cloud Storage bucket and TPU are in the same region.
Ensure you are using a regional Google Cloud Storage bucket in the same region as the TPU for training datasets and checkpoints.
Workflow best practices for development on TPU Pods
When developing a new TPU workload, it is often optimal to begin development on the smallest TPUs and progressively iterate to larger TPU sizes. Start by using a small TPU version (for example, v2-8 or v3-8).
- Test your workload for functionality
- Test and validate performance using the performance tools
Once your workload is functional and reaches your performance targets, scale up to a larger TPU type such as a v2-32 or v3-32. Gradually and iteratively increase the TPU size while validating scalability (functionality and performance) until you reach the desired TPU size.",Training on TPU Pods | Google Cloud,
id,url,body,title,description
175,https://cloud.google.com/tpu/docs/pytorch-xla-performance-profiling-tpu-vm,"PyTorch XLA performance profiling
Profiling is a way to analyze and improve the performance of models. Although there is much more to it, sometimes it helps to think of profiling as timing operations and parts of the code that run on both devices (TPUs) and hosts (CPUs). This guide provides a quick overview of how to profile your code for training or inference. For more information on how to analyze generated profiles, please refer to the following guides.
[PyTorch XLA performance debugging on TPU VMs - part 1](/blog/topics/developers-practitioners/pytorchxla-performance-debugging-tpu-vm-part-1) [PyTorch XLA performance debugging on TPU VMs - part 2](/blog/topics/developers-practitioners/pytorchxla-performance-debugging-cloud-tpu-vm-part-ii) [PyTorch XLA performance debugging on TPU VMs - part 3](/blog/topics/developers-practitioners/pytorchxla-performance-debugging-cloud-tpu-vm-part-iii)
Get Started
Create a TPU
Export environment variables:
$ export TPU_NAME=your_tpu_name $ export ZONE=us-central2-b $ export PROJECT_ID=project-id $ export ACCELERATOR_TYPE=v4-8 $ export RUNTIME_VERSION=tpu-vm-v4-pt-2.0
Export variable descriptions
TPU name
- The name you want to use for your Cloud TPU.
zone
- The
[zone](/tpu/docs/types-zones)where you plan to create your Cloud TPU.
project ID
- The project ID you are using to train and profile your model.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
version
- The Cloud TPU runtime version. A default is shown in the
export variable, but you can also use one from the list of
[supported configurations](/tpu/docs/supported-tpu-configurations).
-
Launch the TPU resources
$ gcloud compute tpus tpu-vm create ${TPU_NAME} \ --zone us-central2-b \ --accelerator-type ${ACCELERATOR_TYPE} \ --version ${RUNTIME_VERSION} \ --project $PROJECT_ID \ --subnetwork=tpusubnet
Move your code to your home directory on the TPU VM using the
[command. For example:](https://cloud.google.com/sdk/gcloud/reference/compute/tpus/tpu-vm/scp)
gcloud scp
$ gcloud compute tpus tpu-vm scp my-code-file ${TPU_NAME}: --zone ${ZONE}
Profiling
A profile can be captured manually through
capture_profile.py or
programmatically from within the training script using the
torch_xla.debug.profiler
APIs.
Starting the Profile Server
In order to capture a profile, a profile server must be running within the training script. Start a server with a port number of your choice, for example 9012 as shown in the following command.
import torch_xla.debug.profiler as xp server = xp.start_server(9012)
The server can be started right at the beginning of your
main function.
You can now capture profiles as described in the following section. The script profiles everything that happens on one TPU device.
Adding Traces
If you would also like
to profile operations on the host machine, you can add
xp.StepTrace or
xp.Trace in your code. These functions trace the Python code on the
host machine.
(You can think of this as measuring how much time it takes to execute the Python
code on the host (CPU) before passing the ""graph"" to the TPU device). You can
add this inside the training loop where the code processes batches of data,
for example,
for step, batch in enumerate(train_dataloader):
with xp.StepTrace('Training_step', step_num=step):
...
or wrap individual parts of the code with
with xp.Trace('loss'):
loss = ...
If you are using Lighting you can skip adding traces as it is done automatically in some parts of the code. However if you want to add additional traces, you are welcome to insert them inside the training loop.
You will be able to capture device activity after the initial compilation; wait until the model starts its training or inference steps.
Manual Capture
The
capture_profile.py script from the Pytorch XLA repository
enables quickly capturing a profile. You can do this by copying the
[capture profile file](https://raw.githubusercontent.com/pytorch/xla/master/scripts/capture_profile.py)
directly to your TPU VM. The following command copies it to
the home directory.
$ gcloud compute tpus tpu-vm ssh ${TPU_NAME} \
--zone us-central2-b \
--worker=all \
--command=""wget https://raw.githubusercontent.com/pytorch/xla/master/scripts/capture_profile.py""
While training is running, execute the following to capture a profile:
$ gcloud compute tpus tpu-vm ssh ${TPU_NAME} \
--zone us-central2-b \
--worker=all \
--command=""python3 capture_profile.py --service_addr ""localhost:9012"" --logdir ~/profiles/ --duration_ms 2000""
This command saves
.xplane.pb files in the
logdir. You can change
the logging directory
~/profiles/ to your preferred location and name. It is also possible to
directly save in the Cloud Storage bucket. To do that, set
logdir to be
gs://your_bucket_name/.
Programmatic Capture
Rather than capturing the profile manually by triggering a script, you can
configure your training script to automatically trigger a profile
by using the
[torch_xla.debug.profiler.trace_detached](https://github.com/pytorch/xla/blob/6d73ca8ecf8e5ef7a76296ac95dbbbe78878f6b3/torch_xla/debug/profiler.py#L93)
API within your train script.
As an example, to automatically capture a profile at a specific epoch and step,
you can configure your training script to consume
PROFILE_STEP,
PROFILE_EPOCH, and
PROFILE_LOGDIR environment
variables:
import os
import torch_xla.debug.profiler as xp
# Within the training script, read the step and epoch to profile from the
# environment.
profile_step = int(os.environ.get('PROFILE_STEP', -1))
profile_epoch = int(os.environ.get('PROFILE_EPOCH', -1))
...
for epoch in range(num_epoch):
...
for step, data in enumerate(epoch_dataloader):
if epoch == profile_epoch and step == profile_step:
profile_logdir = os.environ['PROFILE_LOGDIR']
# Use trace_detached to capture the profile from a background thread
xp.trace_detached('localhost:9012', profile_logdir)
...
This will save the
.xplane.pb files in the directory specified by the
PROFILE_LOGDIR environment variable.
Analysis in TensorBoard
To further analyze profiles you can use
[TensorBoard](https://www.tensorflow.org/tensorboard)
with the profiler plugin either on the same or on another machine (recommended).
To run TensorBoard on a remote machine, connect to it using SSH and enable port forwarding. For example,
$ ssh -L 6006:localhost:6006 remote server address
or
$ gcloud alpha compute tpus tpu-vm ssh $TPU_NAME --zone=$ZONE --ssh-flag=""-4 -L 6006:localhost:6006""
On your remote machine, install the required packages and launch TensorBoard
(assuming you have profiles on that machine under
~/profiles/). If you
stored the profiles in another directory or Cloud Storage bucket,
make sure to specify paths correctly, for example,
gs://your_bucket_name/profiles.
(vm)$ pip install tensorflow tensorboard-plugin-profile
(vm)$ tensorboard --logdir ~/profiles/ --port 6006
(vm)$ pip uninstall tensorflow tf-nightly tensorboard tb-nightly tbp-nightly
Running TensorBoard
In your local browser go to:
[http://localhost:6006/](http://localhost:6006/)
and choose
PROFILE from the dropdown menu to
load your profiles.
Refer to
[TPU tools](/tpu/docs/cloud-tpu-tools) for information on the
TensorBoard tools and how to interpret the output.",PyTorch XLA performance profiling | Cloud TPU | Google Cloud,
id,url,body,title,description
178,https://cloud.google.com/tpu/docs/troubleshooting/trouble-jax,"Troubleshooting JAX - TPU
This guide provides pointers to JAX troubleshooting information to help you identify and resolve problems you might encounter while training JAX models on Cloud TPU.
For a more general guide to
getting started with Cloud TPU, see the
[JAX quickstart](/tpu/docs/run-calculation-jax).
General JAX issues
If you run into issues while developing your training model or
training with JAX, see the
[JAX FAQ.](https://jax.readthedocs.io/en/latest/faq.html)
For more general programming
errors you might encounter when writing a training application with JAX, see
[JAX Errors.](https://jax.readthedocs.io/en/latest/errors.html)
Profiling JAX performance
You can understand how your TPU resources are being utilized using the
tools described in
[Profiling JAX performance.](https://jax.readthedocs.io/en/latest/profiling.html)
Troubleshooting memory issues
You can monitor how the memory is used with the
[JAX Device Memory Profiler](https://jax.readthedocs.io/en/latest/device_memory_profiling.html),
but you cannot directly manage how it is used.
The Device Memory Profiler can be used to:
- Figure out
[which arrays and executables are in TPU memory](https://jax.readthedocs.io/en/latest/device_memory_profiling.html#understanding-how-a-jax-program-is-using-gpu-or-tpu-memory)at a given time, or [Track down memory leaks.](https://jax.readthedocs.io/en/latest/device_memory_profiling.html#debugging-memory-leaks)
You cannot specify how TPU memory is allocated for specific operations.
For more information on JAX-specific TPU performance issues, see
[Performance Notes for using TPUs with JAX](/tpu/docs/run-calculation-jax#performance_notes).
Troubleshooting TPU issues
How can I verify that the TPU is running?
Details
Everything will be run on the TPU as long as JAX doesn't print ""No GPU/TPU found, falling back to CPU.""
You can verify the TPU is active by either looking at
jax.devices(), where
you should see several TPU devices displayed, or verify
programmatically with:
assert jax.devices()[0].platform == 'tpu'.
RuntimeError: Unable to initialize backend 'tpu': UNAVAILABLE: No TPU Platform available.
Details
This runtime error message and/or finding the following in
/tmp/tpu_logs/tpu_driver.WARNING on the TPU VM:
W1118 17:40:20.985243 23901 tpu_version_flag.cc:57] No hardware is found. Using default TPU version:xxxxxx
can indicate that you are running the wrong TPU VM version.
Verify that you are running the
[current JAX runtime version](/tpu/docs/supported-tpu-versions#jax) and retry.",Troubleshooting JAX - TPU | Google Cloud,
id,url,body,title,description
98,https://cloud.google.com/tpu/docs/reference/rest/v2alpha1/projects.locations.nodes/stop,"Method: projects.locations.nodes.stop
Stay organized with collections
Save and categorize content based on your preferences.
Stops a node. This operation is only available with single TPU nodes.
HTTP request
POST https://tpu.googleapis.com/v2alpha1/{name=projects/*/locations/*/nodes/*}:stop
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
string
Required. The resource name.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[Operation](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse#Operation)
Authorization scopes
Requires one of the following OAuth scopes:
-
https://www.googleapis.com/auth/cloud.tpu
-
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Method: projects.locations.nodes.stop | Cloud TPU | Google Cloud,
id,url,body,title,description
124,https://cloud.google.com/tpu/docs/reference/rest/Shared.Types/WaitOperationRequest,"Send feedback
WaitOperationRequest
Stay organized with collections
Save and categorize content based on your preferences.
The request message for
Operations.WaitOperation.
JSON representation
{
""name"" : string ,
""timeout"" : string
}
Fields
name
string
The name of the operation resource to wait on.
timeout
string (
format)
Duration
The maximum duration to wait before timing out. If left blank, the wait will be at most the time permitted by the underlying HTTP/RPC protocol. If RPC context deadline is also specified, the shorter one will be used.
A duration in seconds with up to nine fractional digits, ending with '
s'. Example:
""3.5s"".
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2022-11-28 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]
Need to tell us more?",WaitOperationRequest | Cloud TPU | Google Cloud,
id,url,body,title,description
170,https://cloud.google.com/tpu/docs/pytorch-pods,"Run PyTorch code on TPU Pod slices
PyTorch/XLA requires all TPU VMs to be able to access the model code and data.
You can use a
[startup script](#create-tpu-vm) to download the software needed
to distribute the model data to all TPU VMs.
If you are connecting your TPU VMs to a
[Virtual Private Cloud](/vpc/docs/vpc)
(VPC) you must add a firewall rule in your project to allow ingress for ports
8470 - 8479. For more information about adding firewall rules, see
[Using firewall rules](/firewall/docs/using-firewalls)
Set up your environment
In the Cloud Shell, run the following command to make sure you are running the current version of
gcloud:
$ gcloud components update
If you need to install
gcloud, use the following command:
$ sudo apt install -y google-cloud-sdk
Create some environment variables:
$ export PROJECT_ID=project-id $ export TPU_NAME=tpu-name $ export ZONE=us-central2-b $ export RUNTIME_VERSION=tpu-ubuntu2204-base $ export ACCELERATOR_TYPE=v4-32
Create the TPU VM
$ gcloud compute tpus tpu-vm create ${TPU_NAME} \
--zone=${ZONE} \
--project=${PROJECT_ID} \
--accelerator-type=${ACCELERATOR_TYPE} \
--version ${RUNTIME_VERSION}
Configure and run the training script
Add your SSH certificate to your project:
ssh-add ~/.ssh/google_compute_engine
Install PyTorch/XLA on all TPU VM workers
gcloud compute tpus tpu-vm ssh ${TPU_NAME} \ --zone=${ZONE} \ --project=${PROJECT_ID} \ --worker=all --command="" pip install torch~=2.2.0 torch_xla[tpu]~=2.2.0 torchvision -f https://storage.googleapis.com/libtpu-releases/index.html""
Clone XLA on all TPU VM workers
gcloud compute tpus tpu-vm ssh ${TPU_NAME} \ --zone=${ZONE} \ --project=${PROJECT_ID} \ --worker=all --command=""git clone -b r2.2 https://github.com/pytorch/xla.git""
Run the training script on all workers
gcloud compute tpus tpu-vm ssh ${TPU_NAME} \ --zone=${ZONE} \ --project=${PROJECT_ID} \ --worker=all \ --command=""PJRT_DEVICE=TPU python3 ~/xla/test/test_train_mp_imagenet.py \ --fake_data \ --model=resnet50 \ --num_epochs=1 2>&1 | tee ~/logs.txt""
The training takes about 5 minutes. When it completes, you should see a message similar to the following:
Epoch 1 test end 23:49:15, Accuracy=100.00 10.164.0.11 [0] Max Accuracy: 100.00%
Clean up
When you are done with your TPU VM follow these steps to clean up your resources.
Disconnect from the Compute Engine:
(vm)$ exit
Verify the resources have been deleted by running the following command. Make sure your TPU is no longer listed. The deletion might take several minutes.
$ gcloud compute tpus tpu-vm list \ --zone europe-west4-a",Run PyTorch code on TPU Pod slices | Google Cloud,
id,url,body,title,description
19,https://cloud.google.com/tpu/docs/reference/rest/v1alpha1/projects.locations.acceleratorTypes/get,"Method: projects.locations.acceleratorTypes.get
Stay organized with collections
Save and categorize content based on your preferences.
HTTP request
GET https://tpu.googleapis.com/v1alpha1/{name=projects/*/locations/*/acceleratorTypes/*}
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
string
Required. The resource name.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[AcceleratorType](/tpu/docs/reference/rest/v1alpha1/projects.locations.acceleratorTypes#AcceleratorType)
Authorization scopes
Requires one of the following OAuth scopes:
-
https://www.googleapis.com/auth/cloud.tpu
-
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Method: projects.locations.acceleratorTypes.get | Cloud TPU | Google Cloud,
id,url,body,title,description
82,https://cloud.google.com/tpu/docs/faq,"Frequently Asked Questions - Cloud TPU
This document contains a list of frequently asked questions about Cloud TPUs. It is broken up into sections:
- Framework independent FAQs - questions about using Cloud TPUs regardless of what ML framework you are using.
- JAX FAQS - questions about using Cloud TPUs with JAX.
- PyTorch FAQs - questions about using Cloud TPUs with PyTorch.
Framework independent FAQs
How do I check which process is using the TPU on a Cloud TPU VM?
Run
sudo lsof -w /dev/accel* on the Cloud TPU VM to print the process ID and
other information about the process using the TPU.
How do I add a persistent disk volume to a Cloud TPU VM?
For more information, see
[Add a persistent disk to a TPU VM](https://cloud.google.com/tpu/docs/tutorials/supported-models.)
What storage options are supported/recommended for training with TPU VM?
For more information, see
[Cloud TPU storage options](https://cloud.google.com/tpu/docs/storage-options).
JAX FAQs
How do I know if the TPU is being used by my program?
There are a few ways to double check JAX is using the TPU:
Use the
jax.devices()function. For example:
assert jax.devices()[0].platform == 'tpu'
Profile your program and verify the profile contains TPU operations. For more information, see
[Profiling JAX programs](https://github.com/google/jax/blob/main/docs/profiling.md)
For more information, see
[JAX FAQ](https://jax.readthedocs.io/en/latest/faq.html)
Pytorch FAQs
How do I know if the TPU is being used by my program?
You can run following python commands:
>>> import torch_xla.core.xla_model as xm
>>> xm.get_xla_supported_devices(devkind=""TPU"")
And verify if you can see any TPU devices.",Frequently Asked Questions - Cloud TPU | Google Cloud,
id,url,body,title,description
182,https://cloud.google.com/tpu/docs/tutorials/supported-models,"Supported reference models
Cloud TPU provides a set of reference models that are optimized for fast and accurate training.
Cloud TPU supports the following major and minor framework releases of TensorFlow, PyTorch, and JAX/FLAX.
TensorFlow release numbering has changed with release 2.5.0. Major TensorFlow release numbers end with '0' and all patch release numbers end with numbers greater than '0'. For example, TF 2.10.0 is a major release and TF 2.10.1 is a minor release.
In order to run the latest-supported framework version, check to see if there are any
[patch releases](/tpu/docs/supported-patches)
to the major release. If so, you can run the latest-supported patch
release rather than the major release.
|Framework
|Major version
|Model category
|Reference models
|Supported versions
|TensorFlow
|",Supported reference models | Cloud TPU | Google Cloud,
id,url,body,title,description
159,https://cloud.google.com/tpu/docs/reference/rest/v2/projects.locations.runtimeVersions/get,"Method: projects.locations.runtimeVersions.get
Stay organized with collections
Save and categorize content based on your preferences.
HTTP request
GET https://tpu.googleapis.com/v2/{name=projects/*/locations/*/runtimeVersions/*}
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
string
Required. The resource name.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[RuntimeVersion](/tpu/docs/reference/rest/v2/projects.locations.runtimeVersions#RuntimeVersion)
Authorization scopes
Requires one of the following OAuth scopes:
-
https://www.googleapis.com/auth/cloud.tpu
-
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Method: projects.locations.runtimeVersions.get | Cloud TPU | Google Cloud,
id,url,body,title,description
166,https://cloud.google.com/tpu/docs/storage-options,"Storage options for Cloud TPU data
This document describes data storage options that can be used when training models on Cloud TPU.
Introduction
Cloud TPU requires data storage for:
- dataset downloading and preprocessing
- host input pipeline processing
- model training input
- model training output
There are five storage options for the Cloud TPU application data and training datasets:
- The
[boot disk](#vmboot-disk)for a TPU VM or TPU Node
- A
[persistent disk](#persistent-disk)attached to a TPU VM or TPU Node [Cloud Storage buckets](#gcsbuckets) [Filestore file share](#fileshare)on a Compute Engine VM
For storage cost and performance details
see
[Storage options](https://cloud.google.com/compute/docs/disks).
The boot disk for a TPU VM or TPU Node
By default, each Cloud TPU VM has a 100GB single boot
[persistent disk](/tpu/docs/setup-persistent-disk) that contains
the operating system. The boot disk can also be used to store downloaded
datasets for preprocessing and model input and output data, provided the
total amount does not exceed the available space on the boot disk.
If your training application requires additional storage space beyond the
boot disk default, you can add one or more persistent disks
to your VM or TPU VM instance. There are different procedures for adding
a persistent disk to a TPU Node (a
[Compute Engine VM](https://cloud.google.com/compute/docs/disks/add-persistent-disk))
or to a [TPU VM](/tpu/docs/setup-persistent-disk).
A persistent disk attached to a TPU VM or TPU Node
[Persistent disks](/tpu/docs/compute/docs/disks/persistent-disks) are
durable network storage devices that your VM instances can access like physical
disks in a desktop or a server. The data on each persistent disk is distributed
across several physical disks. Compute Engine manages the physical
disks and the data distribution for you to ensure redundancy and optimal
performance.
Persistent disks are created independently from your virtual machine (VM) instances, so you can keep your data even after you delete your VM instances. Persistent disk performance scales automatically with size, so you can resize your existing persistent disks or add more persistent disks to an instance to meet your performance and storage space requirements.
Persistent disks have built-in redundancy to protect your data against equipment failure and to ensure data availability through datacenter maintenance events. Checksums are calculated for all persistent disk operations, so we can ensure that what you read is what you wrote.
Additionally, you can create
[snapshots](/compute/docs/disks/create-snapshots)
of persistent disks to protect against data loss due to user error. Snapshots
are incremental, and take only minutes to create even if you snapshot disks that
are attached to running instances.
For more information on using persistent disks with TPU VMs, see
[Add a persistent disk to a TPU VM](/tpu/docs/setup-persistent-disk).
Cloud Storage buckets
[Cloud Storage buckets](/compute/docs/disks#gcsbuckets) are the most
flexible, scalable, and durable storage option for your VM
instances. If your training job does not require the lower latency of
[persistent disks](#pdspecs), you can store your dataset in a
Cloud Storage bucket.
The performance of Cloud Storage buckets depends on the
[storage class](/storage/docs/storage-classes) that you select and the
location of the bucket relative to your instance.
Creating your Cloud Storage bucket in the same zone as your VM instance (for TPU Nodes) or your TPU VM gives performance that is comparable to persistent disks but with higher latency and less consistent throughput characteristics.
All Cloud Storage buckets have built-in redundancy to protect your data against equipment failure and to ensure data availability through datacenter maintenance events. Checksums are calculated for all Cloud Storage operations to help ensure that what you read is what you wrote.
Unlike persistent disks, Cloud Storage buckets are not restricted to the zone where your instance is located. Additionally, you can read and write data to a bucket from multiple instances simultaneously. For example, you can configure instances in multiple zones to read and write data in the same bucket rather than replicate the data to persistent disks in multiple zones.
Cloud Storage FUSE
Cloud Storage FUSE allows you to mount and access Cloud Storage buckets as local file systems. This allows applications to read and write objects in your bucket using standard file system semantics.
See the Cloud Storage
[FUSE documentation](https://cloud.google.com/storage/docs/gcs-fuse)
for details about how Cloud storage FUSE works and a description of how
Cloud Storage FUSE operations map to Cloud Storage operations. You can find
additional information about how to use Cloud Storage FUSE, such as how to
install the gcsfuse CLI and mounting buckets on
[GitHub.](https://github.com/GoogleCloudPlatform/gcsfuse/tree/master/docs)
Filestore file share
[Filestore file share](https://cloud.google.com/filestore) is a fully
managed network attached storage (NAS) for Compute Engine. Filestore offers
native compatibility with existing enterprise applications and supports
any NFSv3-compatible client.
Filestore offers
[low latency](https://cloud.google.com/filestore/docs/performance)
for file operations. For workloads that
are latency sensitive, Filestore supports capacity up to 100 TB
and throughput of 25 GB/s and 720K IOPS, with minimal variability in performance.
With Filestore, you can easily
[mount file shares](https://cloud.google.com/filestore/docs/mounting-fileshares)
on Compute Engine VMs.
What's next
- Learn how to
[add a persistent disk to your instance](/compute/docs/disks/add-persistent-disk#create_disk).
- Learn how to
[connect your instance to a Cloud Storage bucket](/compute/docs/disks/gcs-buckets).",Storage options for Cloud TPU data | Google Cloud,
id,url,body,title,description
177,https://cloud.google.com/tpu/docs/reference/rest/v2/projects.locations.acceleratorTypes,"REST Resource: projects.locations.acceleratorTypes
Stay organized with collections
Save and categorize content based on your preferences.
Resource: AcceleratorType
A accelerator type that a Node can be configured with.
|JSON representation
|
{
""name"": string,
""type"": string,
""acceleratorConfigs"": [
{
object (
)
}
]
}
[AcceleratorConfig](/tpu/docs/reference/rest/v2/AcceleratorConfig)
|Fields
|
name
|
string
The resource name.
|
type
|
string
The accelerator type.
|
acceleratorConfigs[]
|
object (
)
[AcceleratorConfig](/tpu/docs/reference/rest/v2/AcceleratorConfig)
The accelerator config.
|
Methods
|
|
Gets AcceleratorType.
|
|
Lists accelerator types supported by this API.
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",REST Resource: projects.locations.acceleratorTypes | Cloud TPU | Google Cloud,
id,url,body,title,description
156,https://cloud.google.com/tpu/docs/troubleshooting/troubleshoot-multislice,"Profiling Multislice environments
Cloud TPU Multislice environments are composed of multiple TPU Pod slices that communicate over the Data Center Network (DCN). You can use the DCN collective stats tool to view information about how effectively your Multislice environment is utilizing the DCN network. Specifically, the DCN Collective Stats tool enables you to:
- View and understand inter-slice network performance based on collected data
- Identify performance bottlenecks
- Optimize your model's performance
All metrics in the DCN collective stats tool are generated on a per-TPU basis.
Terminology
The DCN collective stats tool displays metrics that describe communication that occurs between TPU slices within a Multislice environment. When the TPU runtime initiates inter-slice communication, a series of operations are used:
send
- Interrupts the host to start Direct Memory Access (DMA) and provides a filled buffer to the host to start the data transfer.
send-done
- Signals the host that the data transfer is completed.
recv
- Provides an empty buffer for the host to fill with the transferred data.
recv-done
- Signals the host that the data has been received.
A collective is initiated when a
send operation occurs and is completed when
the matching
recv-done operation occurs.
Slack Time
A measure of time the collective is able to send and receive data.
This does not include the
send,
send-done,
recv or
recv-done operations.
For example, given the following timeline:
Slack time is calculated in this example as:
Slack time = t1 + t2 + t3
Increasing slack time reduces the chances to stall the TPU for a collective. You can increase the slack time by choosing a different sharding method.
Stall duration
The average duration of time the collective spends in the send, send-done, recv, and recv-done operations. Note, this does not include time spent transmitting data. For example, given the following timeline:
Stall duration is calculated in this example as:
Stall duration = tsend + tsend-done + trecv + trecv-done
Observed duration
The amount of time between the
send and
recv-done operations, including the
time sending and receiving data. For example, given the following timeline:
Observed duration is calculated as:
Observed duration = tsend + t1 + tsend-done + t2 + trecv + t3 + trecv-done
Occurrences
The number of times a collective is initiated and completed during a profile
duration. A collective is initiated when a
send operation occurs and is
completed when the matching
recv-end operation occurs. The
send operation
and its matching
recv-done operation must occur within a profile duration to be
included in this metric.
Aggregated total stall
The total amount of time a collective stalls a TPU during a profile duration. Aggregation total stall is calculated as:
Aggregated total stall = stall duration * occurrences
Data transmitted size
The amount of data transmitted over the network for the collective during the profile duration.
Required bandwidth
The bandwidth required to transmit data within the provided slack. You can use this metric to see the number of collectives competing for network bandwidth during the profile duration. Required bandwidth is computed as:
Required bandwidth = data transmitted size / slack time
Tool status
The following table shows the version of TensorFlow or TPU runtime version required for each metric displayed in the DCN Collective Stats tool.
|DCN Collective Stats
|Supported TensorFlow of TPU runtime version
|Slack time
|TensorFlow 2.15.0, tensorboard 2.15.1 and tensorboard-plugin-profile 2.15.0
|Stall duration
|TensorFlow 2.15.0, tensorboard 2.15.1 and tensorboard-plugin-profile 2.15.0
|Observed duration
|TensorFlow 2.15.0, tensorboard 2.15.1 and tensorboard-plugin-profile 2.15.0
|Occurrences
|TensorFlow 2.15.0, tensorboard 2.15.1 and tensorboard-plugin-profile 2.15.0
|Aggregated total stall
|tf-nightly, tb-nightly, tbp-nightly
|Data transmitted size
|tf-nightly, tb-nightly, tbp-nightly
|Required bandwidth
|tf-nightly, tb-nightly, tbp-nightly
How to Analyze DCN Collective Stats Tool
- Run TensorBoard server and go to Profile tab.
- Sort the table in DCN collective stats tool by Aggregated Total Stall in descending order.
- Identify the DCN collective name that has the highest Aggregated Total Stall. If the aggregated stall duration of this collective is significantly high compared to others, this could indicate that there is a bottleneck in the DCN collective.
- Multiply the required bandwidth of the DCN collective by the number of cores.
There are 8 cores per v4 TPU host, so the required bandwidth for a collective
is 8 x the value displayed. If the required bandwidth is greater than the
maximum network bandwidth of the TPU, this may mean the network is congested.
To bring down the required bandwidth, try changing the sharding mechanism you
use. For more information about sharding mechanisms, see
[Cloud TPU Multislice overview](/tpu/docs/multislice-introduction#optimize).
- Generate an HLO dump to determine if there are any compiler issues. It is
better to fan out
sendand
recv-doneoperations for a collective to allow scheduling of more overlapping HLO Ops. Overlapping more HLO operations reduces TPU stall time.
- Check the duration of
recv-doneoperations in the Trace Viewer for the DCN collective that has the maximum aggregated total stall. If the duration of the transfer is high, there could be a bandwidth bottleneck because
recv-doneoperations are usually blocked on the network to get the data.
- If the duration of
recv-doneoperations is not too high compared to the slack time, this could indicate a hardware issue.",Profiling Multislice environments | Cloud TPU | Google Cloud,
id,url,body,title,description
180,https://cloud.google.com/tpu/docs/reference/rest/v2alpha1/projects.locations/generateServiceIdentity,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [ServiceIdentity](#ServiceIdentity) [Try it!](#try-it)
Generates the Cloud TPU service identity for the project.
HTTP request
POST https://tpu.googleapis.com/v2alpha1/{parent=projects/*/locations/*}:generateServiceIdentity
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
parent
|
Required. The parent resource name.
Request body
The request body must be empty.
Response body
Response for
.
[locations.generateServiceIdentity](/tpu/docs/reference/rest/v2alpha1/projects.locations/generateServiceIdentity#google.cloud.tpu.v2alpha1.Tpu.GenerateServiceIdentity)
If successful, the response body contains data with the following structure:
|JSON representation
|
{
""identity"": {
object (
|Fields
|
identity
|
ServiceIdentity that was created or retrieved.
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
ServiceIdentity
The per-product per-project service identity for Cloud TPU service.
|JSON representation
|
{ ""email"": string }
|Fields
|
email
|
The email address of the service identity.",Method: projects.locations.generateServiceIdentity | Cloud TPU | Google Cloud,
id,url,body,title,description
117,https://cloud.google.com/tpu/docs/gke-shared-vpc-setup,"GKE Cluster with Cloud TPU using a Shared VPC
This guide describes how to:
- Set up a Cloud TPU
[GKE](/kubernetes-engine/docs/concepts/kubernetes-engine-overview)cluster using a [Shared VPC](/vpc/docs/shared-vpc)network.
- Setup the required
[APIs](#apis)and [IP ranges](#ip-vpc-peering)to ensure communication between the cluster, the Shared VPC, and Google Managed Services.
- Create
[secondary CIDR ranges](#ip-secondary-ranges)for cluster pods and services.
Concepts
These concepts will be frequently used throughout this guide:
Host Project: A project that contains one or more Shared VPC networks. In this guide, this project will contain your Shared VPC.
Service Project: A project attached to a Host Project by a Shared VPC administrator. This attachment allows it to participate in the Shared VPC. In this guide, this project will contain your Cloud TPU cluster.
Requirements
Enable APIs
Enable the following APIs on the Google Cloud console for your Host Project:
Enable the following APIs on the Google Cloud console for your Service Project:
Setup IP range for VPC Peering to Google managed services
Follow these steps to reserve an IP range in the Shared VPC network in the Host Project. The range will be used by all Google managed services in this VPC network. Cloud TPU is one of the Google managed services.
List existing IP ranges in the Shared VPC network.
$ gcloud beta compute networks list-ip-addresses network \ --project=host-project-id
Choose an available range and reserve it in the Shared VPC network.
$ gcloud beta compute addresses create peering-name \ --global \ --prefix-length=16 \ --network=network \ --purpose=VPC_PEERING \ --project=host-project-id
The peering-name specifies the name of the VPC Peering connection. The name will be used in the next step.
Create a VPC Network Peering connection between the Host Project and Google managed services.
$ gcloud beta services vpc-peerings connect \ --service=servicenetworking.googleapis.com \ --network=network \ --ranges=peering-name \ --project=host-project-id
Create secondary IP ranges for the cluster
In your Shared VPC network, select or create a subnetwork and add two secondary CIDR ranges for the cluster pods and services.
These ranges are for your cluster's pods and services, respectively. The range names will be used in the following steps.
subnet will be the subnetwork in the network of your Host Project.
tier-1-name will be the name of the secondary range used by GKE Pods in subnet.
tier-2-name will be the name of the secondary range used by GKE Services in subnet.
Create a GKE cluster with Cloud TPU
The following command shows how to create a GKE using the existing CIDR ranges in your Shared VPC network, enabling Cloud TPU:
$ gcloud beta container clusters create cluster-name \
--enable-ip-alias \
--network projects/host-project-id/global/networks/network \
--subnetwork projects/host-project-id/regions/region/subnetworks/subnet \
--cluster-secondary-range-name tier-1-name \
--services-secondary-range-name tier-2-name \
--scopes=cloud-platform \
--enable-tpu \
--enable-tpu-service-networking \
--project=service-project-id
- Refer to
[command reference guide for further details on these flags.](/sdk/gcloud/reference/beta/container/clusters/create)
gcloud beta container clusters create
Follow the Pod Spec steps in the guide
[Run Cloud TPU applications on GKE](/tpu/docs/kubernetes-engine-setup#job-spec)
to build a job that uses Cloud TPU resources.
Clean Up
When you've finished with Cloud TPU on GKE, clean up the resources to avoid incurring extra charges to your Cloud Billing account.
Delete the reserved peering IP range.
$ gcloud beta compute addresses delete peering-name \ --global \ --project=host-project-id
Follow the instructions on
[Cleaning up](/kubernetes-engine/docs/how-to/cluster-shared-vpc#cleaning_up)on Setting up Clusters with Shared VPC to delete the cluster and the network resources.",GKE Cluster with Cloud TPU using a Shared VPC | Google Cloud,
id,url,body,title,description
61,https://cloud.google.com/tpu/docs/reference/rest/Shared.Types/ListLocationsRequest,"ListLocationsRequest
Stay organized with collections
Save and categorize content based on your preferences.
|JSON representation
|
{
""name"": string,
""filter"": string,
""pageSize"": integer,
""pageToken"": string
}
|Fields
|
name
|
string
The resource that owns the locations collection, if applicable.
|
filter
|
string
A filter to narrow down results to a preferred subset. The filtering language accepts strings like
""displayName=tokyo"", and is documented in more detail in
[AIP-160](https://google.aip.dev/160).
|
pageSize
|
integer
The maximum number of results to return. If not set, the service selects a default.
|
pageToken
|
string
A page token received from the
nextPageToken field in the response. Send that page token to receive the subsequent page.
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2022-11-28 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",ListLocationsRequest | Cloud TPU | Google Cloud,
id,url,body,title,description
184,https://cloud.google.com/tpu/docs/reference/rest/v2/projects.locations.nodes/get,"Method: projects.locations.nodes.get
Stay organized with collections
Save and categorize content based on your preferences.
Gets the details of a node.
HTTP request
GET https://tpu.googleapis.com/v2/{name=projects/*/locations/*/nodes/*}
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
string
Required. The resource name.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[Node](/tpu/docs/reference/rest/v2/projects.locations.nodes#Node)
Authorization scopes
Requires one of the following OAuth scopes:
-
https://www.googleapis.com/auth/cloud.tpu
-
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Method: projects.locations.nodes.get | Cloud TPU | Google Cloud,
id,url,body,title,description
79,https://cloud.google.com/tpu/docs/setup-persistent-disk,"Add a Persistent Disk to a TPU VM
A TPU VM includes a 100GB boot disk. For some scenarios, your TPU VM might need
additional storage for training or preprocessing. You can add a
[Persistent Disk](/compute/docs/disks#pdspecs)
to expand your local disk capacity.
Overview
A Persistent Disk attached to a single-device TPU (v2-8, v3-8, v4-8, etc.) can be
configured as
read-write or
read-only. When you attach a Persistent Disk to a
TPU VM that is part of a TPU Pod, the disk is attached to each TPU VM in that
Pod. To prevent two or more TPU VMs from a Pod from writing to a Persistent Disk
at once, all Persistent Disks attached to a TPU VM in a Pod must be configured
as
read-only.
read-only disks are useful for storing a dataset for processing
on a TPU Pod.
After creating and attaching a Persistent Disk to your TPU VM, you must mount
the Persistent Disk, specifying where in the file system the Persistent Disk
can be accessed. For more information, see
[Mounting a disk](/compute/docs/disks/format-mount-disk-linux#mounting).
Prerequisites
You need to have a Google Cloud account and project set up before using the
following procedures. If you don't already have a Cloud TPU project set up,
follow the procedure in
[Set up an account and a Cloud TPU project](/tpu/docs/setup-gcp-account)
before continuing.
High-level steps
The high-level steps to set up a Persistent Disk:
[Create a Persistent Disk](#create-pd) [Attach a Persistent Disk to a TPU VM](#attach-pd) [Mount the Persistent Disk](#mount-pd) [Clean up TPU VM and Persistent Disk resources](#cleanup)
Setting up a TPU VM and a Persistent Disk
You can attach a Persistent Disk to a TPU VM when you create the TPU VM. You can also attach a Persistent Disk to an existing TPU VM.
Create a Persistent Disk
Use the following command to create a Persistent Disk:
$ gcloud compute disks create disk-name \
--size disk-size \
--zone zone \
--type pd-balanced
Command flag descriptions
disk-name
- A name of your choosing for the Persistent Disk.
disk-size
- The size of the Persistent Disk in GB.
zone
- The
[zone](/tpu/docs/types-zones)in which to create the Persistent Disk. This needs to be the same zone used to create the TPU.
type
- The
[disk type to add](/compute/docs/disks#disk-types). Supported types are:
pd-standard,
pd-ssdor
pd-balanced.
Attach a Persistent Disk
You can attach a Persistent Disk to your TPU VM when you create the TPU VM or you can add one after the TPU VM is created.
Attach a Persistent Disk when you create a TPU VM
Use the
--data-disk flag to attach a Persistent Disk when you create a TPU VM.
If you are creating a TPU Pod, you must specify
mode=read-only. If you are
creating a single TPU device, you can specify
mode=read-only or
mode=read-write.
The following command creates a single TPU and sets the Persistent Disk mode to
read-write:
$ gcloud compute tpus tpu-vm create tpu-name \
--project project-id \
--zone=zone \
--accelerator-type=v3-8 \
--version=Cloud TPU software version \
--data-disk source=projects/project-id/zones/zone/disks/disk-name,mode=read-write
Command flag descriptions
tpu-name
- The name you have chosen for the TPU resources.
project
- Your project ID.
zone
- The
[zone](/tpu/docs/types-zones)to create your Cloud TPU in.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
version
- The
[Cloud TPU software version](/tpu/docs/supported-tpu-versions#tpu_software_versions)for your framework.
data-disk
- The name and read/write mode of the Persistent Disk to attach to the TPU VM.
Attach a Persistent Disk to an existing TPU VM
Use the
gcloud alpha compute tpus tpu-vm attach-disk command to attach a
Persistent Disk to an existing TPU VM. See the
[
documentation for more details and examples. gcloud](/sdk/gcloud/reference/alpha/compute/tpus/tpu-vm/attach-disk)
$ gcloud alpha compute tpus tpu-vm attach-disk tpu-name \
--zone=zone \
--disk=disk-name \
--mode=disk-mode
Command flag descriptions
tpu-name
- The name of the TPU resources.
zone
- The
[zone](/tpu/docs/types-zones)where the Cloud TPU is located.
disk-name
- The name of the Persistent Disk to attach to the TPU VM.
mode
- The mode of the disk. Mode must be one of:
read-onlyor
read-write.
If you want to delete the Persistent Disk when you delete the TPU VM, you need to set the auto-delete state of the Persistent Disk using the following command:
$ gcloud compute instances set-disk-auto-delete vm-instance \
--zone=zone \
--auto-delete \
--disk=disk-name
Command flag descriptions
vm-instance
- After you SSH into the TPU VM, your shell prompt changes to include your user ID followed by a generated VM instance name (for example. pjohnston@t1v-n-...$). Replace vm-instance with the generated VM instance name,
zone
- The
[zone](/tpu/docs/types-zones)in which the Persistent Disk is located.
auto-delete
- Automatically delete the Persistent Disk when the TPU resources are deleted.
disk-name
- A name of your Persistent Disk.
If your VM shuts down for any reason, the Persistent Disk might be disconnected.
See
[Configure automatic mounting on system restart](/compute/docs/disks/format-mount-disk-linux#configure_automatic_mounting_on_vm_restart) to cause your
Persistent Disk to automatically mount on VM restart.
For more information about automatically deleting a Persistent Disk, see
[Modify a Persistent Disk](/compute/docs/disks/modify-persistent-disk).
Mount a Persistent Disk
In order to access a Persistent Disk from a TPU VM, you must mount the disk. This specifies a location in the TPU VM file system where the Persistent Disk can be accessed.
Connect to your TPU VM using SSH:
$ gcloud compute tpus tpu-vm ssh tpu-name --zone zone
When working with a TPU Pod, there is a one TPU VM for each TPU in the Pod. The preceding command will work for both TPU devices and TPU Pods. If you are using TPU Pods this command will connect you to the first TPU in the Pod (also called worker 0).
From the TPU VM, list the disks attached to the TPU VM:
(vm)$ sudo lsblk
The output from the
lsblkcommand should look like the following:
NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT loop0 7:0 0 55.5M 1 loop /snap/core18/1997 loop1 7:1 0 67.6M 1 loop /snap/lxd/20326 loop2 7:2 0 32.3M 1 loop /snap/snapd/11588 loop3 7:3 0 32.1M 1 loop /snap/snapd/11841 loop4 7:4 0 55.4M 1 loop /snap/core18/2066 sda 8:0 0 300G 0 disk sda1 8:1 0 299.9G 0 part / sda14 8:14 0 4M 0 part sda15 8:15 0 106M 0 part /boot/efi sdb 8:16 0 10G 0 disk <== Persistent Disk
In this example
sdais the boot disk and
sdbis the name of the newly attached Persistent Disk. The name of the attached Persistent Disk will depend upon how many persistent disks are attached to the VM.
When using a TPU Pod, you will need to mount the Persistent Disk on all TPU VMs in your Pod. The name of the Persistent Disk should be the same for all TPU VMs, but it is not guaranteed. For example if you detach and then re-attach the Persistent Disk, the device name will be incremented, changing from
sdbto
sdc, and so on.
If the disk has not been formatted, format the attached Persistent Disk now:
(vm)$ sudo mkfs.ext4 -m 0 -E lazy_itable_init=0,lazy_journal_init=0,discard /dev/sdb
Create a directory to mount the Persistent Disk:
If you are using a TPU device, run the following command to create a directory to mount the Persistent Disk:
(vm)$ sudo mkdir -p /mnt/disks/persist
If you are using a TPU Pod, run the following command outside of your TPU VM. This will create the directory on all TPU VMs in the Pod.
(vm)$ gcloud compute tpus tpu-vm ssh $TPU_NAME --worker=all --command=""sudo mkdir -p /mnt/disks/persist""
Mount the Persistent Disk:
If you are using a TPU device, run the following command to mount the Persistent Disk on your TPU VM.
(vm)$ sudo mount -o discard,defaults /dev/sdb /mnt/disks/persist
If you are using a TPU Pod, run the following command outside of your TPU VM. It will mount the Persistent Disk on all TPU VMs in your Pod.
(vm)$ gcloud compute tpus tpu-vm ssh $TPU_NAME --worker=all --command=""sudo mount -o discard,defaults /dev/sdb /mnt/disks/persist""
Clean up
Delete your TPU resources when you are done with them.
Disconnect from the Compute Engine instance, if you have not already done so:
(vm)$ exit
Your prompt should now be
username@projectname, showing you are in the Cloud Shell.
Delete your Cloud TPU and Compute Engine resources.
$ gcloud compute tpus tpu-vm delete tpu-name \ --zone=zone
Verify the resources have been deleted by running
gcloud list. The deletion might take several minutes. The output from
gcloud listshouldn't display any of the TPU VM resources created by this procedure.
TPU VM
$ gcloud compute tpus tpu-vm list --zone=zone
TPU Node
$ gcloud compute tpus execution-groups list --zone zone
Verify that the Persistent Disk was automatically deleted when the TPU VM was deleted by listing all disks in the zone where you created the Persistent Disk:
$ gcloud compute disks list --filter=""zone:( us-central1-b )""
If the Persistent Disk was not deleted when the TPU VM was deleted, use the following commands to delete it:
$ gcloud compute disks delete disk-name \ --zone zone",Add a Persistent Disk to a TPU VM | Google Cloud,
id,url,body,title,description
59,https://cloud.google.com/tpu/docs/system-architecture-tpu-vm,"System Architecture
Tensor Processing Units (TPUs) are application specific integrated circuits (ASICs) designed by Google to accelerate machine learning workloads. Cloud TPU is a Google Cloud service that makes TPUs available as a scalable resource.
TPUs are designed to perform matrix operations quickly making them ideal for
machine learning workloads. You can run machine learning workloads on TPUs using
frameworks such as
[TensorFlow](https://www.tensorflow.org/), [Pytorch](/tpu/docs/tutorials/pytorch-pod),
and [JAX](https://jax.readthedocs.io/en/latest/).
Cloud TPU terms
If you are new to Cloud TPUs, check out the
[TPU documentation home](https://cloud.google.com/tpu/docs).
The following sections explain terms and related concepts used in this document.
Batch inference
Batch or offline inference refers to doing inference outside of production pipelines typically on a bulk of inputs. Batch inference is used for offline tasks such as data labeling and also for evaluating the trained model. Latency SLOs are not a priority for batch inference.
Inference
Inference is the process of using a trained model to make predictions on new
data. It is used by the
[serving](#serving) process.
Queued resource
A representation of TPU resources, used to enqueue and manage a request for a
single-slice or multi-slice TPU environment. See
[Queued Resources user guide](/tpu/docs/queued-resources)
for more information.
Serving
Serving is the process of deploying a trained machine learning model to a production environment where it can be used to make predictions or decisions. Latency and service-level availability are important for serving.
Single host and multi host
A TPU host is a VM that runs on a physical computer connected to TPU hardware. TPU workloads can use one or more host.
A single-host workload is limited to one TPU VM and can access 1, 4, or 8 TPU chips. A multi-host TPU v5e workload can access 8, 12, 16, 32, 64, 128, or 256 TPU chips with one TPU VM for every four TPU chips. Multi-host workloads distribute training across multiple TPU VMs.
TPU v5e supports single and multi-host training and single host inference.
Multi-host inference is supported using
[Sax](https://github.com/google/saxml).
For more information, see [Large Language Model Serving](/tpu/docs/v5e-inference#large_language_model_serving).
Slices
A Pod slice is a collection of chips all located inside the same TPU Pod connected by high-speed inter chip interconnects (ICI).
v5e slices are described with 2D slice shapes. Each number in the slice shape
corresponds to the number of v5e chips in one of dimension. For example,
4x2
describes an arrangement of 8 v5e chips in a 4 x 2 grid.
TPU v4 slices can be described in terms of v4 chips. v4 slices are described
with 3D shapes. Each number in the slice shape corresponds to the number of v4
chips in a dimension. For example
4x4x8 describes an arrangement of 128
v4 chips in a 4 x 4 x 8 cube.
v4 slices can also be described in terms of the number of TensorCores in the
slice. For example,
v4-128 describes a v4 slice with 128 TensorCores. v4
slices are available with 16, 32, 64, 128, 256, 512, 1024, 2048, or 4096
TensorCores.
TPU v3 slices are described in terms of TensorCores and are available with 32, 128, 512, 1024, or 2048 TensorCores. TPU v2 slices are also described in terms of TensorCores and are available with 32, 128, 256, or 512 TensorCores.
Chip shape and chip topology also refer to slice shapes.
See the table in the
[Accelerator Types](/tpu/docs/supported-tpu-configurations#tpu-v5e-config) section for a list
of supported slice shapes for v5e.
TPU Pod
A TPU Pod is a contiguous set of TPUs grouped together over a specialized network. The number of TPU chips in a TPU Pod is dependent on the TPU version.
TPU VM
A virtual machine running Linux that has access to the underlying TPUs. For v5e TPUs, each TPU VM has direct access to 1, 4, or 8 chips depending on the user-specified accelerator type. For v4 and earlier, each TPU VM has access to 4 TPU chips. A TPU VM is also known as a worker.
TPU chip
A TPU chip contains one or more TensorCores. The number of TensorCores depend on the version of the TPU chip. Each TensorCore consists of one or more matrix-multiply units (MXUs), a vector unit, and a scalar unit.
An MXU is composed of 128 x 128 multiply-accumulators in a
[systolic array](https://en.wikipedia.org/wiki/Systolic_array).
MXUs provide the bulk of the compute power in a TensorCore. Each MXU is capable
of performing 16K multiply-accumulate operations per cycle. All multiplies take
[bfloat16](/tpu/docs/bfloat16) inputs, but all accumulations are performed in
FP32 number format.
The vector unit is used for general computation such as activations and softmax. The scalar unit is used for control flow, calculating memory addresses, and other maintenance operations.
TensorCores
TPU chips have one or two TensorCores to run matrix multiplication. TPU v5e has
one TensorCore per chip. TPU v2, v3, v4, and v5p have two TensorCores per
chip. For more information about TensorCores, see this
[ACM
article](https://dl.acm.org/doi/pdf/10.1145/3360307).
Worker
See
[TPU VM](#tpu-vm).
TPU versions
The exact layout of a TPU depends on the
[TPU version](/tpu/docs/supported-tpu-versions)
that you use. Architectural details and performance characteristics of TPU v2
and v3 are available in [A Domain Specific Supercomputer for Training Deep Neural Networks](https://dl.acm.org/doi/pdf/10.1145/3360307).
TPU v5e
Each v5e chip contains one TensorCore. Each TensorCore has 4 Matrix Multiply Units (MXU), a vector unit, and a scalar unit.
The following diagram illustrates a TPU v5e chip.
The following table shows the key chip specifications and their values for v5e.
|Key chip specifications
|v5e values
|Peak compute per chip (bf16)
|197 TFLOPs
|Peak compute per chip (Int8)
|393 TFLOPs
|HBM2 capacity and bandwidth
|16 GB, 819 GBps
|Interchip Interconnect BW
|1600 Gbps
The following table shows Pod specifications and their values for v5e.
|Key Pod specifications
|v5e values
|TPU Pod size
|256 chips
|Interconnect topology
|2D Torus
|Peak compute per Pod
|100 PetaOps(Int8)
|All-reduce bandwidth per Pod
|51.2 TB/s
|Bisection bandwidth per Pod
|1.6 TB/s
|Data center network bandwidth per Pod
|6.4 Tbps
TPU v4
Each TPU v4 chip contains two TensorCores. Each TensorCore has four MXUs, a vector unit, and a scalar unit. The following table shows the key specifications for a v4 TPU Pod.
|Key specifications
|v4 Pod values
|Peak compute per chip
|275 teraflops (bf16 or int8)
|HBM2 capacity and bandwidth
|32 GiB, 1200 GBps
|Measured min/mean/max power
|90/170/192 W
|TPU Pod size
|4096 chips
|Interconnect topology
|3D mesh
|Peak compute per Pod
|1.1 exaflops (bf16 or int8)
|All-reduce bandwidth per Pod
|1.1 PB/s
|Bisection bandwidth per Pod
|24 TB/s
The following diagram illustrates a TPU v4 chip.
3D mesh and 3D torus
v4 TPUs have a direct connection to the nearest neighboring chips in 3 dimensions, resulting in a 3D mesh of networking connections. When the slice is equal or larger than a single cube, the connections can be configured as a 3D torus. In general, the performance of a 3D configuration will be better than a 3D mesh configuration.
TPU v3
Each v3 TPU chip contains two TensorCores. Each TensorCore has two MXUs, a vector unit, and a scalar unit. The following table shows the key specifications and their values for a v3 TPU Pod.
|Key specifications
|v3 Pod values
|Peak compute per chip
|123 teraflops (bf16)
|HBM2 capacity and bandwidth
|32 GiB, 900 GBps
|Measured min/mean/max power
|123/220/262 W
|TPU Pod size
|1024 chips
|Interconnect topology
|2D torus
|Peak compute per Pod
|126 petaflops (bf16)
|All-reduce bandwidth per Pod
|340 TB/s
|Bisection bandwidth per Pod
|6.4 TB/s
The following diagram illustrates a TPU v3 chip.
Performance benefits of TPU v4 over v3
This section describes the performance benefits of TPU v4
Memory System:
Non Uniform Memory Access (NUMA) is a computer memory architecture for machines that have multiple CPUs. Each CPU has direct access to a block of high-speed memory. A CPU and it's memory is called a NUMA node. NUMA nodes are connected to NUMA nodes that are directly adjacent to each other. A CPU from one NUMA node can access memory in another NUMA node, but this access is slower than accessing memory within a NUMA node.
Software running on a multi-CPU machine can place data needed by a CPU within
its NUMA node, increasing memory throughput. For more information about NUMA,
see
[Non Uniform Memory Access](https://en.wikipedia.org/wiki/Non-uniform_memory_access)
on Wikipedia.
You can take advantage of NUMA-locality benefits by binding your training script to NUMA Node 0.
To enable NUMA node binding:
Install the numactl command line tool.
$ sudo apt-get update $ sudo apt-get install numactl
Use
numactl --cpunodebind=0when launching your training script. This binds your script code to NUMA Node 0.
$ numactl --cpunodebind=0 python3 your-training-script
Enable NUMA node binding if:
- If your workload has a heavy dependence on CPU workloads (for example, image classification, recommendation workloads) regardless of framework.
- If you are using a TPU runtime version without a -pod suffix (for example,
tpu-vm-tf-2.10.0-v4).
Other memory system differences:
- v4 TPU chips have a unified 32-GiB HBM memory space across the entire chip, enabling better coordination between the two on-chip TensorCores.
- Improved HBM performance using latest memory standards and speeds.
- Improved DMA performance profile with built-in support for high-performance striding at 512B granularities.
TensorCores:
- Twice the number of MXUs and a higher clock rate delivering 275 max TFLOPS.
- 2x transposition and permutation bandwidth.
- Load-store memory access model for Common Memory (Cmem).
- Faster MXU weight loading bandwidth and 8-bit mode support to allow lower batch sizes and improved inference latency.
Inter-chip Interconnect:
Six interconnect links per chip to enable network topologies that have smaller network diameters.
Other:
- x16 PCIE gen3 interface to host (direct connect).
- Improved security model.
- Improved energy efficiency.
Performance benefits of TPU v3 over v2
The increased FLOPS per TensorCore and memory capacity in TPU v3 configurations can improve the performance of your models in the following ways:
TPU v3 configurations provide significant performance benefits per TensorCore for compute-bound models. Memory-bound models on TPU v2 configurations might not achieve this same performance improvement if they are also memory-bound on TPU v3 configurations.
In cases where data does not fit into memory on TPU v2 configurations, TPU v3 can provide improved performance and reduced recomputation of intermediate values (rematerialization).
TPU v3 configurations can run new models with batch sizes that did not fit on TPU v2 configurations. For example, TPU v3 might allow deeper ResNets and larger images with RetinaNet.
Models that are nearly input-bound (""infeed"") on TPU v2 because training steps are waiting for input might also be input-bound with Cloud TPU v3. The pipeline performance guide can help you resolve infeed issues.
Cloud TPU VM Architectures
How you interact with the TPU host (and the TPU board) depends upon the TPU VM architecture you're using: TPU Nodes or TPU VMs.
TPU Node Architecture
The TPU Node architecture consists of a user VM that communicates with the TPU host over gRPC. When using this architecture, you cannot directly access the TPU Host, making it difficult to debug training and TPU errors.
TPU VM Architecture
The TPU VM architecture lets you directly connect to the VM physically connected to the TPU device using SSH. You have root access to the VM, so you can run arbitrary code. You can access compiler and runtime debug logs and error messages.",System Architecture | Cloud TPU | Google Cloud,
id,url,body,title,description
196,https://cloud.google.com/tpu/docs/reference/rest/v2/projects.locations.nodes/list,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Query parameters](#body.QUERY_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [Try it!](#try-it)
Lists nodes.
HTTP request
GET https://tpu.googleapis.com/v2/{parent=projects/*/locations/*}/nodes
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
parent
|
Required. The parent resource name.
Query parameters
|Parameters
|
pageSize
|
The maximum number of items to return.
|
pageToken
|
The nextPageToken value returned from a previous List request, if any.
Request body
The request body must be empty.
Response body
Response for
.
[nodes.list](/tpu/docs/reference/rest/v2/projects.locations.nodes/list#google.cloud.tpu.v2.Tpu.ListNodes)
If successful, the response body contains data with the following structure:
|JSON representation
|
{
""nodes"": [
{
object (
|Fields
|
nodes[]
|
The listed nodes.
|
nextPageToken
|
The next page token or empty if none.
|
unreachable[]
|
Locations that could not be reached.
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).",Method: projects.locations.nodes.list | Cloud TPU | Google Cloud,
id,url,body,title,description
74,https://cloud.google.com/tpu/docs/coco-setup,"Downloading, preprocessing, and uploading the COCO dataset
COCO is a large-scale object detection, segmentation, and captioning dataset. Machine learning models that use the COCO dataset include:
- Mask-RCNN
- Retinanet
- ShapeMask
Before you can train a model on a Cloud TPU, you must prepare the training data.
This topic describes how to prepare the
[COCO](http://cocodataset.org) dataset for
models that run on Cloud TPU. The COCO dataset can only be prepared after you
have created a Compute Engine VM. The script used to prepare the data,
download_and_preprocess_coco.sh,
is installed on the VM and must be run on the VM.
After preparing the data by running the
download_and_preprocess_coco.sh
script, you can bring up the Cloud TPU and run the training.
To fully download/preprocess and upload the COCO dataset to a Google Cloud storage bucket takes approximately 2 hours.
In your
[Cloud Shell](https://console.cloud.google.com/), configure
gcloudwith your project ID.
export PROJECT_ID=project-id gcloud config set project ${PROJECT_ID}
In your
[Cloud Shell](https://console.cloud.google.com/), create a Cloud Storage bucket using the following command:
gsutil mb -p ${PROJECT_ID} -c standard -l europe-west4 gs://bucket-name
Launch a Compute Engine VM instance.
This VM instance will only be used to download and preprocess the COCO dataset. Fill in the instance-name with a name of your choosing.
$ gcloud compute tpus execution-groups create \ --vm-only \ --name=instance-name \ --zone=europe-west4-a \ --disk-size=300 \ --machine-type=n1-standard-16 \ --tf-version=2.12.0
Command flag descriptions
vm-only
- Create a VM only. By default the
gcloud compute tpus execution-groupscommand creates a VM and a Cloud TPU.
name
- The name of the Cloud TPU to create.
zone
- The
[zone](https://cloud.google.com/tpu/docs/types-zones)where you plan to create your Cloud TPU.
disk-size
- The size of the hard disk in GB of the VM created by the
gcloud compute tpus execution-groupscommand.
machine-type
- The
[machine type](https://cloud.google.com/compute/docs/machine-types)of the Compute Engine VM to create.
tf-version
- The version of Tensorflow
gcloud compute tpus execution-groupsinstalls on the VM.
-
If you are not automatically logged in to the Compute Engine instance, log in by running the following
sshcommand. When you are logged into the VM, your shell prompt changes from
username@projectnameto
username@vm-name:
$ gcloud compute ssh instance-name --zone=europe-west4-a
Set up two variables, one for the storage bucket you created earlier and one for the directory that holds the training data (DATA_DIR) on the storage bucket.
(vm)$ export STORAGE_BUCKET=gs://bucket-name
(vm)$ export DATA_DIR=${STORAGE_BUCKET}/coco
Install the packages needed to pre-process the data.
(vm)$ sudo apt-get install -y python3-tk && \ pip3 install --user Cython matplotlib opencv-python-headless pyyaml Pillow && \ pip3 install --user ""git+https://github.com/cocodataset/cocoapi#egg=pycocotools&subdirectory=PythonAPI""
Run the
download_and_preprocess_coco.shscript to convert the COCO dataset into a set of TFRecords (
*.tfrecord) that the training application expects.
(vm)$ git clone https://github.com/tensorflow/tpu.git (vm)$ sudo bash tpu/tools/datasets/download_and_preprocess_coco.sh ./data/dir/coco
This installs the required libraries and then runs the preprocessing script. It outputs a number of
*.tfrecordfiles in your local data directory. The COCO download and conversion script takes approximately 1 hour to complete.
Copy the data to your Cloud Storage bucket
After you convert the data into TFRecords, copy them from local storage to your Cloud Storage bucket using the
gsutilcommand. You must also copy the annotation files. These files help validate the model's performance.
(vm)$ gsutil -m cp ./data/dir/coco/*.tfrecord ${DATA_DIR} (vm)$ gsutil cp ./data/dir/coco/raw-data/annotations/*.json ${DATA_DIR}
Clean up the VM resources
Once the COCO dataset has been converted to TFRecords and copied to the DATA_DIR on your Cloud Storage bucket, you can delete the Compute Engine instance.
Disconnect from the Compute Engine instance:
(vm)$ exit
Your prompt should now be
username@projectname, showing you are in the Cloud Shell.
Delete your Compute Engine instance.
$ gcloud compute instances delete instance-name --zone=europe-west4-a","Downloading, preprocessing, and uploading the COCO dataset | Cloud TPU | Google Cloud",
id,url,body,title,description
154,https://cloud.google.com/tpu/docs/reference/rest/v1alpha1/projects.locations.operations,"Resource: Operation
This resource represents a long-running operation that is the result of a network API call.
|JSON representation
|
{ ""name"": string, ""metadata"": { ""@type"": string, field1: ..., ... }, ""done"": boolean, // Union field
|Fields
|
name
|
The server-assigned name, which is only unique within the same service that originally returns it. If you use the default HTTP mapping, the
|
metadata
|
Service-specific metadata associated with the operation. It typically contains progress information and common metadata such as create time. Some services might not provide such metadata. Any method that returns a long-running operation should document the metadata type, if any.
An object containing fields of an arbitrary type. An additional field
|
done
|
If the value is
|Union field
result. The operation result, which can be either an
error or a valid
response. If
done ==
false, neither
error nor
response is set. If
done ==
true, exactly one of
error or
response can be set. Some services might not provide the result.
result can be only one of the following:
|
error
|
The error result of the operation in case of failure or cancellation.
|
response
|
The normal, successful response of the operation. If the original method returns no data on success, such as
An object containing fields of an arbitrary type. An additional field
|
Methods
|
|Starts asynchronous cancellation on a long-running operation.
|
|Deletes a long-running operation.
|
|Gets the latest state of a long-running operation.
|
|Lists operations that match the specified filter in the request.",REST Resource: projects.locations.operations | Cloud TPU | Google Cloud,
id,url,body,title,description
56,https://cloud.google.com/tpu/docs/reference/rest/v1/projects.locations.operations,"Resource: Operation
This resource represents a long-running operation that is the result of a network API call.
|JSON representation
|
{ ""name"": string, ""metadata"": { ""@type"": string, field1: ..., ... }, ""done"": boolean, // Union field
|Fields
|
name
|
The server-assigned name, which is only unique within the same service that originally returns it. If you use the default HTTP mapping, the
|
metadata
|
Service-specific metadata associated with the operation. It typically contains progress information and common metadata such as create time. Some services might not provide such metadata. Any method that returns a long-running operation should document the metadata type, if any.
An object containing fields of an arbitrary type. An additional field
|
done
|
If the value is
|Union field
result. The operation result, which can be either an
error or a valid
response. If
done ==
false, neither
error nor
response is set. If
done ==
true, exactly one of
error or
response can be set. Some services might not provide the result.
result can be only one of the following:
|
error
|
The error result of the operation in case of failure or cancellation.
|
response
|
The normal, successful response of the operation. If the original method returns no data on success, such as
An object containing fields of an arbitrary type. An additional field
|
Methods
|
|Starts asynchronous cancellation on a long-running operation.
|
|Deletes a long-running operation.
|
|Gets the latest state of a long-running operation.
|
|Lists operations that match the specified filter in the request.",REST Resource: projects.locations.operations | Cloud TPU | Google Cloud,
id,url,body,title,description
29,https://cloud.google.com/tpu/docs/tutorials/resnet-rs-2.x,"This tutorial shows you how to train a Keras ResNet-RS model on Cloud TPU using
tf.distribute.TPUStrategy. For more information about ResNet-RS,
see
[Revisiting ResNets: Improved Training and Scaling Strategies](https://arxiv.org/abs/2103.07579).
If you are not familiar with Cloud TPU, it is strongly recommended that you
go through the
[quickstart](https://cloud.google.com/tpu/docs/quickstart) to learn how to
create a TPU and a Compute Engine VM.
Objectives
- Create a Cloud Storage bucket to hold your dataset and model output.
- Prepare a fake imagenet dataset that is similar to the ImageNet dataset.
- Run the training job.
- Verify the output results.
Costs
In this document, you use the following billable components of Google Cloud:
- Compute Engine
- Cloud TPU
- Cloud Storage
To generate a cost estimate based on your projected usage,
use the
[pricing calculator](/products/calculator).
[free trial](/free-trial).
Before you begin
Before starting this tutorial, check that your Google Cloud project is correctly set up.
-
Sign in to your Google Cloud account. If you're new to
Google Cloud,
[create an account](https://console.cloud.google.com/freetrial)to evaluate how our products perform in real-world scenarios. New customers also get $300 in free credits to run, test, and deploy workloads.
-
In the Google Cloud console, on the project selector page, select or
[create a Google Cloud project](/resource-manager/docs/creating-managing-projects).
-
[Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console).
-
In the Google Cloud console, on the project selector page, select or
[create a Google Cloud project](/resource-manager/docs/creating-managing-projects).
-
[Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console).
This walkthrough uses billable components of Google Cloud. Check the
[Cloud TPU pricing page](/tpu/docs/pricing) to
estimate your costs. Be sure to
[clean up](#clean-up)
resources you create when you've finished with them to avoid unnecessary
charges.
Set up your resources
This section provides information on setting up Cloud Storage bucket, VM, and Cloud TPU resources for tutorials.
Open a Cloud Shell window.
Create a variable for your project's ID.
export PROJECT_ID=project-id
Configure Google Cloud CLI to use the project where you want to create Cloud TPU.
gcloud config set project ${PROJECT_ID}
The first time you run this command in a new Cloud Shell VM, an
Authorize Cloud Shellpage is displayed. Click
Authorizeat the bottom of the page to allow
gcloudto make Google Cloud API calls with your credentials.
Create a Service Account for the Cloud TPU project.
gcloud beta services identity create --service tpu.googleapis.com --project $PROJECT_ID
The command returns a Cloud TPU Service Account with following format:
service-PROJECT_NUMBER@cloud-tpu.iam.gserviceaccount.com
Create a Cloud Storage bucket using the following command:
gsutil mb -p ${PROJECT_ID} -c standard -l europe-west4 gs://bucket-name
This Cloud Storage bucket stores the data you use to train your model and the training results. The
gcloud compute tpus execution-groupstool used in this tutorial sets up default permissions for the Cloud TPU Service Account you set up in the previous step. If you want finer-grain permissions, review the
[access level permissions](/tpu/docs/storage-buckets).
The bucket location must be in the same region as your Compute Engine (VM) and your Cloud TPU node.
Launch a Compute Engine VM using the
gcloudcommand.
$ gcloud compute tpus execution-groups create \ --vm-only \ --name=resnet-rs-tutorial \ --zone=europe-west4-a \ --disk-size=300 \ --machine-type=n1-standard-16 \ --tf-version=2.12.0
Command flag descriptions
vm-only
- Create a VM only, do not create a TPU.
name
- The name of the TPU to create.
zone
- The
[zone](https://cloud.google.com/tpu/docs/types-zones)in which to create your Cloud TPU.
disk-size
- The size of the hard disk in GB of the VM created by the
gcloud compute tpus execution-groupscommand.
machine-type
- The
[machine type](https://cloud.google.com/compute/docs/machine-types)of the Compute Engine VM to create.
For more information on the
gcloudcommand, see the
[gcloud Reference](/sdk/gcloud/reference).
-
When prompted, press y to create your Cloud TPU resources.
When the
gcloud compute tpus execution-groupscommand has finished executing, verify that your shell prompt has changed from
username@projectnameto
username@vm-name. This change shows that you are now logged into your Compute Engine VM.
gcloud compute ssh resnet-rs-tutorial --zone=europe-west4-a
As you continue these instructions, run each command that begins with
(vm)$in your Compute Engine instance.
Install necessary packages.
$ pip3 install tensorflow-text==2.8.1 --no-deps
Set Cloud Storage bucket variables
Set up the following environment variables, replacing bucket-name with the name of your Cloud Storage bucket:
(vm)$ export STORAGE_BUCKET=gs://bucket-name
(vm)$ export MODEL_DIR=${STORAGE_BUCKET}/resnet-rs-2x (vm)$ export IMAGENET_DIR=gs://cloud-tpu-test-datasets/fake_imagenet (vm)$ export PYTHONPATH=/usr/share/models (vm)$ export TPU_NAME=resnet-rs-tutorial
The training application expects your training data to be accessible in Cloud Storage. The training application also uses your Cloud Storage bucket to store checkpoints during training.
Cloud TPU single device training and evaluation
ImageNet is an image database. The images in the database are organized into a hierarchy, with each node of the hierarchy depicted by hundreds and thousands of images.
This tutorial uses a demonstration version of the full ImageNet dataset, referred to as fake_imagenet. This demonstration version allows you to test the tutorial, while reducing the storage and time requirements typically associated with running a model against the full ImageNet database.
The fake_imagenet dataset is at this location on Cloud Storage:
gs://cloud-tpu-test-datasets/fake_imagenet
The fake_imagenet dataset is only useful for understanding how to use a Cloud TPU and validating end-to-end performance. The accuracy numbers and saved model will not be meaningful.
For information on how to download and process the full ImageNet dataset, see
[Downloading, preprocessing, and uploading the ImageNet dataset](/tpu/docs/imagenet-setup).
Create a Cloud TPU using the
gcloudcommand.
$ gcloud compute tpus execution-groups create \ --tpu-only \ --accelerator-type=v3-8 \ --name=resnet-rs-tutorial \ --zone=europe-west4-a \ --tf-version=2.12.0
Set the
TPU_NAMEname variable.
(vm)$ export TPU_NAME=resnet-rs-tutorial
Run the training script.
(vm)$ python3 /usr/share/models/official/vision/beta/train.py \ --experiment=resnet_rs_imagenet \ --mode=train_and_eval \ --model_dir=$MODEL_DIR \ --tpu=$TPU_NAME \ --config_file=/usr/share/models/official/vision/beta/configs/experiments/image_classification/imagenet_resnetrs50_i160.yaml \ --params_override=""task.train_data.input_path=$IMAGENET_DIR/train*, task.validation_data.input_path=$IMAGENET_DIR/valid*, trainer.train_steps=100""
Command flag descriptions
experiment
- The name of experiment to run.
mode
- The mode in which to run the script. Valid values are: `train`, `eval` , or `train_and_eval`.
model_dir
- The Cloud Storage bucket where checkpoints and summaries are stored during training. You can use an existing folder to load previously generated checkpoints created on a TPU of the same size and TensorFlow version.
tpu
- The name of the TPU to use.
config_file
- The path to a script configuration file.
params_override
- Override settings set in the script configuration file.
-
This will train ResNet-RS for 100 training steps and will complete on a v3-8 TPU node in less than 5 minutes. The training script output should include text like:
{ 'train_loss': 1.435225, 'train_accuracy': 0.00084427913 }
The training script also performs evaluation. The evaluation output should contain text like this:
Run stats: { 'eval_loss': 0.861013, 'eval_acc': 0.001, 'train_loss': 1.435225, 'train_acc': 0.00084427913, 'step_timestamp_log': [ 'BatchTimestamp<batch_index: 0, timestamp: 1606330585.7613473>', 'BatchTimestamp<batch_index: 500, timestamp: 1606330883.8486104>', 'BatchTimestamp<batch_index: 1000, timestamp: 1606331119.515312>', 'BatchTimestamp<batch_index: 1251, timestamp: 1606331240.7516596>' ], 'train_finish_time': 1606331296.395158, 'avg_exp_per_second': 1951.6983246161021 }
To train the ResNet-RS model to convergence, omit the
trainer.train_steps=100
argument as shown in the following script. Training and evaluation are done
together.
(vm)$ python3 /usr/share/models/official/vision/beta/train.py \
--experiment=resnet_rs_imagenet \
--mode=train_and_eval \
--model_dir=$MODEL_DIR \
--tpu=$TPU_NAME \
--config_file=/usr/share/models/official/vision/beta/configs/experiments/image_classification/imagenet_resnetrs50_i160.yaml \
--params_override=""task.train_data.input_path=$IMAGENET_DIR/train*, task.validation_data.input_path=$IMAGENET_DIR/valid*""
Command flag descriptions
experiment
- The name of experiment to run.
mode
- The mode in which to run the script. Valid values are: `train`, `eval`, or `train_and_eval`.
model_dir
- The Cloud Storage bucket where checkpoints and summaries are stored during training. You can use an existing folder to load previously generated checkpoints created on a TPU of the same size and TensorFlow version.
tpu
- The name of the TPU to use.
config_file
- The path to a script configuration file.
params_override
- Override settings set in the script configuration file.
Since the training and evaluation was done on the fake_imagenet dataset, the output results do not reflect actual output that would appear if the training and evaluation was performed on a real dataset.
At this point, you can either conclude this tutorial and
[clean up](#clean-up)
your Google Cloud resources, or you can further explore running the model on Cloud TPU
Pods.
Use larger models
ResNet-RS provides a family of models of different sizes with larger models
typically being more accurate at the cost of more compute. For more information,
see
[Revisiting ResNets: Improved Training and Scaling Strategies](https://arxiv.org/abs/2103.07579).
You can choose the size of the model to train by changing the config_file in the following command.
(vm)$ python3 /usr/share/models/official/vision/beta/train.py \
--experiment=resnet_rs_imagenet \
--mode=train_and_eval \
--model_dir=$MODEL_DIR \
--tpu=$TPU_NAME \
--config_file=/usr/share/models/official/vision/beta/configs/experiments/image_classification/imagenet_resnetrs200_i256.yaml \
--params_override=""task.train_data.input_path=$IMAGENET_DIR/train*, task.validation_data.input_path=$IMAGENET_DIR/valid*""
The available configs are in
/usr/share/models/official/vision/beta/configs/experiments/
on your VM.
Scale your model with Cloud TPU Pods
Training your model on Cloud TPU Pods may require some changes
to your training script. For information, see
[Training on TPU Pods](/tpu/docs/training-on-tpu-pods).
You can get results faster by scaling your model with Cloud TPU Pods. The fully supported ResNet-RS-50 model can work with the following Pod slices:
- v2-32
- v3-32
With Cloud TPU Pods, training and evaluation are done together.
Training with Cloud TPU Pods
Delete the Cloud TPU resource you created for training the model on a single device.
(vm)$ gcloud compute tpus execution-groups delete resnet-rs-tutorial \ --zone=europe-west4-a \ --tpu-only
Create a new Cloud TPU resource for the Pod, using the
accelerator-typeparameter to specify the Pod slice you want to use. For example, the following command uses a v3-32 Pod slice.
(vm)$ gcloud compute tpus execution-groups create --name=resnet-rs-tutorial \ --accelerator-type=v3-32 \ --zone=europe-west4-a \ --tf-version=2.12.0 \ --tpu-only
Command flag descriptions
name
- The name of the Cloud TPU to create.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
zone
- The
[zone](/tpu/docs/types-zones)where you plan to create your Cloud TPU.
tf-version
- The version of Tensorflow
gcloudinstalls on the VM.
tpu-only
- Create a Cloud TPU only. By default the
gcloudcommand creates a VM and a Cloud TPU.
-
Run the training script.
(vm)$ python3 /usr/share/models/official/vision/beta/train.py \ --experiment=resnet_rs_imagenet \ --mode=train_and_eval \ --model_dir=$MODEL_DIR \ --tpu=$TPU_NAME \ --config_file=/usr/share/models/official/vision/beta/configs/experiments/image_classification/imagenet_resnetrs50_i160.yaml \ --params_override=""task.train_data.input_path=$IMAGENET_DIR/train*, task.validation_data.input_path=$IMAGENET_DIR/valid*, trainer.train_steps=100""
Command flag descriptions
experiment
- The name of experiment to run.
mode
- The mode in which to run the script. Valid values are: `train`, `eval`, or `train_and_eval`.
model_dir
- The Cloud Storage bucket where checkpoints and summaries are stored during training. You can use an existing folder to load previously generated checkpoints created on a TPU of the same size and TensorFlow version.
tpu
- The name of the TPU to use.
config_file
- The path to a script configuration file.
params_override
- Override settings set in the script configuration file.
-
This will train ResNet-RS for 100 training steps and will complete on a v3-8 TPU node in less than 5 minutes. The training script output should include text like:
{ 'train_loss': 1.435225, 'train_accuracy': 0.00084427913 }
The training script also performs evaluation. The evaluation output should contain text like this:
Run stats: { 'eval_loss': 0.861013, 'eval_acc': 0.001, 'train_loss': 1.435225, 'train_acc': 0.00084427913, 'step_timestamp_log': [ 'BatchTimestamp<batch_index: 0, timestamp: 1606330585.7613473>', 'BatchTimestamp<batch_index: 500, timestamp: 1606330883.8486104>', 'BatchTimestamp<batch_index: 1000, timestamp: 1606331119.515312>', 'BatchTimestamp<batch_index: 1251, timestamp: 1606331240.7516596>' ], 'train_finish_time': 1606331296.395158, 'avg_exp_per_second': 1951.6983246161021 }
Training and evaluation are done together. Each epoch has 1251 steps for a total of 112590 training steps and 48 evaluation steps.
Clean up
To avoid incurring charges to your Google Cloud account for the resources used in this tutorial, either delete the project that contains the resources, or keep the project and delete the individual resources.
Disconnect from the Compute Engine instance, if you have not already done so:
(vm)$ exit
Your prompt should now be
username@projectname, showing you are in the Cloud Shell.
In your Cloud Shell, use the following command to delete your Compute Engine VM and Cloud TPU:
$ gcloud compute tpus execution-groups delete resnet-rs-tutorial \ --zone=europe-west4-a
Verify the resources have been deleted by running
gcloud compute tpus execution-groups list. The deletion might take several minutes. A response like the one below indicates your instances have been successfully deleted.
$ gcloud compute tpus execution-groups list \ --zone=europe-west4-a
You should see an empty list of TPUs like the following:
NAME STATUS
Delete your Cloud Storage bucket using
gsutilas shown below. Replace bucket-name with the name of your Cloud Storage bucket.
$ gsutil rm -r gs://bucket-name
What's next
The TensorFlow Cloud TPU tutorials generally train the model using a
sample dataset. The results of this training are not usable
for inference. To use a model for inference, you can train the data on a
publicly available dataset or your own data set. TensorFlow models
trained on Cloud TPUs generally require datasets to be in
[TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) format.
You can use the
[dataset conversion tool sample](https://cloud.google.com/tpu/docs/classification-data-conversion) to convert an image classification dataset into TFRecord format. If you are not using an image classification model, you will have to convert your dataset to [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) format yourself. For more information, see [TFRecord and tf.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord).
Hyperparameter tuning
To improve the model's performance with your dataset, you can tune
the model's hyperparameters. You can find information about hyperparameters
common to all TPU supported models on
[GitHub](https://github.com/tensorflow/tpu/tree/master/models/hyperparameters).
Information about model-specific hyperparameters can be found in the
[source code](https://github.com/tensorflow/tpu/tree/master/models/official)
for each model. For more information on hyperparameter tuning, see
[Overview of hyperparameter tuning](https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview),
[Using the Hyperparameter tuning service](https://cloud.google.com/ai-platform/training/docs/using-hyperparameter-tuning),
and [Tune hyperparameters](https://developers.google.com/machine-learning/guides/text-classification/step-5).
Inference
Once you have trained your model you can use it for inference
(also called prediction).
[AI Platform](https://cloud.google.com/ai-platform/docs/technical-overview)
is a cloud-based solution for developing,
[training](https://cloud.google.com/ai-platform/training/docs),
and [deploying](https://cloud.google.com/ai-platform/prediction/docs/deploying-models)
machine learning models. Once a model is deployed, you can use the
[AI Platform Prediction service](https://cloud.google.com/ai-platform/prediction/docs).",Training on Cloud TPU (TF 2.x) | Google Cloud,
id,url,body,title,description
192,https://cloud.google.com/tpu/docs/reference/rest/v1alpha1/projects.locations.nodes/reimage,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [Try it!](#try-it)
Reimages a node's OS.
HTTP request
POST https://tpu.googleapis.com/v1alpha1/{name=projects/*/locations/*/nodes/*}:reimage
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
The resource name.
Request body
The request body contains data with the following structure:
|JSON representation
|
{ ""tensorflowVersion"": string }
|Fields
|
tensorflowVersion
|
The version for reimage to create.
Response body
If successful, the response body contains an instance of
.
[Operation](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse#Operation)
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).",Method: projects.locations.nodes.reimage | Cloud TPU | Google Cloud,
id,url,body,title,description
130,https://cloud.google.com/tpu/docs/reference/rest/v2alpha1/projects.locations.queuedResources/reset,"Method: projects.locations.queuedResources.reset
Stay organized with collections
Save and categorize content based on your preferences.
Resets a QueuedResource TPU instance
HTTP request
POST https://tpu.googleapis.com/v2alpha1/{name=projects/*/locations/*/queuedResources/*}:reset
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
string
Required. The name of the queued resource.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[Operation](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse#Operation)
Authorization scopes
Requires one of the following OAuth scopes:
-
https://www.googleapis.com/auth/cloud.tpu
-
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Method: projects.locations.queuedResources.reset | Cloud TPU | Google Cloud,
id,url,body,title,description
129,https://cloud.google.com/tpu/docs/tutorials/resnet-2.x,"This tutorial shows you how to train a Keras ResNet model on Cloud TPU using
tf.distribute.TPUStrategy.
If you are not familiar with Cloud TPU, it is strongly recommended that you
go through the
[quickstart](https://cloud.google.com/tpu/docs/quick-starts) for your framework
to learn how to create a TPU and a Compute Engine VM.
Objectives
- Create a Cloud Storage bucket to hold your dataset and model output.
- Prepare a fake imagenet dataset that is similar to the ImageNet dataset.
- Run the training job.
- Verify the output results.
Costs
In this document, you use the following billable components of Google Cloud:
- Compute Engine
- Cloud TPU
- Cloud Storage
To generate a cost estimate based on your projected usage,
use the
[pricing calculator](/products/calculator).
[free trial](/free-trial).
Before you begin
Before starting this tutorial, check that your Google Cloud project is correctly set up.
-
Sign in to your Google Cloud account. If you're new to
Google Cloud,
[create an account](https://console.cloud.google.com/freetrial)to evaluate how our products perform in real-world scenarios. New customers also get $300 in free credits to run, test, and deploy workloads.
-
In the Google Cloud console, on the project selector page, select or
[create a Google Cloud project](/resource-manager/docs/creating-managing-projects).
-
[Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console).
-
In the Google Cloud console, on the project selector page, select or
[create a Google Cloud project](/resource-manager/docs/creating-managing-projects).
-
[Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console).
This walkthrough uses billable components of Google Cloud. Check the
[Cloud TPU pricing page](/tpu/docs/pricing) to
estimate your costs. Be sure to
[clean up](#clean-up)
resources you create when you've finished with them to avoid unnecessary
charges.
Cloud TPU single device training
This section provides information on setting up Cloud Storage bucket, VM, and Cloud TPU resources for single device training.
Open a Cloud Shell window.
Create a variable for your project's ID.
$ export PROJECT_ID=project-id
Configure the Google Cloud CLI to use the project where you want to create Cloud TPU.
$ gcloud config set project ${PROJECT_ID}
The first time you run this command in a new Cloud Shell VM, an
Authorize Cloud Shellpage is displayed. Click
Authorizeat the bottom of the page to allow
gcloudto make API calls with your credentials.
Create a Service Account for the Cloud TPU project.
Service accounts allow the Cloud TPU service to access other Google Cloud services.
$ gcloud beta services identity create --service tpu.googleapis.com --project $PROJECT_ID
The command returns a Cloud TPU Service Account with following format:
service-PROJECT_NUMBER@cloud-tpu.iam.gserviceaccount.com
Create a Cloud Storage bucket using the following command:
$ gsutil mb -p ${PROJECT_ID} -c standard -l europe-west4 gs://bucket-name
This Cloud Storage bucket stores the data you use to train your model and the training results. The
gcloudcommand used in this tutorial to set up the TPU also sets up default permissions for the Cloud TPU Service Account you set up in the previous step. If you want finer-grain permissions, review the
[access level permissions](/tpu/docs/storage-buckets).
Prepare your dataset or use fake_imagenet
ImageNet is an image database. The images in the database are organized into a hierarchy, with each node of the hierarchy depicted by hundreds and thousands of images.
This tutorial uses a demonstration version of the full ImageNet dataset, referred to as fake_imagenet. This demonstration version allows you to test the tutorial, while reducing the storage and time requirements typically associated with running a model against the full ImageNet database.
The fake_imagenet dataset is at this location on Cloud Storage:
gs://cloud-tpu-test-datasets/fake_imagenet
The fake_imagenet dataset is only useful for understanding how to use a Cloud TPU and validating end-to-end performance. The accuracy numbers and saved model will not be meaningful.
If you want to use the full ImageNet dataset, see
[Downloading, preprocessing, and uploading the ImageNet dataset](/tpu/docs/imagenet-setup).
Launch TPU resources using the
gcloudcommand. The command you use depends on whether you are using TPU VMs or TPU nodes. For more information on the two VM architecture, see
[System Architecture](/tpu/docs/system-architecture-tpu-vm).
TPU VM
$ gcloud compute tpus tpu-vm create resnet-tutorial \ --zone=us-central2-b \ --accelerator-type=v4-8 \ --version=tpu-vm-tf-2.15.0-pjrt
Command flag descriptions
zone
- The
[zone](/tpu/docs/types-zones)where you plan to create your Cloud TPU.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
version
- The Cloud TPU
[software version](/tpu/docs/supported-tpu-versions#tpu_software_versions).
TPU Node
gcloud compute tpus execution-groups create \ --project=${PROJECT_ID} \ --zoneus-central2-b \ --name=resnet-tutorial \ --disk-size=300 \ --machine-type=n1-standard-16 \ --accelerator-type=v3-8 \ --tf-version=2.12.0
Command flag descriptions
project
- Your Google Cloud project ID
zone
- The
[zone](/tpu/docs/types-zones)where you plan to create your Cloud TPU.
name
- The name of the Cloud TPU to create.
disk-size
- The size of the hard disk in GB of the VM created by the
gcloudcommand.
machine-type
- The
[machine type](/compute/docs/machine-types)of the Compute Engine VM to create.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
tf-version
- The version of Tensorflow
gcloudinstalls on the VM.
For more information on the
gcloudcommand, see the
[gcloud Reference](/sdk/gcloud/reference).
-
If you are not automatically logged in to the Compute Engine instance, log in by running the following
sshcommand. When you are logged into the VM, your shell prompt changes from
username@projectnameto
username@vm-name:
TPU VM
$ gcloud compute tpus tpu-vm ssh resnet-tutorial --zone=us-central2-b
TPU Node
$ gcloud compute ssh resnet-tutorial --zone=us-central2-b
As you continue these instructions, run each command that begins with
(vm)$in your VM session window.
Set the Cloud TPU name variable.
TPU VM
(vm)$ export TPU_NAME=local
TPU Node
(vm)$ export TPU_NAME=resnet-tutorial
Set Cloud Storage bucket variables
Set up the following environment variables, replacing bucket-name with the name of your Cloud Storage bucket:
(vm)$ export STORAGE_BUCKET=gs://bucket-name
(vm)$ export MODEL_DIR=${STORAGE_BUCKET}/resnet-2x (vm)$ export DATA_DIR=gs://cloud-tpu-test-datasets/fake_imagenet
The training application expects your training data to be accessible in Cloud Storage. The training application also uses your Cloud Storage bucket to store checkpoints during training.
- If you set
--version=tpu-vm-tf-2.15.0-pjrtwhen creating your TPU, set the following environment variables to enable the PJRT runtime:
(vm)$ export NEXT_PLUGGABLE_DEVICE_USE_C_API=true (vm)$ export TF_PLUGGABLE_DEVICE_LIBRARY_PATH=/lib/libtpu.so
Install TensorFlow requirements.
The command you use depends on whether you are using TPU VMs or TPU Nodes.
TPU VM
(vm)$ pip3 install -r /usr/share/tpu/models/official/requirements.txt
TPU Node
(vm)$ pip3 install --user -r /usr/share/models/official/requirements.txt
The ResNet training script requires an extra package. Install it now:
(vm)$ pip3 install --user tensorflow-model-optimization>=0.1.3
Change to directory that stores the model:
TPU VM
(vm)$ cd /usr/share/tpu/tensorflow/resnet50_keras
TPU Node
(vm)$ cd /usr/share/models
Set the
PYTHONPATHenvironment variable:
TPU VM
(vm)$ /usr/share/tpu/tensorflow/resnet50_keras""
TPU Node
(vm)$ export PYTHONPATH=""${PYTHONPATH}:/usr/share/models""
Run the training script. This uses a fake_imagenet dataset and trains ResNet for 100 steps.
TPU VM
(vm)$ resnet50.py --tpu=local --data=gs://cloud-tpu-test-datasets/fake_imagenet
Command flag descriptions
tpu
- The name of your TPU.
data
- Specifies the directory where checkpoints and summaries are stored
during model training. If the folder is missing, the program creates
one. When using a Cloud TPU, the
model_dirmust be a Cloud Storage path (
gs://...). You can reuse an existing folder to load current checkpoint data and to store additional checkpoints as long as the previous checkpoints were created using TPU of the same size and TensorFlow version.
TPU Node
(vm)$ python3 official/vision/train.py \ --tpu=${TPU_NAME} \ --experiment=resnet_imagenet \ --mode=train_and_eval \ --config_file=official/vision/configs/experiments/image_classification/imagenet_resnet50_tpu.yaml \ --model_dir=${MODEL_DIR} \ --params_override=""task.train_data.input_path=${DATA_DIR}/train*, task.validation_data.input_path=${DATA_DIR}/validation*,task.train_data.global_batch_size=2048,task.validation_data.global_batch_size=2048,trainer.train_steps=100""
Command flag descriptions
tpu
- The name of your TPU.
model_dir
- Specifies the directory where checkpoints and summaries are stored
during model training. If the folder is missing, the program creates
one. When using a Cloud TPU, the
model_dirmust be a Cloud Storage path (
gs://...). You can reuse an existing folder to load current checkpoint data and to store additional checkpoints as long as the previous checkpoints were created using TPU of the same size and TensorFlow version.
-
This will train ResNet for 100 steps and will complete on a v3-8 TPU node in approximately 3 minutes. At the end of the 100 steps, output similar to the following appears:
I0624 17:04:26.974905 140457742666816 controller.py:290] eval | step: 100 | eval time: 23.3 sec | output: {'accuracy': 0.0010141226, 'top_5_accuracy': 0.0051457332, 'validation_loss': 8.448798} eval | step: 100 | eval time: 23.3 sec | output: {'accuracy': 0.0010141226, 'top_5_accuracy': 0.0051457332, 'validation_loss': 8.448798}
You have now completed the single-device training example. Use the following steps to delete the current single-device TPU resources.
Disconnect from the Compute Engine instance:
(vm)$ exit
Your prompt should now be
username@projectname, showing you are in the Cloud Shell.
Delete the TPU resource.
TPU VM
$ gcloud compute tpus tpu-vm delete resnet-tutorial \ --zone=us-central2-b
Command flag descriptions
zone
- The
[zone](https://cloud.google.com/tpu/docs/types-zones)where your Cloud TPU resided.
TPU Node
$ gcloud compute tpus execution-groups delete resnet-tutorial \ --zone=us-central2-b
Command flag descriptions
zone
- The
[zone](https://cloud.google.com/tpu/docs/types-zones)that contains the TPU to delete.
-
At this point, you can either conclude this tutorial and
[clean up](#cleanup),
or you can continue and explore running the model on Cloud TPU Pods.
Scaling your model with Cloud TPU Pods
Training your model on Cloud TPU Pods may require some changes
to your training script. For information, see
[Training on TPU Pods](/tpu/docs/training-on-tpu-pods).
TPU Pod training
This section provides information on setting up a Cloud Storage bucket and Cloud TPU resources for Pod training.
Open a Cloud Shell window.
Create a variable for your project's ID.
export PROJECT_ID=project-id
Configure Google Cloud CLI to use the project where you want to create Cloud TPU.
gcloud config set project ${PROJECT_ID}
The first time you run this command in a new Cloud Shell VM, an
Authorize Cloud Shellpage is displayed. Click
Authorizeat the bottom of the page to allow
gcloudto make Google Cloud API calls with your credentials.
Create a Service Account for the Cloud TPU project.
gcloud beta services identity create --service tpu.googleapis.com --project $PROJECT_ID
The command returns a Cloud TPU Service Account with following format:
service-PROJECT_NUMBER@cloud-tpu.iam.gserviceaccount.com
Create a Cloud Storage bucket using the following command or use a bucket you created earlier for your project:
gsutil mb -p ${PROJECT_ID} -c standard -l us-central2 gs://bucket-name
This Cloud Storage bucket stores the data you use to train your model and the training results. The
gcloudcommand used in this tutorial sets up default permissions for the Cloud TPU Service Account you set up in the previous step. If you want finer-grain permissions, review the
[access level permissions](/tpu/docs/storage-buckets).
The bucket location must be in the same region as your TPU VM.
Prepare your dataset or use fake_imagenet
ImageNet is an image database. The images in the database are organized into a hierarchy, with each node of the hierarchy depicted by hundreds and thousands of images.
The default Pod training accesses a demonstration version of the full ImageNet dataset, referred to as fake_imagenet. This demonstration version allows you to test Pod training, while reducing the storage and time requirements typically associated with training a model against the full ImageNet database.
The fake_imagenet dataset is only useful for understanding how to use a Cloud TPU and validating end-to-end performance. The accuracy numbers and saved model will not be meaningful.
If you want to use the full ImageNet dataset, see
[Downloading, preprocessing, and uploading the ImageNet dataset](/tpu/docs/imagenet-setup).
Launch your Cloud TPU resources using the
gcloudcommand.
The command you use depends on whether you are using a TPU VM or a TPU node. For more information on the two VM architecture, see
[System Architecture](/tpu/docs/system-architecture-tpu-vm). For more information on the
gcloudcommand, see the
[gcloud Reference](/sdk/gcloud/reference). This tutorial specifies a v3-32 Pod. For other Pod options, see the [available TPU types page](/tpu/docs/supported-tpu-configurations).
TPU VM
$ gcloud compute tpus tpu-vm create resnet-tutorial \ --zone=us-central2-b \ --accelerator-type=4-32 \ --version=tpu-vm-tf-2.15.0-pod-pjrt
Command flag descriptions
zone
- The
[zone](/tpu/docs/types-zones)where you plan to create your Cloud TPU.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
version
- The Cloud TPU
[software version](/tpu/docs/supported-tpu-versions#tpu_software_versions).
TPU Node
$ gcloud compute tpus execution-groups create \ --zone=us-central2-b \ --name=resnet-tutorial \ --accelerator-type=v3-32 \ --tf-version=2.12.0
-
If you are not automatically logged in to the Compute Engine instance, log in by running the following
sshcommand. When you are logged into the VM, your shell prompt changes from
username@projectnameto
username@vm-instance-name:
TPU VM
$ gcloud compute tpus tpu-vm ssh resnet-tutorial --zone=us-central2-b
TPU Node
$ gcloud compute ssh resnet-tutorial --zone=us-central2-b
As you continue these instructions, run each command that begins with
(vm)$in your VM session window.
Export Cloud TPU setup variables:
(vm)$ export ZONE=us-central2-b (vm)$ export STORAGE_BUCKET=gs://bucket-name
(vm)$ export TPU_NAME=resnet-tutorial (vm)$ export DATA_DIR=gs://cloud-tpu-test-datasets/fake_imagenet (vm)$ export MODEL_DIR=${STORAGE_BUCKET}/resnet-2x-pod
The training application expects your training data to be accessible in Cloud Storage. The training application also uses your Cloud Storage bucket to store checkpoints during training.
The ResNet training script requires an extra package. Install it now.:
TPU VM
(vm)$ pip3 install --user tensorflow-model-optimization>=0.1.3
TPU Node
(vm)$ pip3 install --user tensorflow-model-optimization>=0.1.3
Install TensorFlow requirements.
The command you use depends on whether you are using TPU VMs or TPU Nodes.
TPU VM
(vm)$ pip3 install -r /usr/share/tpu/models/official/requirements.txt
TPU Node
(vm)$ pip3 install --user -r /usr/share/models/official/requirements.txt
Set the
PYTHONPATHenvironment variable:
TPU VM
(vm)$ export PYTHONPATH=""PYTHONPATH=/usr/share/tpu/tensorflow/resnet50_keras"" (vm)$ export TPU_LOAD_LIBRARY=0
TPU Node
(vm)$ export PYTHONPATH=""${PYTHONPATH}:/usr/share/models""
Change to directory that stores the model:
TPU VM
(vm)$ cd /usr/share/tpu/tensorflow/resnet50_keras
TPU Node
(vm)$ cd /usr/share/models
Train the model.
(vm)$ resnet50.py --tpu=${TPU_NAME} --data=gs://cloud-tpu-test-datasets/fake_imagenet
Command flag descriptions
tpu
- The name of your TPU.
data
- Specifies the directory where checkpoints and summaries are stored
during model training. If the folder is missing, the program creates
one. When using a Cloud TPU, the
model_dirmust be a Cloud Storage path (
gs://...). You can reuse an existing folder to load current checkpoint data and to store additional checkpoints as long as the previous checkpoints were created using Cloud TPU of the same size and TensorFlow version.
-
This procedure trains the model on the fake_imagenet dataset to 100 training steps and 13 evaluation steps. This training takes approximately 2 minutes on a v3-32 Cloud TPU. When the training and evaluation complete, messages similar to the following appears:
{'accuracy': 0.0009716797, 'learning_rate': 0.10256411, 'top_5_accuracy': 0.0049560545, 'training_loss': 8.5587225} train | step: 100 | steps/sec: 1.2 | output: {'accuracy': 0.0009716797, 'learning_rate': 0.10256411, 'top_5_accuracy': 0.0049560545, 'training_loss': 8.5587225} eval | step: 100 | eval time: 24.8 sec | output: {'accuracy': 0.0010141226, 'top_5_accuracy': 0.004356971, 'validation_loss': 8.50038} eval | step: 100 | eval time: 24.8 sec | output: {'accuracy': 0.0010141226, 'top_5_accuracy': 0.004356971, 'validation_loss': 8.50038}
Clean up
To avoid incurring charges to your Google Cloud account for the resources used in this tutorial, either delete the project that contains the resources, or keep the project and delete the individual resources.
Disconnect from the Compute Engine instance, if you have not already done so:
(vm)$ exit
Your prompt should now be
username@projectname, showing you are in the Cloud Shell.
Delete your Cloud TPU and Compute Engine resources. The command you use to delete your resources depends upon whether you are using TPU VMs or TPU Nodes. For more information, see
[System Architecture](/tpu/docs/system-architecture-tpu-vm).
TPU VM
$ gcloud compute tpus tpu-vm delete resnet-tutorial \ --zone=us-central2-b
TPU Node
$ gcloud compute tpus execution-groups delete resnet-tutorial \ --zone=us-central2-b
Verify the resources have been deleted by running
gcloud compute tpus execution-groups list. The deletion might take several minutes. The output from the following command should not include any of the TPU resources created in this tutorial:
TPU VM
$ gcloud compute tpus tpu-vm list --zone=us-central2-b
TPU Node
$ gcloud compute tpus execution-groups list --zone=us-central2-b
Run
gsutilas shown, replacing bucket-name with the name of the Cloud Storage bucket you created for this tutorial:
$ gsutil rm -r gs://bucket-name
What's next
The TensorFlow Cloud TPU tutorials generally train the model using a
sample dataset. The results of this training are not usable
for inference. To use a model for inference, you can train the data on a
publicly available dataset or your own data set. TensorFlow models
trained on Cloud TPUs generally require datasets to be in
[TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) format.
You can use the
[dataset conversion tool sample](https://cloud.google.com/tpu/docs/classification-data-conversion) to convert an image classification dataset into TFRecord format. If you are not using an image classification model, you will have to convert your dataset to [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) format yourself. For more information, see [TFRecord and tf.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord).
Hyperparameter tuning
To improve the model's performance with your dataset, you can tune
the model's hyperparameters. You can find information about hyperparameters
common to all TPU supported models on
[GitHub](https://github.com/tensorflow/tpu/tree/master/models/hyperparameters).
Information about model-specific hyperparameters can be found in the
[source code](https://github.com/tensorflow/tpu/tree/master/models/official)
for each model. For more information on hyperparameter tuning, see
[Overview of hyperparameter tuning](https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview),
[Using the Hyperparameter tuning service](https://cloud.google.com/ai-platform/training/docs/using-hyperparameter-tuning),
and [Tune hyperparameters](https://developers.google.com/machine-learning/guides/text-classification/step-5).
Inference
Once you have trained your model you can use it for inference
(also called prediction).
[AI Platform](https://cloud.google.com/ai-platform/docs/technical-overview)
is a cloud-based solution for developing,
[training](https://cloud.google.com/ai-platform/training/docs),
and [deploying](https://cloud.google.com/ai-platform/prediction/docs/deploying-models)
machine learning models. Once a model is deployed, you can use the
[AI Platform Prediction service](https://cloud.google.com/ai-platform/prediction/docs).
- Learn how to train and evaluate using your own data in place of the
fake_imagenet or ImageNet datasets by following the
[dataset conversion tutorial](/tpu/docs/classification-data-conversion). The tutorial explains how to use the image classification data converter example script to convert a raw dataset for image classification into TFRecords usable by Cloud TPU Tensorflow models.
- Run a Cloud TPU
[colab](https://colab.sandbox.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/03_Flower_pictures_to_TFRecords.ipynb)that demonstrates how to run an image classification model using your own image data.
- Explore the other
[Cloud TPU tutorials](/tpu/docs/tutorials).
- Learn to use the
[TPU monitoring tools in TensorBoard](/tpu/docs/cloud-tpu-tools).
- See how to train ResNet with
[Cloud TPU and GKE](/tpu/docs/tutorials/kubernetes-engine-resnet).",Training ResNet on Cloud TPU (TF 2.x) | Google Cloud,
id,url,body,title,description
83,https://cloud.google.com/tpu/docs/tutorials/shapemask-2.x,"This document demonstrates how to run the ShapeMask model using Cloud TPU with the COCO dataset.
The instructions below assume you are already familiar with running a model on
Cloud TPU. If you are new to Cloud TPU, you can
refer to the
[Quickstart](/tpu/docs/quickstart) for a basic introduction.
If you plan to train on a TPU Pod slice,
review
[Training on TPU Pods](/tpu/docs/training-on-tpu-pods)
to understand parameter changes required for Pod slices.
Objectives
- Prepare the COCO dataset
- Create a Cloud Storage bucket to hold your dataset and model output
- Set up TPU resources for training and evaluation
- Run training and evaluation on a single Cloud TPU or a Cloud TPU Pod
Costs
In this document, you use the following billable components of Google Cloud:
- Compute Engine
- Cloud TPU
- Cloud Storage
To generate a cost estimate based on your projected usage,
use the
[pricing calculator](/products/calculator).
[free trial](/free-trial).
Before you begin
Before starting this tutorial, check that your Google Cloud project is correctly set up.
-
Sign in to your Google Cloud account. If you're new to
Google Cloud,
[create an account](https://console.cloud.google.com/freetrial)to evaluate how our products perform in real-world scenarios. New customers also get $300 in free credits to run, test, and deploy workloads.
-
In the Google Cloud console, on the project selector page, select or
[create a Google Cloud project](/resource-manager/docs/creating-managing-projects).
-
[Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console).
-
In the Google Cloud console, on the project selector page, select or
[create a Google Cloud project](/resource-manager/docs/creating-managing-projects).
-
[Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console).
This walkthrough uses billable components of Google Cloud. Check the
[Cloud TPU pricing page](/tpu/docs/pricing) to
estimate your costs. Be sure to
[clean up](#clean_up)
resources you create when you've finished with them to avoid unnecessary
charges.
Cloud TPU single device training
This section provides information on setting up Cloud Storage, VM, and Cloud TPU resources for single device training.
If you plan to train on a TPU Pod slice, review
[Training on TPU Pods](/tpu/docs/training-on-tpu-pods) to
understand the changes required to train on Pod slices.
In your
[Cloud Shell](https://console.cloud.google.com/), create a variable for your project's ID.
export PROJECT_ID=project-id
Configure Google Cloud CLI to use the project where you want to create Cloud TPU.
gcloud config set project ${PROJECT_ID}
The first time you run this command in a new Cloud Shell VM, an
Authorize Cloud Shellpage is displayed. Click
Authorizeat the bottom of the page to allow
gcloudto make GCP API calls with your credentials.
Create a Service Account for the Cloud TPU project.
gcloud beta services identity create --service tpu.googleapis.com --project $PROJECT_ID
The command returns a Cloud TPU Service Account with following format:
service-PROJECT_NUMBER@cloud-tpu.iam.gserviceaccount.com
Prepare the COCO dataset
This tutorial uses the COCO dataset. The dataset needs to be in TFRecord format on a Cloud Storage bucket to be used for the training.
The bucket location must be in the
same region as your virtual machine (VM) and your TPU node. VMs and TPU nodes
are located in
[specific zones](/tpu/docs/types-zones), which
are subdivisions within a region.
The Cloud Storage bucket stores the data you use to train
your model and the training results. The
gcloud compute tpus execution-groups tool used in this tutorial
sets up default permissions for the Cloud TPU Service Account you set up
in the previous step. If you want finer-grain permissions, review the
[access level permissions](/tpu/docs/storage-buckets).
If you already have the COCO dataset prepared
on a Cloud Storage bucket that is located in the
[zone](/tpu/docs/types-zones-tpu-vm) you will
be using to train the model, you can [launch the TPU resources](#launch-tpu-resources)
and prepare Cloud TPU for training.
Otherwise,
use the following steps to prepare the dataset.
In your
[Cloud Shell](https://console.cloud.google.com/), configure
gcloudwith your project ID.
export PROJECT_ID=project-id gcloud config set project ${PROJECT_ID}
In your
[Cloud Shell](https://console.cloud.google.com/), create a Cloud Storage bucket using the following command:
gsutil mb -p ${PROJECT_ID} -c standard -l europe-west4 gs://bucket-name
Launch a Compute Engine VM instance.
This VM instance will only be used to download and preprocess the COCO dataset. Fill in the instance-name with a name of your choosing.
$ gcloud compute tpus execution-groups create \ --vm-only \ --name=instance-name \ --zone=europe-west4-a \ --disk-size=300 \ --machine-type=n1-standard-16 \ --tf-version=2.12.0
Command flag descriptions
vm-only
- Create a VM only. By default the
gcloud compute tpus execution-groupscommand creates a VM and a Cloud TPU.
name
- The name of the Cloud TPU to create.
zone
- The
[zone](https://cloud.google.com/tpu/docs/types-zones)where you plan to create your Cloud TPU.
disk-size
- The size of the hard disk in GB of the VM created by the
gcloud compute tpus execution-groupscommand.
machine-type
- The
[machine type](https://cloud.google.com/compute/docs/machine-types)of the Compute Engine VM to create.
tf-version
- The version of Tensorflow
gcloud compute tpus execution-groupsinstalls on the VM.
-
If you are not automatically logged in to the Compute Engine instance, log in by running the following
sshcommand. When you are logged into the VM, your shell prompt changes from
username@projectnameto
username@vm-name:
$ gcloud compute ssh instance-name --zone=europe-west4-a
Set up two variables, one for the storage bucket you created earlier and one for the directory that holds the training data (DATA_DIR) on the storage bucket.
(vm)$ export STORAGE_BUCKET=gs://bucket-name
(vm)$ export DATA_DIR=${STORAGE_BUCKET}/coco
Install the packages needed to pre-process the data.
(vm)$ sudo apt-get install -y python3-tk && \ pip3 install --user Cython matplotlib opencv-python-headless pyyaml Pillow && \ pip3 install --user ""git+https://github.com/cocodataset/cocoapi#egg=pycocotools&subdirectory=PythonAPI""
Run the
download_and_preprocess_coco.shscript to convert the COCO dataset into a set of TFRecords (
*.tfrecord) that the training application expects.
(vm)$ git clone https://github.com/tensorflow/tpu.git (vm)$ sudo bash tpu/tools/datasets/download_and_preprocess_coco.sh ./data/dir/coco
This installs the required libraries and then runs the preprocessing script. It outputs a number of
*.tfrecordfiles in your local data directory. The COCO download and conversion script takes approximately 1 hour to complete.
Copy the data to your Cloud Storage bucket
After you convert the data into TFRecords, copy them from local storage to your Cloud Storage bucket using the
gsutilcommand. You must also copy the annotation files. These files help validate the model's performance.
(vm)$ gsutil -m cp ./data/dir/coco/*.tfrecord ${DATA_DIR} (vm)$ gsutil cp ./data/dir/coco/raw-data/annotations/*.json ${DATA_DIR}
Clean up the VM resources
Once the COCO dataset has been converted to TFRecords and copied to the DATA_DIR on your Cloud Storage bucket, you can delete the Compute Engine instance.
Disconnect from the Compute Engine instance:
(vm)$ exit
Your prompt should now be
username@projectname, showing you are in the Cloud Shell.
Delete your Compute Engine instance.
$ gcloud compute instances delete instance-name --zone=europe-west4-a
Launch the TPU resources and train the model
Use the
gcloudcommand to launch the TPU resources. The command you use depends on whether you are using TPU VMs or TPU nodes. For more information on the two VM architecture, see
[System Architecture](/tpu/docs/system-architecture-tpu-vm).
TPU VM
$ gcloud compute tpus tpu-vm create shapemask-tutorial \ --zone=europe-west4-a \ --accelerator-type=v3-8 \ --version=tpu-vm-tf-2.15.0-pjrt
Command flag descriptions
zone
- The
[zone](/tpu/docs/types-zones)where you plan to create your Cloud TPU.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
version
- The Cloud TPU
[software version](/tpu/docs/supported-tpu-versions#tpu_software_versions).
TPU Node
$ gcloud compute tpus execution-groups create \ --zone=europe-west4-a \ --name=shapemask-tutorial \ --accelerator-type=v3-8 \ --machine-type=n1-standard-8 \ --disk-size=300 \ --tf-version=2.12.0
Command flag descriptions
zone
- The
[zone](/tpu/docs/types-zones)where you plan to create your Cloud TPU.
name
- The TPU name. If not specified, defaults to your username.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
machine-type
- The
[machine type](/compute/docs/machine-types)of the Compute Engine VM to create.
disk-size
- The root volume size of your Compute Engine VM (in GB).
tf-version
- The version of Tensorflow
gcloudinstalls on the VM.
For more information on the
gcloudcommand, see the
[gcloud Reference](/sdk/gcloud/reference).
-
If you are not automatically logged in to the Compute Engine instance, log in by running the following
sshcommand. When you are logged into the VM, your shell prompt changes from
username@projectnameto
username@vm-name:
TPU VM
gcloud compute tpus tpu-vm ssh shapemask-tutorial --zone=europe-west4-a
TPU Node
gcloud compute ssh shapemask-tutorial --zone=europe-west4-a
As you continue these instructions, run each command that begins with
(vm)$in your VM session window.
Install TensorFlow requirements.
TPU VM
(vm)$ pip3 install -r /usr/share/tpu/models/official/requirements.txt
TPU Node
(vm)$ pip3 install -r /usr/share/models/official/requirements.txt
The training script requires an extra package. Install it now:
TPU VM
(vm)$ pip3 install --user tensorflow-model-optimization>=0.1.3
TPU Node
(vm)$ pip3 install --user tensorflow-model-optimization>=0.1.3
Set the storage bucket name variable. Replace bucket-name with the name of your storage bucket:
(vm)$ export STORAGE_BUCKET=gs://bucket-name
Set the Cloud TPU name variable.
TPU VM
(vm)$ export TPU_NAME=local
TPU Node
(vm)$ export TPU_NAME=shapemask-tutorial
Set the
PYTHONPATHenvironment variable:
TPU VM
(vm)$ export PYTHONPATH=""/usr/share/tpu/models:${PYTHONPATH}""
TPU Node
(vm)$ export PYTHONPATH=""${PYTHONPATH}:/usr/share/models""
Change to directory that stores the model:
TPU VM
(vm)$ cd /usr/share/tpu/models/official/legacy/detection
TPU Node
(vm)$ cd /usr/share/models/official/legacy/detection
Add some required environment variables:
(vm)$ export RESNET_CHECKPOINT=gs://cloud-tpu-checkpoints/retinanet/resnet50-checkpoint-2018-02-07 (vm)$ export DATA_DIR=${STORAGE_BUCKET}/coco (vm)$ export TRAIN_FILE_PATTERN=${DATA_DIR}/train-* (vm)$ export EVAL_FILE_PATTERN=${DATA_DIR}/val-* (vm)$ export VAL_JSON_FILE=${DATA_DIR}/instances_val2017.json (vm)$ export SHAPE_PRIOR_PATH=gs://cloud-tpu-checkpoints/shapemask/kmeans_class_priors_91x20x32x32.npy (vm)$ export MODEL_DIR=${STORAGE_BUCKET}/shapemask
- If you set
--version=tpu-vm-tf-2.15.0-pjrtwhen creating your TPU, set the following environment variables to enable the PJRT runtime:
(vm)$ export NEXT_PLUGGABLE_DEVICE_USE_C_API=true (vm)$ export TF_PLUGGABLE_DEVICE_LIBRARY_PATH=/lib/libtpu.so
Train the ShapeMask model:
The following script runs a sample training that trains for just 100 steps and takes approxiately 10 minutes to complete on a v3-8 TPU. To train to convergence takes about 22,500 steps and approximately 6 hours on a v3-8 TPU.
(vm)$ python3 main.py \ --strategy_type=tpu \ --tpu=${TPU_NAME} \ --model_dir=${MODEL_DIR} \ --mode=train \ --model=shapemask \ --params_override=""{train: {total_steps: 100, learning_rate: {init_learning_rate: 0.08, learning_rate_levels: [0.008, 0.0008], learning_rate_steps: [15000, 20000], }, checkpoint: { path: ${RESNET_CHECKPOINT},prefix: resnet50}, train_file_pattern: ${TRAIN_FILE_PATTERN}}, shapemask_head: {use_category_for_mask: true, shape_prior_path: ${SHAPE_PRIOR_PATH}}, shapemask_parser: {output_size: [640, 640]}}""
Command flag descriptions
strategy_type
- To train the Shapemask model on a TPU, you must set the
distribution_strategyto
tpu.
tpu
- The name of the Cloud TPU. This is set using the
TPU_NAMEenvironment variable.
model_dir
- The directory where checkpoints and summaries are stored during
model training. If the folder is missing, the program creates one.
When using a Cloud TPU, the
model_dirmust be a Cloud Storage path (
gs://...). You can reuse an existing folder to load current checkpoint data and to store additional checkpoints as long as the previous checkpoints were created using a Cloud TPU of the same size and TensorFlow version.
mode
- Set this to
trainto train the model or
evalto evaluate the model.
params_override
- A JSON string that overrides default script parameters. For more
information on script parameters, see
/usr/share/models/official/legacy/detection/main.py.
When the training completes, a message similar to the following appears:
Train Step: 100/100 / loss = {'total_loss': 10.815635681152344, 'loss': 10.815635681152344, 'retinanet_cls_loss': 1.4915691614151, 'l2_regularization_loss': 4.483549118041992, 'retinanet_box_loss': 0.013074751943349838, 'shapemask_prior_loss': 0.17314358055591583, 'shapemask_coarse_mask_loss': 1.953366756439209, 'shapemask_fine_mask_loss': 2.216097831726074, 'model_loss': 6.332086086273193, 'learning_rate': 0.021359999} / training metric = {'total_loss': 10.815635681152344, 'loss': 10.815635681152344, 'retinanet_cls_loss': 1.4915691614151, 'l2_regularization_loss': 4.483549118041992, 'retinanet_box_loss': 0.013074751943349838, 'shapemask_prior_loss': 0.17314358055591583, 'shapemask_coarse_mask_loss': 1.953366756439209, 'shapemask_fine_mask_loss': 2.216097831726074, 'model_loss': 6.332086086273193, 'learning_rate': 0.021359999}
-
Run the script to evaluate the ShapeMask model. This takes about 10 minutes on a v3-8 TPU:
(vm)$ python3 main.py \ --strategy_type=tpu \ --tpu=${TPU_NAME} \ --model_dir=${MODEL_DIR} \ --checkpoint_path=${MODEL_DIR} \ --mode=eval_once \ --model=shapemask \ --params_override=""{eval: { val_json_file: ${VAL_JSON_FILE}, eval_file_pattern: ${EVAL_FILE_PATTERN}, eval_samples: 5000 }, shapemask_head: {use_category_for_mask: true, shape_prior_path: ${SHAPE_PRIOR_PATH}}, shapemask_parser: {output_size: [640, 640]}}""
Command flag descriptions
strategy_type
- To train the Shapemask model on a TPU, you must set the
distribution_strategyto
tpu.
tpu
- The name of the Cloud TPU. This is set using the
TPU_NAMEenvironment variable.
model_dir
- The directory where checkpoints and summaries are stored during
model training. If the folder is missing, the program creates one.
When using a Cloud TPU, the
model_dirmust be a Cloud Storage path (
gs://...). You can reuse an existing folder to load current checkpoint data and to store additional checkpoints as long as the previous checkpoints were created using a Cloud TPU of the same size and TensorFlow version.
mode
- Set this to
trainto train the model or
evalto evaluate the model.
params_override
- A JSON string that overrides default script parameters. For more
information on script parameters, see
/usr/share/models/official/legacy/detection/main.py.
When the evaluation completes, a message similar to the following appears:
DONE (t=5.47s). Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.000 Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.000 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.000 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.000 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.000 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.000 Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
You have now completed single-device training and evaluation. Use the following steps to delete the current single-device TPU resources.
-
Disconnect from the Compute Engine instance:
(vm)$ exit
Your prompt should now be
username@projectname, showing you are in the Cloud Shell.
Delete the TPU resource.
TPU VM
$ gcloud compute tpus tpu-vm delete shapemask-tutorial \ --zone=europe-west4-a
Command flag descriptions
zone
- The
[zone](https://cloud.google.com/tpu/docs/types-zones)where your Cloud TPU resided.
TPU Node
$ gcloud compute tpus execution-groups delete shapemask-tutorial \ --tpu-only \ --zone=europe-west4-a
Command flag descriptions
tpu-only
- Deletes only the Cloud TPU. The VM remains available.
zone
- The
[zone](https://cloud.google.com/tpu/docs/types-zones)that contains the TPU to delete.
At this point, you can either conclude this tutorial and
[clean up](#cleanup), or you can continue and explore running the model on Cloud TPU Pods.
-
Scale your model with Cloud TPU Pods
Training your model on Cloud TPU Pods may require some changes
to your training script. For information, see
[Training on TPU Pods](/tpu/docs/training-on-tpu-pods).
TPU Pod training
Open a Cloud Shell window.
Create a variable for your project's ID.
export PROJECT_ID=project-id
Configure Google Cloud CLI to use the project where you want to create Cloud TPU.
gcloud config set project ${PROJECT_ID}
The first time you run this command in a new Cloud Shell VM, an
Authorize Cloud Shellpage is displayed. Click
Authorizeat the bottom of the page to allow
gcloudto make GCP API calls with your credentials.
Create a Service Account for the Cloud TPU project.
Service accounts allow the Cloud TPU service to access other Google Cloud Platform services.
gcloud beta services identity create --service tpu.googleapis.com --project $PROJECT_ID
The command returns a Cloud TPU Service Account with following format:
service-PROJECT_NUMBER@cloud-tpu.iam.gserviceaccount.com
Create a Cloud Storage bucket using the following command or use a bucket you created earlier for your project.
gsutil mb -p ${PROJECT_ID} -c standard -l europe-west4 gs://bucket-name
If you previously prepared the COCO dataset and moved it to your storage bucket, you can use it again for Pod training. If you have not yet prepared the COCO dataset,
[prepare it now](#prepare-coco)and return here to set up the Pod training.
Launch a Cloud TPU Pod
This tutorial specifies a v3-32 Pod. For other Pod options, see the
[available TPU types page](/tpu/docs/supported-tpu-configurations).
TPU VM
$ gcloud compute tpus tpu-vm create shapemask-tutorial \ --zone=europe-west4-a \ --accelerator-type=v3-32 \ --version=tpu-vm-tf-2.15.0-pod-pjrt
Command flag descriptions
zone
- The
[zone](/tpu/docs/types-zones)where you plan to create your Cloud TPU.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
version
- The Cloud TPU
[software version](/tpu/docs/supported-tpu-versions#tpu_software_versions).
TPU Node
$ gcloud compute tpus execution-groups create \ --zone=europe-west4-a \ --name=shapemask-tutorial \ --accelerator-type=v3-32 \ --machine-type=n1-standard-8 \ --disk-size=300 \ --tf-version=2.12.0
Command flag descriptions
zone
- The
[zone](/tpu/docs/types-zones)where you plan to create your Cloud TPU.
name
- The TPU name. If not specified, defaults to your username.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
machine-type
- The
[machine type](/compute/docs/machine-types)of the Compute Engine VM to create.
disk-size
- The root volume size of your Compute Engine VM (in GB).
tf-version
- The version of Tensorflow
gcloudinstalls on the VM.
-
If you are not automatically logged in to the Compute Engine instance, log in by running the following
sshcommand. When you are logged into the VM, your shell prompt changes from
username@projectnameto
username@vm-name:
TPU VM
gcloud compute tpus tpu-vm ssh shapemask-tutorial --zone=europe-west4-a
TPU Node
gcloud compute ssh shapemask-tutorial --zone=europe-west4-a
As you continue these instructions, run each command that begins with
(vm)$in your VM session window.
Install TensorFlow requirements.
TPU VM
(vm)$ pip3 install -r /usr/share/tpu/models/official/requirements.txt
TPU Node
(vm)$ pip3 install -r /usr/share/models/official/requirements.txt
The training script requires an extra package. Install it now:
TPU VM
(vm)$ pip3 install --user tensorflow-model-optimization>=0.1.3
TPU Node
(vm)$ pip3 install --user tensorflow-model-optimization>=0.1.3
Set up the following environment variables, replacing bucket-name with the name of your Cloud Storage bucket:
(vm)$ export STORAGE_BUCKET=gs://bucket-name
The training application expects your training data to be accessible in Cloud Storage. The training application also uses your Cloud Storage bucket to store checkpoints during training.
Update the required training variables.
(vm)$ export MODEL_DIR=${STORAGE_BUCKET}/shapemask-pods (vm)$ export DATA_DIR=${STORAGE_BUCKET}/coco (vm)$ export RESNET_CHECKPOINT=gs://cloud-tpu-checkpoints/retinanet/resnet50-checkpoint-2018-02-07 (vm)$ export TRAIN_FILE_PATTERN=${DATA_DIR}/train-* (vm)$ export EVAL_FILE_PATTERN=${DATA_DIR}/val-* (vm)$ export VAL_JSON_FILE=${DATA_DIR}/instances_val2017.json (vm)$ export SHAPE_PRIOR_PATH=gs://cloud-tpu-checkpoints/shapemask/kmeans_class_priors_91x20x32x32.npy
Set some required environment variables:
TPU VM
(vm)$ export PYTHONPATH=""/usr/share/tpu/models:${PYTHONPATH}"" (vm)$ export TPU_LOAD_LIBRARY=0
TPU Node
(vm)$ export PYTHONPATH=""${PYTHONPATH}:/usr/share/models""
Change to directory that stores the model:
TPU VM
(vm)$ cd /usr/share/tpu/models/official/legacy/detection
TPU Node
(vm)$ cd /usr/share/models/official/legacy/detection
Start the Pod training.
The sample training runs for just 20 steps and takes approximately 10 minutes to complete on a v3-32 TPU node. To train to convergence takes about 11,250 steps and approximately 2 hours on a v3-32 TPU Pod.
(vm)$ python3 main.py \ --strategy_type=tpu \ --tpu=${TPU_NAME} \ --model_dir=${MODEL_DIR} \ --mode=train \ --model=shapemask \ --params_override=""{train: { batch_size: 128, iterations_per_loop: 500, total_steps: 20, learning_rate: {'learning_rate_levels': [0.008, 0.0008], 'learning_rate_steps': [10000, 13000] }, checkpoint: { path: ${RESNET_CHECKPOINT}, prefix: resnet50/ }, train_file_pattern: ${TRAIN_FILE_PATTERN} }, eval: { val_json_file: ${VAL_JSON_FILE}, eval_file_pattern: ${EVAL_FILE_PATTERN}}, shapemask_head: {use_category_for_mask: true, shape_prior_path: ${SHAPE_PRIOR_PATH}} }""
Command flag descriptions
strategy_type
- To train the Shapemask model on a TPU, you must set the
distribution_strategyto
tpu.
tpu
- The name of the Cloud TPU. This is set using the
TPU_NAMEenvironment variable.
model_dir
- The directory where checkpoints and summaries are stored during
model training. If the folder is missing, the program creates one.
When using a Cloud TPU, the
model_dirmust be a Cloud Storage path (
gs://...). You can reuse an existing folder to load current checkpoint data and to store additional checkpoints as long as the previous checkpoints were created using a Cloud TPU of the same size and TensorFlow version.
mode
- Set this to
trainto train the model or
evalto evaluate the model.
params_override
- A JSON string that overrides default script parameters. For more
information on script parameters, see
/usr/share/models/official/legacy/detection/main.py.
-
Clean up
To avoid incurring charges to your Google Cloud account for the resources used in this tutorial, either delete the project that contains the resources, or keep the project and delete the individual resources.
Disconnect from the Compute Engine instance, if you have not already done so:
(vm)$ exit
Your prompt should now be
username@projectname, showing you are in the Cloud Shell.
Delete your Cloud TPU and Compute Engine resources. The command you use to delete your resources depends upon whether you are using TPU VMs or TPU Nodes. For more information, see
[System Architecture](/tpu/docs/system-architecture-tpu-vm).
TPU VM
$ gcloud compute tpus tpu-vm delete shapemask-tutorial \ --zone=europe-west4-a
TPU Node
$ gcloud compute tpus execution-groups delete shapemask-tutorial \ --zone=europe-west4-a
Verify the resources have been deleted by running
gcloud compute tpus execution-groups list. The deletion might take several minutes. The output from the following command should not include any of the TPU resources created in this tutorial:
$ gcloud compute tpus execution-groups list --zone=europe-west4-a
Run
gsutilas shown, replacing bucket-name with the name of the Cloud Storage bucket you created for this tutorial:
$ gsutil rm -r gs://bucket-name
What's next
Train with different image sizes
You can explore using a larger neural network (for example, ResNet-101 instead of ResNet-50). A larger input image and a more powerful neural network will yield a slower but more precise model.
Use a different basis
Alternatively, you can explore pre-training a ResNet model on your own dataset and using it as a basis for your ShapeMask model. With some more work, you can also swap in an alternative neural network in place of ResNet. Finally, if you are interested in implementing your own object detection models, this network may be a good basis for further experimentation.",Training ShapeMask on Cloud TPU (TF 2.x) | Google Cloud,
id,url,body,title,description
54,https://cloud.google.com/tpu/docs/ray-guide,"Scale ML workloads using Ray
Introduction
The Cloud TPU Ray tool combines
[Cloud TPU API](/tpu/docs/reference/rest)
and
[Ray Jobs](https://docs.ray.io/en/latest/cluster/running-applications/job-submission/index.html)
with the aim of improving users' development experience on Cloud TPU. This
user guide provides a minimal example of how you can use [Ray](https://ray.io) with
Cloud TPUs. These examples are not meant to be used in production
services and are for illustrative purposes only.
What's included in this tool?
For your convenience, the tool provides:
- Generic abstractions that hide away boilerplate for common TPU actions
- Toy examples that you can fork for your own basic workflows
Specifically:
[: Python wrapper for basic TPU operations using the](https://github.com/tensorflow/tpu/blob/master/tools/ray_tpu/legacy/tpu_api.py)
tpu_api.py
[Cloud TPU API](/tpu/docs/reference/rest). [: Class representation of a TPU. This is essentially a wrapper for](https://github.com/tensorflow/tpu/blob/master/tools/ray_tpu/legacy/tpu_controller.py)
tpu_controller.py
tpu_api.py.
[: TPU controller with Ray functionality. This abstracts away boilerplate for Ray cluster and Ray Jobs.](https://github.com/tensorflow/tpu/blob/master/tools/ray_tpu/legacy/ray_tpu_controller.py)
ray_tpu_controller.py
[: Basic example that shows how to use](https://github.com/tensorflow/tpu/blob/master/tools/ray_tpu/legacy/run_basic_jax.py)
run_basic_jax.py
RayTpuControllerfor
print(jax.device_count()).
[: Basic example that shows how Ray Tune can be used with JAX/Flax on MNIST.](https://github.com/tensorflow/tpu/blob/master/tools/ray_tpu/src/tune/run_hp_search.py)
run_hp_search.py
[: Example that showcases how you can use](https://github.com/tensorflow/tpu/blob/master/tools/ray_tpu/legacy/run_pax_autoresume.py)
run_pax_autoresume.py
RayTpuControllerfor fault tolerant training using PAX as an example workload.
Setting up Ray cluster head node
One of the basic ways you can use Ray with a TPU Pod is to set up the TPU Pod as a Ray cluster. Creating a separate CPU VM as coordinator VM is the natural way to do this. The following graphic shows an example of a Ray cluster configuration:
The following commands show how you can set up a Ray cluster using the Google Cloud CLI:
$ gcloud compute instances create my_tpu_admin --machine-type=n1-standard-4 ... $ gcloud compute ssh my_tpu_admin $ (vm) pip3 install ray[default] $ (vm) ray start --head --port=6379 --num-cpus=0 ... # (Ray returns the IP address of the HEAD node, for example, RAY_HEAD_IP) $ (vm) gcloud compute tpus tpu-vm create $TPU_NAME ... --metadata startup-script=""pip3 install ray && ray start --address=$RAY_HEAD_IP --resources='{\""tpu_host\"": 1}'""
For your convenience, we also provide basic scripts for creating a coordinator
VM and deploying the contents of this folder to your coordinator VM. For source code, see
[
and
create_cpu.sh](https://github.com/tensorflow/tpu/blob/master/tools/ray_tpu/create_cpu.sh) [.](https://github.com/tensorflow/tpu/blob/master/tools/ray_tpu/deploy.sh)
deploy.sh
These scripts set some default values:
create_cpu.shwill create a VM named
$USER-adminand will utilize whatever project and zone are set to your
gcloud configdefaults. Run
gcloud config listto see those defaults.
create_cpu.shby default allocates a boot disk size of 200GB.
deploy.shassumes that your VM name is
$USER-admin. If you change that value in
create_cpu.shbe sure to change it in
deploy.sh.
To use the convenience scripts:
Clone the GitHub repository to your local machine and enter the
ray_tpufolder:
$ git clone https://github.com/tensorflow/tpu.git $ cd tpu/tools/ray_tpu/
If you do not have a dedicated service account for TPU administration (highly recommended), set one up:
$ ./create_tpu_service_account.sh
Create a coordinator VM:
$ ./create_cpu.sh
This script installs dependencies on the VM by using a
[startup script](/compute/docs/instances/startup-scripts/linux)and automatically blocks until the startup script is complete.
Deploy local code to the coordinator VM:
$ ./deploy.sh
SSH to the VM:
$ gcloud compute ssh $USER-admin -- -L8265:localhost:8265
Port forwarding is enabled here as Ray will automatically start a dashboard at port 8265. From the machine that you SSH to your coordinator VM, you will be able to access this dashboard at
[http://127.0.0.1:8265/](http://127.0.0.1:8265/).
If you skipped step 0, set up your gcloud credentials within the CPU VM:
$ (vm) gcloud auth login --update-adc
This step sets project ID info and allows Cloud TPU API to run on the coordinator VM.
Install requirements:
$ (vm) pip3 install -r src/requirements.txt
Start Ray on the coordinator VM, and the coordinator VM becomes the head node of the Ray cluster:
$ (vm) ray start --head --port=6379 --num-cpus=0
Usage examples
Basic JAX example
[
is a minimal example that demonstrates how you can use the Ray Jobs and Ray
runtime environment on a Ray cluster with TPU VMs to run a JAX workload. run_basic_jax.py](https://github.com/tensorflow/tpu/blob/master/tools/ray_tpu/legacy/run_basic_jax.py)
For ML frameworks compatible with Cloud TPUs that use a
multi-controller programming model, such as JAX and PyTorch/XLA PJRT, you must run
at least one process per host. For more information, see
[Multi-process programming model](https://jax.readthedocs.io/en/latest/multi_process.html#multi-process-programming-model).
In practice, this might look like:
$ gcloud compute tpus tpu-vm scp my_bug_free_python_code my_tpu:~/ --worker=all $ gcloud compute tpus tpu-vm ssh my_tpu --worker=all --command=""python3 ~/my_bug_free_python_code/main.py""
If you have more than ~16 hosts, such as a v4-128, you will run into SSH scalability issues and your command might have to change to:
$ gcloud compute tpus tpu-vm scp my_bug_free_python_code my_tpu:~/ --worker=all --batch-size=8 $ gcloud compute tpus tpu-vm ssh my_tpu --worker=all --command=""python3 ~/my_bug_free_python_code/main.py &"" --batch-size=8
This can become a hindrance on developer velocity if
my_bug_free_python_code
contains bugs. One of the ways you can solve this problem is by using an
orchestrator like Kubernetes or Ray. Ray includes the concept of a
[Runtime environment](https://docs.ray.io/en/latest/ray-core/handling-dependencies.html#runtime-environments)
that, when applied, deploys code and dependencies when the Ray application is
run.
Combining the Ray runtime environment with Ray cluster and Ray Jobs allows you to bypass the SCP/SSH cycle. Assuming you followed the above examples, you can run this with:
$ python3 legacy/run_basic_jax.py
The output is similar to the following:
2023-03-01 22:12:10,065 INFO worker.py:1364 -- Connecting to existing Ray cluster at address: 10.130.0.19:6379... 2023-03-01 22:12:10,072 INFO worker.py:1544 -- Connected to Ray cluster. View the dashboard at http://127.0.0.1:8265 W0301 22:12:11.148555 140341931026240 ray_tpu_controller.py:143] TPU is not found, create tpu... Creating TPU: $USER-ray-test Request: {'accelerator_config': {'topology': '2x2x2', 'type': 'V4'}, 'runtimeVersion': 'tpu-ubuntu2204-base', 'networkConfig': {'enableExternalIps': True}, 'metadata': {'startup-script': '#! /bin/bash\necho ""hello world""\nmkdir -p /dev/shm\nsudo mount -t tmpfs -o size=100g tmpfs /dev/shm\n pip3 install ray[default]\nray start --resources=\'{""tpu_host"": 1}\' --address=10.130.0.19:6379'}} Create TPU operation still running... ... Create TPU operation complete. I0301 22:13:17.795493 140341931026240 ray_tpu_controller.py:121] Detected 0 TPU hosts in cluster, expecting 2 hosts in total I0301 22:13:17.795823 140341931026240 ray_tpu_controller.py:160] Waiting for 30s for TPU hosts to join cluster...  I0301 22:15:17.986352 140341931026240 ray_tpu_controller.py:121] Detected 2 TPU hosts in cluster, expecting 2 hosts in total I0301 22:15:17.986503 140341931026240 ray_tpu_controller.py:90] Ray already started on each host. 2023-03-01 22:15:18,010 INFO dashboard_sdk.py:315 -- Uploading package gcs://_ray_pkg_3599972ae38ce933.zip. 2023-03-01 22:15:18,010 INFO packaging.py:503 -- Creating a file package for local directory '/home/$USER/src'. 2023-03-01 22:15:18,080 INFO dashboard_sdk.py:362 -- Package gcs://_ray_pkg_3599972ae38ce933.zip already exists, skipping upload. I0301 22:15:18.455581 140341931026240 ray_tpu_controller.py:169] Queued 2 jobs. ... I0301 22:15:48.523541 140341931026240 ray_tpu_controller.py:254] [ADMIN]: raysubmit_WRUtVB7nMaRTgK39: Status is SUCCEEDED I0301 22:15:48.561111 140341931026240 ray_tpu_controller.py:256] [raysubmit_WRUtVB7nMaRTgK39]: E0301 22:15:36.294834089 21286 credentials_generic.cc:35] Could not get HOME environment variable. 8 I0301 22:15:58.575289 140341931026240 ray_tpu_controller.py:254] [ADMIN]: raysubmit_yPCPXHiFgaCK2rBY: Status is SUCCEEDED I0301 22:15:58.584667 140341931026240 ray_tpu_controller.py:256] [raysubmit_yPCPXHiFgaCK2rBY]: E0301 22:15:35.720800499 8561 credentials_generic.cc:35] Could not get HOME environment variable. 8
Fault tolerant training
This example showcases how you can use
RayTpuController to implement fault
tolerant training. For this example, we pretrain a simple LLM on
[PAX](https://github.com/google/paxml)
on a v4-16, but note that you can replace this PAX workload with any other long
running workload. For source code, see
[. run_pax_autoresume.py](https://github.com/tensorflow/tpu/blob/master/tools/ray_tpu/legacy/run_basic_jax.py)
To run this example:
Clone
paxmlto your coordinator VM:
$ git clone https://github.com/google/paxml.git
To demonstrate the ease-of-use that the Ray Runtime Environment provides for making and deploying JAX changes, this example requires you to modify PAX.
Add a new experiment config:
$ cat <<EOT >> paxml/paxml/tasks/lm/params/lm_cloud.py @experiment_registry.register class TestModel(LmCloudSpmd2BLimitSteps): ICI_MESH_SHAPE = [1, 4, 2] CHECKPOINT_POLICY = layers.AutodiffCheckpointType.SAVE_CONTEXT_AND_OUT_PROJ def task(self) -> tasks_lib.SingleTask.HParams: task_p = super().task() task_p.train.num_train_steps = 1000 task_p.train.save_interval_steps = 100 return task_p EOT
Run
run_pax_autoresume.py:
$ python3 legacy/run_pax_autoresume.py --model_dir=gs://your/gcs/bucket
As the workload runs, experiment with what happens when you delete your TPU, by default, named
$USER-tpu-ray:
$ gcloud compute tpus tpu-vm delete -q $USER-tpu-ray --zone=us-central2-b
Ray will detect the TPU is down with following message:
I0303 05:12:47.384248 140280737294144 checkpointer.py:64] Saving item to gs://$USER-us-central2/pax/v4-16-autoresume-test/checkpoints/checkpoint_00000200/metadata. W0303 05:15:17.707648 140051311609600 ray_tpu_controller.py:127] TPU is not found, create tpu... 2023-03-03 05:15:30,774 WARNING worker.py:1866 -- The node with node id: 9426f44574cce4866be798cfed308f2d3e21ba69487d422872cdd6e3 and address: 10.130.0.113 and node name: 10.130.0.113 has been marked dead because the detector has missed too many heartbeats from it. This can happen when a (1) raylet crashes unexpectedly (OOM, preempted node, etc.) (2) raylet has lagging heartbeats due to slow network or busy workload. 2023-03-03 05:15:33,243 WARNING worker.py:1866 -- The node with node id: 214f5e4656d1ef48f99148ddde46448253fe18672534467ee94b02ba and address: 10.130.0.114 and node name: 10.130.0.114 has been marked dead because the detector has missed too many heartbeats from it. This can happen when a (1) raylet crashes unexpectedly (OOM, preempted node, etc.) (2) raylet has lagging heartbeats due to slow network or busy workload.
And the job will automatically recreate the TPU VM and restart the training job so that it can resume the training from the latest checkpoint (200 step in this example):
I0303 05:22:43.141277 140226398705472 train.py:1149] Training loop starting... I0303 05:22:43.141381 140226398705472 summary_utils.py:267] Opening SummaryWriter `gs://$USER-us-central2/pax/v4-16-autoresume-test/summaries/train`... I0303 05:22:43.353654 140226398705472 summary_utils.py:267] Opening SummaryWriter `gs://$USER-us-central2/pax/v4-16-autoresume-test/summaries/eval_train`... I0303 05:22:44.008952 140226398705472 py_utils.py:350] Starting sync_global_devices Start training loop from step: 200 across 8 devices globally
HyperParameter search
This example showcases using Ray Tune from the Ray AIR to hyperparameter tune
MNIST from JAX/FLAX. For source code, see
[. run_hp_search.py](https://github.com/tensorflow/tpu/blob/master/tools/ray_tpu/src/tune/run_hp_search.py)
To run this example:
Install the requirements:
$ pip3 install -r src/tune/requirements.txt
Run
run_hp_search.py:
$ python3 src/tune/run_hp_search.py
The output is similar to the following:
Number of trials: 3/3 (3 TERMINATED) +-----------------------------+------------+-------------------+-----------------+------------+--------+--------+------------------+ | Trial name | status | loc | learning_rate | momentum | acc | iter | total time (s) | |-----------------------------+------------+-------------------+-----------------+------------+--------+--------+------------------| | hp_search_mnist_8cbbb_00000 | TERMINATED | 10.130.0.84:21340 | 1.15258e-09 | 0.897988 | 0.0982 | 3 | 82.4525 | | hp_search_mnist_8cbbb_00001 | TERMINATED | 10.130.0.84:21340 | 0.000219523 | 0.825463 | 0.1009 | 3 | 73.1168 | | hp_search_mnist_8cbbb_00002 | TERMINATED | 10.130.0.84:21340 | 1.08035e-08 | 0.660416 | 0.098 | 3 | 71.6813 | +-----------------------------+------------+-------------------+-----------------+------------+--------+--------+------------------+ 2023-03-02 21:50:47,378 INFO tune.py:798 -- Total run time: 318.07 seconds (318.01 seconds for the tuning loop). ...
Troubleshooting
Ray head node is unable to connect
If you run a workload that creates/deletes the TPU lifecycle, sometimes this doesn't disconnect the TPU hosts from the Ray cluster. This may show up as gRPC errors that signal that the Ray head node is unable to connect to a set of IP addresses.
As a result, you may need to terminate your ray session (
ray stop) and restart
it (
ray start --head --port=6379 --num-cpus=0).
Ray Job fails directly without any log output
PAX is experimental and this example may break due to pip dependencies. If that happens, you may see something like this:
I0303 20:50:36.084963 140306486654720 ray_tpu_controller.py:174] Queued 2 jobs. I0303 20:50:36.136786 140306486654720 ray_tpu_controller.py:238] Requested to clean up 1 stale jobs from previous failures. I0303 20:50:36.148653 140306486654720 ray_tpu_controller.py:253] Job status: Counter({<JobStatus.FAILED: 'FAILED'>: 2}) I0303 20:51:38.582798 140306486654720 ray_tpu_controller.py:126] Detected 2 TPU hosts in cluster, expecting 2 hosts in total W0303 20:51:38.589029 140306486654720 ray_tpu_controller.py:196] Detected job raysubmit_8j85YLdHH9pPrmuz FAILED. 2023-03-03 20:51:38,641 INFO dashboard_sdk.py:362 -- Package gcs://_ray_pkg_ae3cacd575e24531.zip already exists, skipping upload. 2023-03-03 20:51:38,706 INFO dashboard_sdk.py:362 -- Package gcs://_ray_pkg_ae3cacd575e24531.zip already exists, skipping upload.
To see the root cause of the error, you can go to
[http://127.0.0.1:8265/](http://127.0.0.1:8265/)
and view the dashboard for the running/failed jobs which will provide more
information.
runtime_env_agent.log shows all the error information related to
runtime_env setup, for example:
60 INFO: pip is looking at multiple versions of
to determine which version is compatible with other requirements. This could take a while. 61 INFO: pip is looking at multiple versions of orbax to determine which version is compatible with other requirements. This could take a while. 62 ERROR: Cannot install paxml because these package versions have conflicting dependencies. 63 64 The conflict is caused by: 65 praxis 0.3.0 depends on t5x 66 praxis 0.2.1 depends on t5x 67 praxis 0.2.0 depends on t5x 68 praxis 0.1 depends on t5x 69 70 To fix this you could try to: 71 1. loosen the range of package versions you've specified 72 2. remove package versions to allow pip attempt to solve the dependency conflict 73 74 ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts",Scale ML workloads using Ray | Cloud TPU | Google Cloud,
id,url,body,title,description
86,https://cloud.google.com/tpu/docs,"Tensor Processing Units (TPUs) are Google's custom-developed application-specific integrated
circuits (ASICs) used to accelerate machine learning workloads. Cloud TPUs allow you to access
TPUs from Compute Engine,
[
Google Kubernetes Engine](/tpu/docs/kubernetes-engine-setup) and [Vertex AI](/vertex-ai/docs/training/training-with-tpu-vm). Learn more about [TPUs.](/tpu/docs/tpus)
Tensor Processing Units (TPUs) are Google's custom-developed application-specific integrated
circuits (ASICs) used to accelerate machine learning workloads. Cloud TPUs allow you to access
TPUs from Compute Engine,",Cloud TPU documentation | Google Cloud,"Google Cloud TPU technical documentation provides information on custom-designed machine learning accelerators, including how to use them to train and deploy machine learning models."
id,url,body,title,description
137,https://cloud.google.com/tpu/docs/quota,"Quotas
This document lists the quotas that apply to Cloud TPU. For information
about Cloud TPU pricing, see
[Cloud TPU pricing](/tpu/pricing).
A quota restricts how much of a shared Google Cloud resource your Google Cloud project can use, including hardware, software, and network components. Therefore, quotas are a part of a system that does the following:
- Monitors your use or consumption of Google Cloud products and services.
- Restricts your consumption of those resources for reasons, which include ensuring fairness and reducing spikes in usage.
- Maintains configurations that automatically enforce prescribed restrictions.
- Provides a means to request or make changes to the quota.
In most cases, when a quota is exceeded, the system immediately blocks access to the relevant Google resource, and the task that you're trying to perform fails. In most cases, quotas apply to each Google Cloud project and are shared across all applications and IP addresses that use that Google Cloud project.
Quota allocation
Quota is granted differently based on the TPU version you are using. For TPU v4 or later, quota is specified in terms of Cloud TPU chips or TensorCores. All TPU v4s are treated as slices, so there is no concept of a single TPU device. You can use your quota in any combination of slices. For example, if you have quota for a v4-32 slice, you can use this quota to create four v4-8 slices.
Creating a v5e instance for inference (
v5litepod-1,
v5litepod-4,
v5litepod-8) requires serving quota types:
tpu-v5s-litepod-serving for
on-demand TPUs,
tpu-v5s-litepod-serving-preemptible for preemptible TPUs, and
tpu-v5s-litepod-serving-reserved for reserved TPUs.
For TPU v2 and v3, quota is specified in terms of TensorCores. A single Cloud TPU device comprises four TPU chips and eight TensorCores, two TensorCores per TPU chip. TPU v2 and v3 have separate quotas for single devices and for TPU Pods. You cannot use a v2 or v3 TPU Pod quota for v2-8 or v3-8 TPUs. For example, if you have quota for a v3-32 slice, you cannot use it to create four v3-8 TPUs.
For more information about TPU chips and TensorCores, see
[TPU System
architecture](/tpu/docs/system-architecture-tpu-vm).
Quota types
There are separate quotas for reserved, on-demand, and preemptible Cloud TPU resources. The following table compares the features of each type of quota.
|Quota type
|Description
|Default value
|How to request
|Flags for TPU creation
|Reserved
|Quota for reserved TPUs. A reservation provides a high level of assurance in obtaining Cloud TPU capacity. Reserved instances are protected from stockouts but are subject to interruptions. You must have a committed use discount (CUD) to access reserved resources.
|0
|
To request a reservation, fill out the
[flag.](https://cloud.google.com/sdk/gcloud/reference/compute/tpus/tpu-vm/create#--reserved)
--reserved
All others: 0
[Request additional quota](#pod-quota). [Preemptible TPUs](https://cloud.google.com/tpu/docs/preemptible).
All others: 0
[Request additional quota](#pod-quota). [flag or the](https://cloud.google.com/sdk/gcloud/reference/compute/tpus/tpu-vm/create#--preemptible)
--preemptible
[flag for a](https://cloud.google.com/sdk/gcloud/reference/alpha/compute/tpus/queued-resources/create#--best-effort)
--best-effort
[queued resource](https://cloud.google.com/tpu/docs/queued-resources)request.
View and request additional quota
You can view the quota allocated for your Google Cloud project on the
[Quotas page in the
Google Cloud console](https://console.cloud.google.com/apis/api/tpu.googleapis.com/quotas). If you
need additional Cloud TPU quota, you can request it from the Quotas
page. For more information, see [Request a higher quota
limit](https://cloud.google.com/docs/quota_detail/view_manage#requesting_higher_quota).",Quotas | Cloud TPU | Google Cloud,
id,url,body,title,description
146,https://cloud.google.com/tpu/docs/reference/rest/v2alpha1/projects.locations.operations,"Resource: Operation
This resource represents a long-running operation that is the result of a network API call.
|JSON representation
|
{ ""name"": string, ""metadata"": { ""@type"": string, field1: ..., ... }, ""done"": boolean, // Union field
|Fields
|
name
|
The server-assigned name, which is only unique within the same service that originally returns it. If you use the default HTTP mapping, the
|
metadata
|
Service-specific metadata associated with the operation. It typically contains progress information and common metadata such as create time. Some services might not provide such metadata. Any method that returns a long-running operation should document the metadata type, if any.
An object containing fields of an arbitrary type. An additional field
|
done
|
If the value is
|Union field
result. The operation result, which can be either an
error or a valid
response. If
done ==
false, neither
error nor
response is set. If
done ==
true, exactly one of
error or
response can be set. Some services might not provide the result.
result can be only one of the following:
|
error
|
The error result of the operation in case of failure or cancellation.
|
response
|
The normal, successful response of the operation. If the original method returns no data on success, such as
An object containing fields of an arbitrary type. An additional field
|
Methods
|
|Starts asynchronous cancellation on a long-running operation.
|
|Deletes a long-running operation.
|
|Gets the latest state of a long-running operation.
|
|Lists operations that match the specified filter in the request.",REST Resource: projects.locations.operations | Cloud TPU | Google Cloud,
id,url,body,title,description
193,https://cloud.google.com/tpu/docs/tutorials/efficientnet-2.x,"This tutorial shows you how to train a Keras EfficientNet model on
Cloud TPU using
tf.distribute.TPUStrategy.
If you are not familiar with Cloud TPU, it is
strongly recommended that you go through the
[quickstart](https://cloud.google.com/tpu/docs/quickstart) to learn how to
create a Cloud TPU and Compute Engine VM.
Objectives
- Create a Cloud Storage bucket to hold your dataset and model output.
- Prepare a fake imagenet dataset that is similar to the ImageNet dataset.
- Run the training job.
- Verify the output results.
Costs
In this document, you use the following billable components of Google Cloud:
- Compute Engine
- Cloud TPU
- Cloud Storage
To generate a cost estimate based on your projected usage,
use the
[pricing calculator](/products/calculator).
[free trial](/free-trial).
Before you begin
Before starting this tutorial, check that your Google Cloud project is correctly set up.
-
Sign in to your Google Cloud account. If you're new to
Google Cloud,
[create an account](https://console.cloud.google.com/freetrial)to evaluate how our products perform in real-world scenarios. New customers also get $300 in free credits to run, test, and deploy workloads.
-
In the Google Cloud console, on the project selector page, select or
[create a Google Cloud project](/resource-manager/docs/creating-managing-projects).
-
[Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console).
-
In the Google Cloud console, on the project selector page, select or
[create a Google Cloud project](/resource-manager/docs/creating-managing-projects).
-
[Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console).
This walkthrough uses billable components of Google Cloud. Check the
[Cloud TPU pricing page](/tpu/docs/pricing) to
estimate your costs. Be sure to
[clean up](#clean-up)
resources you create when you've finished with them to avoid unnecessary
charges.
Cloud TPU single device training
This section describes how to configure Cloud TPU resources and train the EfficientNet model using a single Cloud TPU device.
Open a Cloud Shell window.
Create a variable for your project's ID.
export PROJECT_ID=project-id
Configure Google Cloud CLI to use the project where you want to create Cloud TPU.
gcloud config set project ${PROJECT_ID}
The first time you run this command in a new Cloud Shell VM, an
Authorize Cloud Shellpage is displayed. Click
Authorizeat the bottom of the page to allow
gcloudto make API calls with your credentials.
For more information on the
gcloudcommand, see the
[gcloud Reference](/sdk/gcloud/reference).
Create a Service Account for the Cloud TPU project.
Service accounts allow the Cloud TPU service to access other Google Cloud services.
gcloud beta services identity create --service tpu.googleapis.com --project $PROJECT_ID
The command returns a Cloud TPU Service Account with following format:
service-PROJECT_NUMBER@cloud-tpu.iam.gserviceaccount.com
Export TPU setup variables
Set the zone where you will train the model and store any training-related data.
$ export ZONE=europe-west4-a
Create a Cloud Storage bucket using the following command:
gsutil mb -p ${PROJECT_ID} -c standard -l europe-west4 gs://bucket-name/
This Cloud Storage bucket stores the data you use to train your model and the training results. The
gcloud compute tpus execution-groupscommand used in this tutorial sets up default permissions for the Cloud TPU Service Account you set up in the previous step. If you want finer-grain permissions, review the
[access level permissions](/tpu/docs/storage-buckets).
The bucket location must be in the same region as your Compute Engine (VM) and your Cloud TPU node.
Prepare your dataset or use fake_imagenet
ImageNet is an image database. The images in the database are organized into a hierarchy, with each node of the hierarchy depicted by hundreds and thousands of images.
This tutorial uses a demonstration version of the full ImageNet dataset, referred to as fake_imagenet. This demonstration version allows you to test the tutorial, while reducing the storage and time requirements typically associated with running a model against the full ImageNet database.
The fake_imagenet dataset is at this location on Cloud Storage:
gs://cloud-tpu-test-datasets/fake_imagenet
The fake_imagenet dataset is only useful for understanding how to use a Cloud TPU and validating end-to-end performance. The accuracy numbers and saved model will not be meaningful.
If you want to use the full ImageNet dataset, see
[Downloading, preprocessing, and uploading the ImageNet dataset](/tpu/docs/imagenet-setup).
Launch TPU resources using the
gcloudcommand. The command you use depends on whether you are using TPU VMs or TPU nodes. For more information on the two VM architecture, see
[System Architecture](/tpu/docs/system-architecture-tpu-vm).
TPU VM
$ gcloud compute tpus tpu-vm create efficientnet-tutorial \ --zone=${ZONE} \ --accelerator-type=v3-8 \ --version=tpu-vm-tf-2.15.0-pjrt
Command flag descriptions
zone
- The
[zone](/tpu/docs/types-zones)where you plan to create your Cloud TPU.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
version
- The Cloud TPU
[software version](/tpu/docs/supported-tpu-versions#tpu_software_versions).
TPU Node
gcloud compute tpus execution-groups create \ --name=efficientnet-tutorial \ --zone=${ZONE} \ --disk-size=300 \ --machine-type=n1-standard-16 \ --tf-version=2.12.0 \ --accelerator-type=v3-8
Command flag descriptions
project
- Your Google Cloud project ID
name
- The name of the Cloud TPU to create.
zone
- The
[zone](/tpu/docs/types-zones)where you plan to create your Cloud TPU.
disk-size
- The size of the hard disk in GB of the VM created by the
gcloudcommand.
machine-type
- The
[machine type](/compute/docs/machine-types)of the Compute Engine VM to create.
tf-version
- The version of Tensorflow
gcloudinstalls on the VM.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
For more information on the
gcloudcommand, see the
[gcloud Reference](/sdk/gcloud/reference).
-
If you are not automatically logged in to the Compute Engine instance, log in by running the following
sshcommand. When you are logged into the VM, your shell prompt changes from
username@projectnameto
username@vm-name:
TPU VM
gcloud compute tpus tpu-vm ssh efficientnet-tutorial --zone=${ZONE}
TPU Node
gcloud compute ssh efficientnet-tutorial --zone=${ZONE}
Set the Cloud TPU name variable.
TPU VM
(vm)$ export TPU_NAME=local
TPU Node
(vm)$ export TPU_NAME=efficientnet-tutorial
Set Cloud Storage bucket variables
Replace bucket-name with the name of your Cloud Storage bucket:
(vm)$ export STORAGE_BUCKET=gs://bucket-name
(vm)$ export MODEL_DIR=${STORAGE_BUCKET}/efficientnet-2x (vm)$ export DATA_DIR=gs://cloud-tpu-test-datasets/fake_imagenet
The training application expects your training data to be accessible in Cloud Storage. The training application also uses your Cloud Storage bucket to store checkpoints during training.
- If you set
--version=tpu-vm-tf-2.15.0-pjrtwhen creating your TPU, set the following environment variables to enable the PJRT runtime:
(vm)$ export NEXT_PLUGGABLE_DEVICE_USE_C_API=true (vm)$ export TF_PLUGGABLE_DEVICE_LIBRARY_PATH=/lib/libtpu.so
Install TensorFlow requirements.
The command you use depends on whether you are using TPU VMs or TPU Nodes.
TPU VM
(vm)$ pip3 install -r /usr/share/tpu/models/official/requirements.txt
TPU Node
(vm)$ pip3 install --user -r /usr/share/models/official/requirements.txt
The EfficientNet training script requires extra packages (TPU VM only). Install them now:
TPU VM
(vm)$ sudo pip3 install tensorflow-addons (vm)$ sudo pip3 install tensorflow-model-optimization>=0.1.3
Set some required environment variables:
TPU VM
(vm)$ export PYTHONPATH=""${PYTHONPATH}:/usr/share/tpu/models""
TPU Node
(vm)$ export PYTHONPATH=""${PYTHONPATH}:/usr/share/models""
The EfficientNet model is pre-installed on your Compute Engine VM.
Change to directory that stores the model:
TPU VM
(vm)$ cd /usr/share/tpu/models/official/legacy/image_classification
TPU Node
(vm)$ cd /usr/share/models/official/legacy/image_classification
Train the model. This uses a fake_imagenet dataset and trains EfficientNet for one epoch.
(vm)$ python3 classifier_trainer.py \ --mode=train_and_eval \ --model_type=efficientnet \ --dataset=imagenet \ --tpu=${TPU_NAME} \ --data_dir=${DATA_DIR} \ --model_dir=${MODEL_DIR} \ --config_file=configs/examples/efficientnet/imagenet/efficientnet-b0-tpu.yaml \ --params_override=""train.epochs=1, train_dataset.builder=records, validation_dataset.builder=records""
Command flag descriptions
mode
- One of
train,
eval, or
train_and_eval.
model_type
- The type of the model. For example,
efficientnet.
dataset
- The name of the dataset. For example,
imagenet.
tpu
- The name of the Cloud TPU to run training or evaluation.
data_dir
- Specifies the Cloud Storage path for training input. It is set to the fake_imagenet dataset in this example.
model_dir
- The Cloud Storage path where checkpoints and summaries are stored during model training. You can reuse an existing folder to load previously generated checkpoints and to store additional checkpoints as long as the previous checkpoints were created using a Cloud TPU of the same size and TensorFlow version.
config_file
- The path to the json file containing the pre-trained EfficientNet model. This file contains the model architecture.
params_override
- A JSON string that overrides default script parameters. For more
information on script parameters, see
/usr/share/models/official/legacy/detection/main.py.
This will train EfficientNet for 1 epoch and will complete on a v3-8 Cloud TPU node in approximately 40 minutes. When the training script completes, output similar to the following appears:
Run stats: { 'accuracy_top_1': 0.0010172526817768812, 'eval_loss': 7.104171276092529, 'loss': 7.113735675811768, 'training_accuracy_top_1': 0.0009773431811481714, 'step_timestamp_log': [ 'BatchTimestamp<batch_index: 0, timestamp: 1604960724.2224622>', 'BatchTimestamp<batch_index: 1251, timestamp: 1604961281.3745298>' ], 'train_finish_time': 1604961342.6359076, 'avg_exp_per_second': 2071.493269569079 }
To train the EfficientNet to convergence on the ImageNet dataset, run it for 90 epochs as shown in the following script. Training and evaluation are done together. Each epoch has 1251 steps for a total of 112590 training steps and 48 evaluation steps.
(vm)$ python3 classifier_trainer.py \ --mode=train_and_eval \ --model_type=efficientnet \ --dataset=imagenet \ --tpu=${TPU_NAME} \ --data_dir=${DATA_DIR} \ --model_dir=${MODEL_DIR} \ --config_file=configs/examples/efficientnet/imagenet/efficientnet-b0-tpu.yaml \ --params_override=""train_dataset.builder=records, validation_dataset.builder=records""
Command flag descriptions
mode
- One of
train,
eval, or
train_and_eval.
model_type
- The type of the model. For example,
efficientnet, etc.
dataset
- The name of the dataset. For example,
imagenet.
tpu
- The name of the Cloud TPU to run training or evaluation.
data_dir
- Specifies the Cloud Storage path for training input. It is set to the fake_imagenet dataset in this example.
model_dir
- The Cloud Storage path where checkpoints and summaries are stored during model training. You can reuse an existing folder to load previously generated checkpoints and to store additional checkpoints as long as the previous checkpoints were created using a Cloud TPU of the same size and TensorFlow version.
config_file
- The path to the JSON file containing the pre-trained EfficientNet model. This file contains the model architecture.
params_override
- A JSON string that overrides default script parameters. For more
information on script parameters, see
/usr/share/models/official/legacy/detection/main.py.
Since the training was done on the fake_imagenet dataset, the output results do not reflect actual output that would appear if the training was performed on a real dataset.
You have now completed single-device training. Use the following steps to delete the current single-device TPU resources.
-
Disconnect from the Compute Engine instance:
(vm)$ exit
Your prompt should now be
username@projectname, showing you are in the Cloud Shell.
Delete the TPU resource.
TPU VM
$ gcloud compute tpus tpu-vm delete efficientnet-tutorial \ --zone=${ZONE}
Command flag descriptions
zone
- The
[zone](https://cloud.google.com/tpu/docs/types-zones)where your Cloud TPU resided.
TPU Node
$ gcloud compute tpus execution-groups delete efficientnet-tutorial \ --tpu-only \ --zone=${ZONE}
Command flag descriptions
tpu-only
- Deletes only the Cloud TPU. The VM remains available.
zone
- The
[zone](https://cloud.google.com/tpu/docs/types-zones)that contains the TPU to delete.
-
At this point, you can either conclude this tutorial and
[clean up](#cleanup),
or you can continue and explore running the model on Cloud TPU Pods.
Scale your model with Cloud TPU Pods
Training your model on Cloud TPU Pods may require some changes
to your training script. For information, see
[Training on TPU Pods](/tpu/docs/training-on-tpu-pods).
Cloud TPU Pod training
This section provides information on setting up a Cloud Storage bucket and Cloud TPU resources for Pod training.
Open a Cloud Shell window.
Create a variable for your project's ID.
export PROJECT_ID=project-id
Configure Google Cloud CLI to use the project where you want to create Cloud TPU.
gcloud config set project ${PROJECT_ID}
The first time you run this command in a new Cloud Shell VM, an
Authorize Cloud Shellpage is displayed. Click
Authorizeat the bottom of the page to allow
gcloudto make Google Cloud API calls with your credentials.
Create a Service Account for the Cloud TPU project.
Service accounts allow the Cloud TPU service to access other Google Cloud services.
gcloud beta services identity create --service tpu.googleapis.com --project $PROJECT_ID
The command returns a Cloud TPU Service Account with following format:
service-PROJECT_NUMBER@cloud-tpu.iam.gserviceaccount.com
Create a Cloud Storage bucket using the following command or use a bucket you created earlier for your project:
gsutil mb -p ${PROJECT_ID} -c standard -l europe-west4 gs://bucket-name
This Cloud Storage bucket stores the data you use to train your model and the training results. The
gcloudcommand used in this tutorial sets up default permissions for the Cloud TPU Service Account you set up in the previous step. If you want finer-grain permissions, review the
[access level permissions](/tpu/docs/storage-buckets).
The bucket location must be in the same region as your TPU VM.
Export TPU setup variables
Set the zone where you will train the model and store any training-related data.
$ export ZONE=europe-west4-a
Prepare your dataset or use fake_imagenet
ImageNet is an image database. The images in the database are organized into a hierarchy, with each node of the hierarchy depicted by hundreds and thousands of images.
The default Pod training accesses a demonstration version of the full ImageNet dataset, referred to as fake_imagenet. This demonstration version allows you to test Pod training, while reducing the storage and time requirements typically associated with training a model against the full ImageNet database.
The fake_imagenet dataset is only useful for understanding how to use a Cloud TPU and validating end-to-end performance. The accuracy numbers and saved model will not be meaningful.
If you want to use the full ImageNet dataset, see
[Downloading, preprocessing, and uploading the ImageNet dataset](/tpu/docs/imagenet-setup).
Launch your Cloud TPU resources using the
gcloudcommand.
The command you use depends on whether you are using a TPU VM or a TPU node. For more information on the two VM architecture, see
[System Architecture](/tpu/docs/system-architecture-tpu-vm). For more information on the
gcloudcommand, see the
[gcloud Reference](/sdk/gcloud/reference). This tutorial specifies a v3-32 Pod. For other Pod options, see the [available TPU types page](/tpu/docs/supported-tpu-configurations).
TPU VM
$ gcloud compute tpus tpu-vm create efficientnet-tutorial \ --zone=${ZONE} \ --accelerator-type=v3-32 \ --version=tpu-vm-tf-2.15.0-pod-pjrt
Command flag descriptions
zone
- The
[zone](/tpu/docs/types-zones)where you plan to create your Cloud TPU.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
version
- The Cloud TPU
[software version](/tpu/docs/supported-tpu-versions#tpu_software_versions).
TPU Node
(vm)$ gcloud compute tpus execution-groups create --name=efficientnet-tutorial \ --accelerator-type=v3-32 \ --zone=${ZONE} \ --tf-version=2.12.0
Command flag descriptions
name
- The name of the Cloud TPU to create.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
zone
- The
[zone](/tpu/docs/types-zones)where you plan to create your Cloud TPU.
tf-version
- The version of Tensorflow
gcloudinstalls on the VM.
tpu-only
- Create a Cloud TPU only. By default the
gcloudcommand creates a VM and a Cloud TPU.
-
If you are not automatically logged in to the Compute Engine instance, log in by running the following
sshcommand. When you are logged into the VM, your shell prompt changes from
username@projectnameto
username@vm-name:
TPU VM
$ gcloud compute tpus tpu-vm ssh efficientnet-tutorial --zone=${ZONE}
TPU Node
$ gcloud compute ssh efficientnet-tutorial --zone=${ZONE}
As you continue these instructions, run each command that begins with
(vm)$in your VM session window.
Export TPU setup variables:
(vm)$ export STORAGE_BUCKET=gs://bucket-name
(vm)$ export TPU_NAME=efficientnet-tutorial (vm)$ export DATA_DIR=gs://cloud-tpu-test-datasets/fake_imagenet (vm)$ export MODEL_DIR=${STORAGE_BUCKET}/efficientnet-2x-pod
The training application expects your training data to be accessible in Cloud Storage. The training application also uses your Cloud Storage bucket to store checkpoints during training.
Install TensorFlow requirements.
TPU VM
(vm)$ pip3 install -r /usr/share/tpu/models/official/requirements.txt
TPU Node
(vm)$ pip3 install -r /usr/share/models/official/requirements.txt
Set some required environment variables:
TPU VM
(vm)$ export PYTHONPATH=""/usr/share/tpu/models:${PYTHONPATH}"" (vm)$ export TPU_LOAD_LIBRARY=0
TPU Node
(vm)$ export PYTHONPATH=""${PYTHONPATH}:/usr/share/models""
The EfficientNet model is pre-installed on your Compute Engine VM.
Change to directory that stores the model:
TPU VM
(vm)$ cd /usr/share/tpu/models/official/legacy/image_classification/
TPU Node
(vm)$ cd /usr/share/models/official/legacy/image_classification/
Train the model.
(vm)$ python3 classifier_trainer.py \ --mode=train_and_eval \ --model_type=efficientnet \ --dataset=imagenet \ --tpu=${TPU_NAME} \ --data_dir=${DATA_DIR} \ --model_dir=${MODEL_DIR} \ --config_file=configs/examples/efficientnet/imagenet/efficientnet-b0-tpu.yaml \ --params_override=""train.epochs=1, train_dataset.builder=records, validation_dataset.builder=records""
Command flag descriptions
mode
- When set to
train_and_evalthis script trains and evaluates the model. When set to
export_onlythis script exports a saved model.
model_type
- The type of the model. For example,
efficientnet, etc.
dataset
- The name of the dataset. For example,
imagenet.
tpu
- Uses the name specified in the TPU_NAME variable.
data_dir
- Specifies the Cloud Storage path for training input. It is set to the fake_imagenet dataset in this example.
model_dir
- The Cloud Storage path where checkpoints and summaries are stored during model training. You can reuse an existing folder to load previously generated checkpoints and to store additional checkpoints as long as the previous checkpoints were created using a Cloud TPU of the same size and TensorFlow version.
config_file
- The path to the json file containing the pre-trained EfficientNet model. This file contains the model architecture.
params_override
- A JSON string that overrides default script parameters. For more
information on script parameters, see
/usr/share/models/official/legacy/detection/main.py.
-
The procedure trains the model on the fake_imagenet dataset to 1 epoch (312 total training steps and 12 evaluation steps). This training takes approximately 2 minutes on a v3-32 Cloud TPU. When the training and evaluation complete, a message similar to the following appears:
Run stats: { 'accuracy_top_1': 0.0009969075908884406, 'eval_loss': 7.105168342590332, 'loss': 7.114983081817627, 'training_accuracy_top_1': 0.0010031675919890404, 'step_timestamp_log': [ 'BatchTimestamp<batch_index: 0, timestamp: 1605041621.4997303>', 'BatchTimestamp<batch_index: 312, timestamp: 1605041970.8633356>' ], 'train_finish_time': 1605042032.2274444, 'avg_exp_per_second': 3111.5120716536226 }
Clean up
To avoid incurring charges to your Google Cloud account for the resources used in this tutorial, either delete the project that contains the resources, or keep the project and delete the individual resources.
Disconnect from the Compute Engine instance, if you have not already done so:
(vm)$ exit
Your prompt should now be
username@projectname, showing you are in the Cloud Shell.
Delete your Cloud TPU and Compute Engine resources.
TPU VM
$ gcloud compute tpus tpu-vm delete efficientnet-tutorial \ --zone=${ZONE}
TPU Node
$ gcloud compute tpus execution-groups delete efficientnet-tutorial \ --zone=${ZONE}
Verify the resources have been deleted by running
gcloud compute tpus execution-groups list. The deletion might take several minutes. The output from the following command should not include any of the TPU resources created in this tutorial:
$ gcloud compute tpus execution-groups list --zone=${ZONE}
Delete your Cloud Storage bucket using
gsutilas shown below. Replace bucket-name with the name of your Cloud Storage bucket.
$ gsutil rm -r gs://bucket-name
What's next
The TensorFlow Cloud TPU tutorials generally train the model using a
sample dataset. The results of this training are not usable
for inference. To use a model for inference, you can train the data on a
publicly available dataset or your own data set. TensorFlow models
trained on Cloud TPUs generally require datasets to be in
[TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) format.
You can use the
[dataset conversion tool sample](https://cloud.google.com/tpu/docs/classification-data-conversion) to convert an image classification dataset into TFRecord format. If you are not using an image classification model, you will have to convert your dataset to [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) format yourself. For more information, see [TFRecord and tf.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord).
Hyperparameter tuning
To improve the model's performance with your dataset, you can tune
the model's hyperparameters. You can find information about hyperparameters
common to all TPU supported models on
[GitHub](https://github.com/tensorflow/tpu/tree/master/models/hyperparameters).
Information about model-specific hyperparameters can be found in the
[source code](https://github.com/tensorflow/tpu/tree/master/models/official)
for each model. For more information on hyperparameter tuning, see
[Overview of hyperparameter tuning](https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview),
[Using the Hyperparameter tuning service](https://cloud.google.com/ai-platform/training/docs/using-hyperparameter-tuning),
and [Tune hyperparameters](https://developers.google.com/machine-learning/guides/text-classification/step-5).
Inference
Once you have trained your model you can use it for inference
(also called prediction).
[AI Platform](https://cloud.google.com/ai-platform/docs/technical-overview)
is a cloud-based solution for developing,
[training](https://cloud.google.com/ai-platform/training/docs),
and [deploying](https://cloud.google.com/ai-platform/prediction/docs/deploying-models)
machine learning models. Once a model is deployed, you can use the
[AI Platform Prediction service](https://cloud.google.com/ai-platform/prediction/docs).
Learn how to train and evaluate using your own data in place of the fake_imagenet or ImageNet datasets by following the
[dataset conversion tutorial](/tpu/docs/classification-data-conversion). The tutorial explains how to use the image classification data converter example script to convert a raw dataset for image classification into TFRecords usable by Cloud TPU Tensorflow models.
Run a Cloud TPU
[colab](https://colab.sandbox.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/03_Flower_pictures_to_TFRecords.ipynb)that demonstrates how to run an image classification model using your own image data.
Explore the other
[Cloud TPU tutorials](/tpu/docs/tutorials).
Learn to use the
[TPU monitoring tools in TensorBoard](/tpu/docs/cloud-tpu-tools).",Training EfficientNet on Cloud TPU (TF 2.x) | Google Cloud,
id,url,body,title,description
16,https://cloud.google.com/tpu/docs/reference/rest/v1alpha1/projects.locations.nodes,"[Resource: Node](#Node) [State](#State) [SchedulingConfig](#SchedulingConfig) [NetworkEndpoint](#NetworkEndpoint) [Health](#Health) [ApiVersion](#ApiVersion) [Symptom](#Symptom) [SymptomType](#SymptomType) [Methods](#METHODS_SUMMARY)
Resource: Node
A TPU instance.
|JSON representation
|
{ ""name"": string, ""description"": string, ""acceleratorType"": string, ""ipAddress"": string, ""port"": string, ""state"": enum (
|Fields
|
name
|
Output only. Immutable. The name of the TPU
|
description
|
The user-supplied description of the TPU. Maximum of 512 characters.
|
acceleratorType
|
Required. The type of hardware accelerators associated with this node.
|
ipAddress
|
Output only. DEPRECATED! Use networkEndpoints instead. The network address for the TPU Node as visible to Compute Engine instances.
|
port
|
Output only. DEPRECATED! Use networkEndpoints instead. The network port for the TPU Node as visible to Compute Engine instances.
|
state
|
Output only. The current state for the TPU Node.
|
healthDescription
|
Output only. If this field is populated, it contains a description of why the TPU Node is unhealthy.
|
tensorflowVersion
|
Required. The version of Tensorflow running in the Node.
|
network
|
The name of a network they wish to peer the TPU node to. It must be a preexisting Compute Engine network inside of the project on which this API has been activated. If none is provided, ""default"" will be used.
|
cidrBlock
|
The CIDR block that the TPU node will use when selecting an IP address. This CIDR block must be a /29 block; the Compute Engine networks API forbids a smaller block, and using a larger block would be wasteful (a node can only consume one IP address). Errors will occur if the CIDR block has already been used for a currently existing TPU node, the CIDR block conflicts with any subnetworks in the user's provided network, or the provided network is peered with another network that is using that CIDR block.
|
serviceAccount
|
Output only. The service account used to run the tensor flow services within the node. To share resources, including Google Cloud Storage data, with the Tensorflow job running in the Node, this account must have permissions to that data.
|
createTime
|
Output only. The time when the node was created.
A timestamp in RFC3339 UTC ""Zulu"" format, with nanosecond resolution and up to nine fractional digits. Examples:
|
schedulingConfig
|
The scheduling options for this node.
|
networkEndpoints[]
|
Output only. The network endpoints where TPU workers can be accessed and sent work. It is recommended that Tensorflow clients of the node reach out to the 0th entry in this map first.
|
health
|
The health status of the TPU node.
|
labels
|
Resource labels to represent user-provided metadata.
An object containing a list of
|
useServiceNetworking
|
Whether the VPC peering for the node is set up through Service Networking API. The VPC Peering should be set up before provisioning the node. If this field is set, cidrBlock field should not be specified. If the network, that you want to peer the TPU Node to, is Shared VPC networks, the node must be created with this this field enabled.
|
apiVersion
|
Output only. The API version that created this Node.
|
symptoms[]
|
Output only. The Symptoms that have occurred to the TPU Node.
State
Represents the different states of a TPU node during its lifecycle.
|Enums
|
STATE_UNSPECIFIED
|TPU node state is not known/set.
|
CREATING
|TPU node is being created.
|
READY
|TPU node has been created.
|
RESTARTING
|TPU node is restarting.
|
REIMAGING
|TPU node is undergoing reimaging.
|
DELETING
|TPU node is being deleted.
|
REPAIRING
|TPU node is being repaired and may be unusable. Details can be found in the
help_description field.
|
STOPPED
|TPU node is stopped.
|
STOPPING
|TPU node is currently stopping.
|
STARTING
|TPU node is currently starting.
|
PREEMPTED
|TPU node has been preempted. Only applies to Preemptible TPU Nodes.
|
TERMINATED
|TPU node has been terminated due to maintenance or has reached the end of its life cycle (for preemptible nodes).
|
HIDING
|TPU node is currently hiding.
|
HIDDEN
|TPU node has been hidden.
|
UNHIDING
|TPU node is currently unhiding.
SchedulingConfig
Sets the scheduling options for this node.
|JSON representation
|
{ ""preemptible"": boolean, ""reserved"": boolean }
|Fields
|
preemptible
|
Defines whether the node is preemptible.
|
reserved
|
Whether the node is created under a reservation.
NetworkEndpoint
A network endpoint over which a TPU worker can be reached.
|JSON representation
|
{ ""ipAddress"": string, ""port"": integer }
|Fields
|
ipAddress
|
The IP address of this network endpoint.
|
port
|
The port of this network endpoint.
Health
Health defines the status of a TPU node as reported by Health Monitor.
|Enums
|
HEALTH_UNSPECIFIED
|Health status is unknown: not initialized or failed to retrieve.
|
HEALTHY
|The resource is healthy.
|
DEPRECATED_UNHEALTHY
|The resource is unhealthy.
|
TIMEOUT
|The resource is unresponsive.
|
UNHEALTHY_TENSORFLOW
|The in-guest ML stack is unhealthy.
|
UNHEALTHY_MAINTENANCE
|The node is under maintenance/priority boost caused rescheduling and will resume running once rescheduled.
ApiVersion
TPU API Version.
|Enums
|
API_VERSION_UNSPECIFIED
|API version is unknown.
|
V1_ALPHA1
|TPU API V1Alpha1 version.
|
V1
|TPU API V1 version.
|
V2_ALPHA1
|TPU API V2Alpha1 version.
Symptom
A Symptom instance.
|JSON representation
|
{
""createTime"": string,
""symptomType"": enum (
|Fields
|
createTime
|
Timestamp when the Symptom is created.
A timestamp in RFC3339 UTC ""Zulu"" format, with nanosecond resolution and up to nine fractional digits. Examples:
|
symptomType
|
Type of the Symptom.
|
details
|
Detailed information of the current Symptom.
|
workerId
|
A string used to uniquely distinguish a worker within a TPU node.
SymptomType
SymptomType represents the different types of Symptoms that a TPU can be at.
|Enums
|
SYMPTOM_TYPE_UNSPECIFIED
|Unspecified symptom.
|
LOW_MEMORY
|TPU VM memory is low.
|
OUT_OF_MEMORY
|TPU runtime is out of memory.
|
EXECUTE_TIMED_OUT
|TPU runtime execution has timed out.
|
MESH_BUILD_FAIL
|TPU runtime fails to construct a mesh that recognizes each TPU device's neighbors.
|
HBM_OUT_OF_MEMORY
|TPU HBM is out of memory.
|
PROJECT_ABUSE
|Abusive behaviors have been identified on the current project.
|
Methods
|
|Creates a node.
|
|Deletes a node.
|
|Gets the details of a node.
|
|Lists nodes.
|
|Reimages a node's OS.
|
|Starts a node.
|
|Stops a node.",REST Resource: projects.locations.nodes | Cloud TPU | Google Cloud,
id,url,body,title,description
186,https://cloud.google.com/tpu/docs/reference/rest/v2/projects.locations/list,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Query parameters](#body.QUERY_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [Try it!](#try-it)
Lists information about the supported locations for this service.
HTTP request
GET https://tpu.googleapis.com/v2/{name=projects/*}/locations
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
The resource that owns the locations collection, if applicable.
Query parameters
|Parameters
|
filter
|
A filter to narrow down results to a preferred subset. The filtering language accepts strings like
|
pageSize
|
The maximum number of results to return. If not set, the service selects a default.
|
pageToken
|
A page token received from the
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[ListLocationsResponse](/tpu/docs/reference/rest/Shared.Types/ListLocationsResponse)
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).",Method: projects.locations.list | Cloud TPU | Google Cloud,
id,url,body,title,description
174,https://cloud.google.com/tpu/docs/tutorials/dlrm-dcn-2.x,"This tutorial shows how to train DLRM and DCN v2 ranking models
which can be used for tasks such as click-through rate (CTR) prediction.
See the note in
[Set up to run the DLRM or DCN model](#run-model) to
see how to set parameters to train either a DLRM or a DCN v2 ranking model.
The model inputs are numerical and categorical features, and output is a scalar (for example click probability). The model can be trained and evaluated on Cloud TPU. The deep ranking models are both memory intensive (for embedding tables/lookup) and compute intensive for deep networks (MLPs). TPUs are designed for both.
The model uses a TPUEmbedding layer for categorical features. TPU embedding supports large embedding tables with fast lookup, the size of embedding tables scales linearly with the size of a TPU pod. Up to 90 GB embedding tables can be used for TPU v3-8, 5.6 TB for a v3-512 Pod, and 22.4 TB for a v3-2048 TPU Pod.
The model code is in the
[TensorFlow Recommenders library](https://github.com/tensorflow/recommenders/tree/main/tensorflow_recommenders/experimental/models),
while input pipeline, configuration and training loop is described in
the [TensorFlow Model Garden](https://github.com/tensorflow/models/tree/master/official/recommendation/ranking).
Objectives
- Set up the training environment
- Run the training job using synthetic data
- Verify the output results
Costs
In this document, you use the following billable components of Google Cloud:
- Compute Engine
- Cloud TPU
- Cloud Storage
To generate a cost estimate based on your projected usage,
use the
[pricing calculator](/products/calculator).
[free trial](/free-trial).
Before you begin
Before starting this tutorial, check that your Google Cloud project is correctly set up.
-
Sign in to your Google Cloud account. If you're new to
Google Cloud,
[create an account](https://console.cloud.google.com/freetrial)to evaluate how our products perform in real-world scenarios. New customers also get $300 in free credits to run, test, and deploy workloads.
-
In the Google Cloud console, on the project selector page, select or
[create a Google Cloud project](/resource-manager/docs/creating-managing-projects).
-
[Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console).
-
In the Google Cloud console, on the project selector page, select or
[create a Google Cloud project](/resource-manager/docs/creating-managing-projects).
-
[Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console).
This walkthrough uses billable components of Google Cloud. Check the
[Cloud TPU pricing page](/tpu/docs/pricing) to
estimate your costs. Be sure to
[clean up](#clean-up)
resources you create when you've finished with them to avoid unnecessary
charges.
Set up your resources
This section provides information on setting up Cloud Storage bucket, VM, and Cloud TPU resources used by this tutorial.
Open a Cloud Shell window.
Create a variable for your project's ID.
export PROJECT_ID=project-id
Configure Google Cloud CLI to use the project where you want to create Cloud TPU.
gcloud config set project ${PROJECT_ID}
The first time you run this command in a new Cloud Shell VM, an
Authorize Cloud Shellpage is displayed. Click
Authorizeat the bottom of the page to allow
gcloudto make API calls with your credentials.
Create a Service Account for the Cloud TPU project.
gcloud beta services identity create --service tpu.googleapis.com --project $PROJECT_ID
The command returns a Cloud TPU Service Account with following format:
service-PROJECT_NUMBER@cloud-tpu.iam.gserviceaccount.com
Create a Cloud Storage bucket using the following command where the
-loption specifies the region where the bucket should be created. See the
[types and zones](/tpu/docs/types-zones)for more details on zones and regions:
gsutil mb -p ${PROJECT_ID} -c standard -l europe-west4 gs://bucket-name
This Cloud Storage bucket stores the data you use to train your model and the training results. The
gcloud compute tpus execution-groupstool used in this tutorial sets up default permissions for the Cloud TPU Service Account you set up in the previous step. If you want finer-grain permissions, review the
[access level permissions](/tpu/docs/storage-buckets).
The bucket location must be in the same region as your Compute Engine (VM) and your Cloud TPU node.
Launch a Compute Engine VM and Cloud TPU using the
gcloudcommand. The command you use depends on whether you are using a TPU VM or a TPU node. For more information on the two VM architecture, see
[System Architecture](/tpu/docs/system-architecture-tpu-vm). For more information on the
gcloudcommand, see the
[gcloud Reference](/sdk/gcloud/reference).
TPU VM
$ gcloud compute tpus tpu-vm create dlrm-dcn-tutorial \ --zone=europe-west4-a \ --accelerator-type=v3-8 \ --version=tpu-vm-tf-2.15.0-se
Command flag descriptions
zone
- The
[zone](/tpu/docs/types-zones)where you plan to create your Cloud TPU.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
version
- The Cloud TPU
[software version](/tpu/docs/supported-tpu-versions#tpu_software_versions).
TPU Node
$ gcloud compute tpus execution-groups create \ --name=dlrm-dcn-tutorial \ --zone=europe-west4-a \ --disk-size=300 \ --machine-type=n1-standard-8 \ --tf-version=2.12.0
Command flag descriptions
vm-only
- Create a VM only. By default the
gcloud compute tpus execution-groupscommand creates a VM and a Cloud TPU.
name
- The name of the Cloud TPU to create.
zone
- The
[zone](/tpu/docs/types-zones)where you plan to create your Cloud TPU.
disk-size
- The size of the hard disk in GB of the VM created by the
gcloud compute tpus execution-groupscommand.
machine-type
- The
[machine type](/compute/docs/machine-types)of the Compute Engine VM to create.
tf-version
- The version of TensorFlow
ctpuinstalls on the VM.
-
If you are not automatically logged in to the Compute Engine instance, log in by running the following
sshcommand. When you are logged into the VM, your shell prompt changes from
username@projectnameto
username@vm-name:
TPU VM
gcloud compute tpus tpu-vm ssh dlrm-dcn-tutorial --zone=europe-west4-a
TPU Node
gcloud compute ssh dlrm-dcn-tutorial --zone=europe-west4-a
As you continue these instructions, run each command that begins with
(vm)$in your VM session window.
Set Cloud Storage bucket variables
Set up the following environment variables, replacing bucket-name with the name of your Cloud Storage bucket:
(vm)$ export STORAGE_BUCKET=gs://bucket-name (vm)$ export PYTHONPATH=""/usr/share/tpu/models/:${PYTHONPATH}"" (vm)$ export EXPERIMENT_NAME=dlrm-exp
Set an environment variable for the TPU name.
TPU VM
(vm)$ export TPU_NAME=local
TPU Node
(vm)$ export TPU_NAME=dlrm-dcn-tutorial
The training application expects your training data to be accessible in Cloud Storage. The training application also uses your Cloud Storage bucket to store checkpoints during training.
Set up to run the DLRM or DCN model with synthetic data
The model can be trained on various datasets. Two commonly used ones
are
[Criteo Terabyte](https://labs.criteo.com/2013/12/download-terabyte-click-logs/)
and [Criteo Kaggle](https://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset/).
This tutorial trains on synthetic data by setting the flag
use_synthetic_data=True.
The synthetic dataset is only useful for understanding how to use a Cloud TPU and validating end-to-end performance. The accuracy numbers and saved model won't be meaningful.
Visit the
[Criteo Terabyte](https://labs.criteo.com/2013/12/download-terabyte-click-logs/)
and [Criteo Kaggle](https://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset/)
websites for information on how to download and
[preprocess](https://github.com/tensorflow/models/tree/master/official/recommendation/ranking#preprocess-the-data)
these datasets.
Install required packages.
(vm)$ pip3 install tensorflow-recommenders (vm)$ pip3 install -r /usr/share/tpu/models/official/requirements.txt
Change to the script directory.
TPU VM
(vm)$ cd /usr/share/tpu/models/official/recommendation/ranking
TPU Node
(vm)$ cd /usr/share/models/official/recommendation/ranking
Run the training script. This uses a fake, Criteo-like dataset to train the DLRM model. The training takes approximately 20 minutes.
export EMBEDDING_DIM=32 python3 train.py --mode=train_and_eval \ --model_dir=${STORAGE_BUCKET}/model_dirs/${EXPERIMENT_NAME} --params_override="" runtime: distribution_strategy: 'tpu' task: use_synthetic_data: true train_data: input_path: '${DATA_DIR}/train/*' global_batch_size: 16384 validation_data: input_path: '${DATA_DIR}/eval/*' global_batch_size: 16384 model: num_dense_features: 13 bottom_mlp: [512,256,${EMBEDDING_DIM}] embedding_dim: ${EMBEDDING_DIM} top_mlp: [1024,1024,512,256,1] interaction: 'dot' vocab_sizes: [39884406, 39043, 17289, 7420, 20263, 3, 7120, 1543, 63, 38532951, 2953546, 403346, 10, 2208, 11938, 155, 4, 976, 14, 39979771, 25641295, 39664984, 585935, 12972, 108, 36] trainer: use_orbit: false validation_interval: 1000 checkpoint_interval: 1000 validation_steps: 500 train_steps: 1000 steps_per_loop: 1000 ""
This training runs for approximately 10 minutes on a v3-8 TPU. When it completes, you will see messages similar to the following:
I0621 21:32:58.519792 139675269142336 tpu_embedding_v2_utils.py:907] Done with log of TPUEmbeddingConfiguration. I0621 21:32:58.540874 139675269142336 tpu_embedding_v2.py:389] Done initializing TPU Embedding engine. 1000/1000 [==============================] - 335s 335ms/step - auc: 0.7360 - accuracy: 0.6709 - prediction_mean: 0.4984 - label_mean: 0.4976 - loss: 0.0734 - regularization_loss: 0.0000e+00 - total_loss: 0.0734 - val_auc: 0.7403 - val_accuracy: 0.6745 - val_prediction_mean: 0.5065 - val_label_mean: 0.4976 - val_loss: 0.0749 - val_regularization_loss: 0.0000e+00 - val_total_loss: 0.0749 Model: ""ranking"" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= tpu_embedding (TPUEmbedding) multiple 1 _________________________________________________________________ mlp (MLP) multiple 154944 _________________________________________________________________ mlp_1 (MLP) multiple 2131969 _________________________________________________________________ dot_interaction (DotInteract multiple 0 _________________________________________________________________ ranking_1 (Ranking) multiple 0 ================================================================= Total params: 2,286,914 Trainable params: 2,286,914 Non-trainable params: 0 _________________________________________________________________ I0621 21:43:54.977140 139675269142336 train.py:177] Train history: {'auc': [0.7359596490859985], 'accuracy': [0.67094486951828], 'prediction_mean': [0.4983849823474884], 'label_mean': [0.4975697994232178], 'loss': [0.07338511198759079], 'regularization_loss': [0], 'total_loss': [0.07338511198759079], 'val_auc': [0.7402724623680115], 'val_accuracy': [0.6744520664215088], 'val_prediction_mean': [0.5064718723297119], 'val_label_mean': [0.4975748658180237], 'val_loss': [0.07486172765493393], 'val_regularization_loss': [0], 'val_total_loss': [0.07486172765493393]}
Clean up
To avoid incurring charges to your Google Cloud account for the resources used in this tutorial, either delete the project that contains the resources, or keep the project and delete the individual resources.
Disconnect from the Compute Engine instance, if you have not already done so:
(vm)$ exit
Your prompt should now be
username@projectname, showing you are in the Cloud Shell.
Delete your Cloud TPU and Compute Engine resources. The command you use to delete your resources depends upon whether you are using TPU VMs or TPU Nodes. For more information, see
[System Architecture](/tpu/docs/system-architecture-tpu-vm).
TPU VM
$ gcloud compute tpus tpu-vm delete dlrm-dcn-tutorial \ --zone=europe-west4-a
TPU Node
$ gcloud compute tpus execution-groups delete dlrm-dcn-tutorial \ --zone=europe-west4-a
Verify the resources have been deleted by running
gcloud compute tpus execution-groups list. The deletion might take several minutes. The output from the following command shouldn't include any of the resources created in this tutorial:
TPU VM
$ gcloud compute tpus tpu-vm list --zone=europe-west4-a
TPU Node
$ gcloud compute tpus execution-groups list --zone=europe-west4-a
Delete your Cloud Storage bucket using
gsutil. Replace bucket-name with the name of your Cloud Storage bucket.
$ gsutil rm -r gs://bucket-name
What's next
The TensorFlow Cloud TPU tutorials generally train the model using a
sample dataset. The results of this training are not usable
for inference. To use a model for inference, you can train the data on a
publicly available dataset or your own data set. TensorFlow models
trained on Cloud TPUs generally require datasets to be in
[TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) format.
You can use the
[dataset conversion tool sample](https://cloud.google.com/tpu/docs/classification-data-conversion) to convert an image classification dataset into TFRecord format. If you are not using an image classification model, you will have to convert your dataset to [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) format yourself. For more information, see [TFRecord and tf.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord).
Hyperparameter tuning
To improve the model's performance with your dataset, you can tune
the model's hyperparameters. You can find information about hyperparameters
common to all TPU supported models on
[GitHub](https://github.com/tensorflow/tpu/tree/master/models/hyperparameters).
Information about model-specific hyperparameters can be found in the
[source code](https://github.com/tensorflow/tpu/tree/master/models/official)
for each model. For more information on hyperparameter tuning, see
[Overview of hyperparameter tuning](https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview),
[Using the Hyperparameter tuning service](https://cloud.google.com/ai-platform/training/docs/using-hyperparameter-tuning),
and [Tune hyperparameters](https://developers.google.com/machine-learning/guides/text-classification/step-5).
Inference
Once you have trained your model you can use it for inference
(also called prediction).
[AI Platform](https://cloud.google.com/ai-platform/docs/technical-overview)
is a cloud-based solution for developing,
[training](https://cloud.google.com/ai-platform/training/docs),
and [deploying](https://cloud.google.com/ai-platform/prediction/docs/deploying-models)
machine learning models. Once a model is deployed, you can use the
[AI Platform Prediction service](https://cloud.google.com/ai-platform/prediction/docs).",Training DLRM and DCN on Cloud TPU (TF 2.x) | Google Cloud,
id,url,body,title,description
135,https://cloud.google.com/tpu/docs/SAX-on-inference-converter,"SAX on Cloud TPU v5e
SAX cluster (SAX cell)
SAX admin server and SAX model server are two essential components that run a SAX cluster.
SAX admin server
The SAX admin server monitors and coordinates all SAX model servers in a SAX cluster. In a SAX cluster, you can launch multiple SAX admin servers, where only one of the SAX admin server is active through leader election, the others are standby servers. When the active admin server fails, a standby admin server will become active. The active SAX admin server assigns model replicas and inference requests to available SAX model servers.
SAX admin storage bucket
Each SAX cluster requires a Cloud Storage bucket to store the configurations and locations of SAX admin servers and SAX model servers in the SAX cluster.
SAX model server
The SAX model server loads a model checkpoint and runs inference with
[GSPMD](https://arxiv.org/abs/2105.04663). A SAX model server runs on a single
TPU VM worker. Single-host TPU model serving requires a single SAX model
server on a single-host TPU VM. Multi-host TPU model serving requires a
group of SAX model servers on a multi-host TPU slice. Multi-host model serving
is currently not available, but this document does provide [an example
with a 175B test model](#multi-host-preview) for preview.
SAX model serving
The following section walks through the workflow for serving language models using SAX. It uses the GPT-J 6B model as an example for single-host model serving.
Before starting, install the Cloud TPU SAX Docker images on your TPU VM:
sudo usermod -a -G docker ${USER} newgrp docker gcloud auth configure-docker us-docker.pkg.dev SAX_ADMIN_SERVER_IMAGE_NAME=""us-docker.pkg.dev/cloud-tpu-images/inference/sax-admin-server"" SAX_MODEL_SERVER_IMAGE_NAME=""us-docker.pkg.dev/cloud-tpu-images/inference/sax-model-server"" SAX_UTIL_IMAGE_NAME=""us-docker.pkg.dev/cloud-tpu-images/inference/sax-util"" SAX_VERSION=v1.0.0 export SAX_ADMIN_SERVER_IMAGE_URL=${SAX_ADMIN_SERVER_IMAGE_NAME}:${SAX_VERSION} export SAX_MODEL_SERVER_IMAGE_URL=${SAX_MODEL_SERVER_IMAGE_NAME}:${SAX_VERSION} export SAX_UTIL_IMAGE_URL=""${SAX_UTIL_IMAGE_NAME}:${sax_version}"" docker pull ${SAX_ADMIN_SERVER_IMAGE_URL} docker pull ${SAX_MODEL_SERVER_IMAGE_URL} docker pull ${SAX_UTIL_IMAGE_URL}
Set some other variables you will use later:
export SAX_ADMIN_SERVER_DOCKER_NAME=""sax-admin-server"" export SAX_MODEL_SERVER_DOCKER_NAME=""sax-model-server"" export SAX_CELL=""/sax/test""
GPT-J 6B single-host model serving example
Single-host model serving is applicable to single-host TPU slice, that is, v5litepod-1, v5litepod-4 and v5litepod-8.
Create a SAX cluster
Create a Cloud Storage storage bucket for the SAX cluster:
SAX_ADMIN_STORAGE_BUCKET=${your_admin_storage_bucket} gcloud storage buckets create gs://${SAX_ADMIN_STORAGE_BUCKET} \ --project=${PROJECT_ID}
You might need another Cloud Storage storage bucket to store the checkpoint.
SAX_DATA_STORAGE_BUCKET=${your_data_storage_bucket}
SSH into your TPU VM in a terminal to launch the SAX admin server:
docker run \ --name ${SAX_ADMIN_SERVER_DOCKER_NAME} \ -it \ -d \ --rm \ --network host \ --env GSBUCKET=${SAX_ADMIN_STORAGE_BUCKET} \ ${SAX_ADMIN_SERVER_IMAGE_URL}
You can check the docker log by:
docker logs -f ${SAX_ADMIN_SERVER_DOCKER_NAME}
The output in the log will look similar to the following:
I0829 01:22:31.184198 7 config.go:111] Creating config fs_root: ""gs://test_sax_admin/sax-fs-root"" I0829 01:22:31.347883 7 config.go:115] Created config fs_root: ""gs://test_sax_admin/sax-fs-root"" I0829 01:22:31.360837 24 admin_server.go:44] Starting the server I0829 01:22:31.361420 24 ipaddr.go:39] Skipping non-global IP address 127.0.0.1/8. I0829 01:22:31.361455 24 ipaddr.go:39] Skipping non-global IP address ::1/128. I0829 01:22:31.361462 24 ipaddr.go:39] Skipping non-global IP address fe80::4001:aff:fe8e:fc8/64. I0829 01:22:31.361469 24 ipaddr.go:39] Skipping non-global IP address fe80::42:bfff:fef9:1bd3/64. I0829 01:22:31.361474 24 ipaddr.go:39] Skipping non-global IP address fe80::20fb:c3ff:fe5b:baac/64. I0829 01:22:31.361482 24 ipaddr.go:56] IPNet address 10.142.15.200 I0829 01:22:31.361488 24 ipaddr.go:56] IPNet address 172.17.0.1 I0829 01:22:31.456952 24 admin.go:305] Loaded config: fs_root: ""gs://test_sax_admin/sax-fs-root"" I0829 01:22:31.609323 24 addr.go:105] SetAddr /gcs/test_sax_admin/sax-root/sax/test/location.proto ""10.142.15.200:10000"" I0829 01:22:31.656021 24 admin.go:325] Updated config: fs_root: ""gs://test_sax_admin/sax-fs-root"" I0829 01:22:31.773245 24 mgr.go:781] Loaded manager state I0829 01:22:31.773260 24 mgr.go:784] Refreshing manager state every 10s I0829 01:22:31.773285 24 admin.go:350] Starting the server on port 10000 I0829 01:22:31.773292 24 cloud.go:506] Starting the HTTP server on port 8080
-
Launch a single-host SAX model server into the SAX cluster:
At this point, the SAX cluster contains only the SAX admin server. You can connect to your TPU VM over SSH in a second terminal to launch a SAX model server in your SAX cluster:
docker run \ --privileged \ -it \ -d \ --rm \ --network host \ --name ${SAX_MODEL_SERVER_DOCKER_NAME} \ --env SAX_ROOT=gs://${SAX_ADMIN_STORAGE_BUCKET}/sax-root \ ${SAX_MODEL_SERVER_IMAGE_URL} \ --sax_cell=${SAX_CELL} \ --port=10001 \ --platform_chip=tpuv4 \ --platform_topology=1x1
Convert model checkpoint:
You need to install PyTorch and Transformers to download the GPT-J checkpoint from EleutherAI:
pip3 install accelerate pip3 install torch pip3 install transformers
To convert the checkpoint to SAX checkpoint, you need to install
paxml:
pip3 install paxml==1.1.0
The following
[script](https://github.com/google/saxml/blob/main/saxml/tools/convert_gptj_ckpt.py)converts the GPT-J checkpoint to SAX checkpoint:
python3 -m convert_gptj_ckpt --base EleutherAI/gpt-j-6b --pax pax_6b
After the conversion is done:
ls checkpoint_00000000/
You need to create a commit_success file and placed in the sub directories:
gsutil -m cp -r checkpoint_00000000 ${CHECKPOINT_PATH} touch commit_success.txt gsutil cp commit_success.txt ${CHECKPOINT_PATH}/ gsutil cp commit_success.txt ${CHECKPOINT_PATH}/metadata/ gsutil cp commit_success.txt ${CHECKPOINT_PATH}/state/
Publish the model to SAX cluster
You can now publish GPT-J with the checkpoint converted in the
[previous step](#convert-checkpoint).
MODEL_NAME=gptjtokenizedbf16bs32 MODEL_CONFIG_PATH=saxml.server.pax.lm.params.gptj.GPTJ4TokenizedBF16BS32 REPLICA=1
To publish the GPT-J (and steps afterward), use SSH to connect to your TPU VM in a third terminal:
docker run \ ${SAX_UTIL_IMAGE_URL} \ --sax_root=gs://${SAX_ADMIN_STORAGE_BUCKET}/sax-root \ publish \ ${SAX_CELL}/${MODEL_NAME} \ ${MODEL_CONFIG_PATH} \ ${CHECKPOINT_PATH} \ ${REPLICA}
You will see a lot of activity from the model server Docker log until you see something like the following to indicate the model has loaded successfully:
I0829 01:33:49.287459 139865140229696 servable_model.py:697] loading completed.
Generate inference results
For GPT-J, input and output must be formatted as a comma separated token ID string. You will need to tokenize the text input.
TEXT = ""Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction\:\nSummarize the following news article\:\n\n### Input\:\nMarch 10, 2015 . We're truly international in scope on Tuesday. We're visiting Italy, Russia, the United Arab Emirates, and the Himalayan Mountains. Find out who's attempting to circumnavigate the globe in a plane powered partially by the sun, and explore the mysterious appearance of craters in northern Asia. You'll also get a view of Mount Everest that was previously reserved for climbers. On this page you will find today's show Transcript and a place for you to request to be on the CNN Student News Roll Call. TRANSCRIPT . Click here to access the transcript of today's CNN Student News program. Please note that there may be a delay between the time when the video is available and when the transcript is published. CNN Student News is created by a team of journalists who consider the Common Core State Standards, national standards in different subject areas, and state standards when producing the show. ROLL CALL . For a chance to be mentioned on the next CNN Student News, comment on the bottom of this page with your school name, mascot, city and state. We will be selecting schools from the comments of the previous show. You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call! Thank you for using CNN Student News!\n\n### Response\:
You can obtain the token IDs string through the EleutherAI/gpt-j-6b tokenizer:
from transformers import GPT2Tokenizer tokenizer = GPT2Tokenizer.from_pretrained(""EleutherAI/gpt-j-6b"") :
Tokenize the input text:
encoded_example = tokenizer(TEXT) input_ids = encoded_example.input_ids INPUT_STR = "","".join([str(input_id) for input_id in input_ids])
You can expect a token ID string similar to the following:
>>> INPUT_STR '21106,318,281,12064,326,8477,257,4876,11,20312,351,281,5128,326,3769,2252,4732,13,19430,257,2882,326,20431,32543,262,2581,13,198,198,21017,46486,25,198,13065,3876,1096,262,1708,1705,2708,25,198,198,21017,23412,25,198,16192,838,11,1853,764,775,821,4988,3230,287,8354,319,3431,13,775,821,10013,8031,11,3284,11,262,1578,4498,24880,11,290,262,42438,22931,21124,13,9938,503,508,338,9361,284,2498,4182,615,10055,262,13342,287,257,6614,13232,12387,416,262,4252,11,290,7301,262,11428,5585,286,1067,8605,287,7840,7229,13,921,1183,635,651,257,1570,286,5628,41336,326,373,4271,10395,329,39311,13,1550,428,2443,345,481,1064,1909,338,905,42978,290,257,1295,329,345,284,2581,284,307,319,262,8100,13613,3000,8299,4889,13,48213,6173,46023,764,6914,994,284,1895,262,14687,286,1909,338,8100,13613,3000,1430,13,4222,3465,326,612,743,307,257,5711,1022,262,640,618,262,2008,318,1695,290,618,262,14687,318,3199,13,8100,13613,3000,318,2727,416,257,1074,286,9046,508,2074,262,8070,7231,1812,20130,11,2260,5423,287,1180,2426,3006,11,290,1181,5423,618,9194,262,905,13,15107,3069,42815,764,1114,257,2863,284,307,4750,319,262,1306,8100,13613,3000,11,2912,319,262,4220,286,428,2443,351,534,1524,1438,11,37358,11,1748,290,1181,13,775,481,307,17246,4266,422,262,3651,286,262,2180,905,13,921,1276,307,257,4701,393,257,3710,2479,1511,393,4697,284,2581,257,3068,319,262,8100,13613,3000,8299,4889,0,6952,345,329,1262,8100,13613,3000,0,198,198,21017,18261,25'
To generate a summary for your article:
docker run \ ${SAX_UTIL_IMAGE_URL} \ --sax_root=gs://${SAX_ADMIN_STORAGE_BUCKET}/sax-root \ lm.generate \ ${SAX_CELL}/${MODEL_NAME} \ ${INPUT_STR}
You can expect something similar to:
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------+ | GENERATE | SCORE | +----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------+ | 1212,2443,3407,262,905,42978,764,198,11041,262,42978,284,1037,2444,351,3555,35915,290,25818,764,198,2953,262,4220,286,262,2443,11,2912,329,257,2863,284,307,4750,319,8100,13613,3000,13,220,921,1276,307,257,4701,393,257,3710,2479,1511,393,4697,284,2581,257,3068,319,262,8100,13613,3000,8299,4889,13,50256 | -0.023136413 | | 1212,2443,3407,262,905,42978,764,198,11041,262,42978,284,1037,2444,351,3555,35915,290,25818,764,198,2953,262,4220,286,262,2443,11,2912,329,257,2863,284,307,4750,319,8100,13613,3000,13,220,921,1276,307,257,4701,393,257,3710,2479,1511,393,4697,284,2581,257,3068,319,262,8100,13613,3000,8299,4889,0,50256 | -0.91842502 | | 1212,2443,3407,262,905,42978,764,198,11041,262,42978,284,1037,2444,351,3555,35915,290,25818,764,198,2953,262,4220,286,262,2443,11,2912,329,257,2863,284,307,4750,319,8100,13613,3000,13,921,1276,307,257,4701,393,257,3710,2479,1511,393,4697,284,2581,257,3068,319,262,8100,13613,3000,8299,4889,13,50256 | -1.1726116 | | 1212,2443,3407,262,905,42978,764,198,11041,262,42978,284,1037,2444,351,3555,35915,290,25818,764,198,2953,262,4220,286,262,2443,11,2912,329,257,2863,284,307,4750,319,8100,13613,3000,13,220,921,1276,307,1511,393,4697,284,2581,257,3068,319,262,8100,13613,3000,8299,4889,13,50256 | -1.2472695 | +----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------+
To detokenize the output token IDs string:
output_token_ids = [int(token_id) for token_id in OUTPUT_STR.split(',')] OUTPUT_TEXT = tokenizer.decode(output_token_ids, skip_special_tokens=True)
You can expect the detokenized text as:
>>> OUTPUT_TEXT 'This page includes the show Transcript.\nUse the Transcript to help students with reading comprehension and vocabulary.\nAt the bottom of the page, comment for a chance to be mentioned on CNN Student News. You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call.'
[Clean up](#cleanup)your Docker containers and Cloud Storage storage buckets.
175B multi-host model serving preview
Some of the large language models will require a multi-host TPU slice, that is, v5litepod-16 and above. In those cases, all multi-host TPU hosts will need to have a copy of a SAX model server, and all model servers function as a SAX model server group to serve the large model on a multi-host TPU slice.
Create a new SAX cluster
You can follow the same step of Create a SAX cluster in the
[GPT-J walk through](#single-host-example)to create a new SAX cluster and a SAX admin server.
Or, if you already have an existing SAX cluster, you can launch a multi-host model server into your SAX cluster.
Launch a multi-host SAX model server into a SAX cluster
Use the same command to create a multi-host TPU slice as you use for a single-host TPU slice, just specify the appropriate multi-host accelerator type:
ACCELERATOR_TYPE=v5litepod-32 ZONE=us-east1-c gcloud alpha compute tpus queued-resources create ${QUEUED_RESOURCE_ID} \ --node-id ${TPU_NAME} \ --project ${PROJECT_ID} \ --zone ${ZONE} \ --accelerator-type ${ACCELERATOR_TYPE} \ --runtime-version ${RUNTIME_VERSION} \ --service-account ${SERVICE_ACCOUNT} \ --reserved
To pull the SAX model server image to all TPU hosts/workers and launch them:
gcloud compute tpus tpu-vm ssh ${TPU_NAME} \ --project ${PROJECT_ID} \ --zone ${ZONE} \ --worker=all \ --command="" gcloud auth configure-docker \ us-docker.pkg.dev # Pull sax model server image docker pull ${SAX_MODEL_SERVER_IMAGE_URL} # Run model server docker run \ --privileged \ -it \ -d \ --rm \ --network host \ --name ${SAX_MODEL_SERVER_DOCKER_NAME} \ --env SAX_ROOT=gs://${SAX_ADMIN_STORAGE_BUCKET}/sax-root \ ${SAX_MODEL_SERVER_IMAGE_URL} \ --sax_cell=${SAX_CELL} \ --port=10001 \ --platform_chip=tpuv4 \ --platform_topology=1x1""
Publish the model to SAX cluster
This example uses a
[LmCloudSpmd175B32Test](https://github.com/google/saxml/blob/main/saxml/server/pax/lm/params/lm_cloud.py)model:
MODEL_NAME=lmcloudspmd175b32test MODEL_CONFIG_PATH=saxml.server.pax.lm.params.lm_cloud.LmCloudSpmd175B32Test CHECKPOINT_PATH=None REPLICA=1
To publish the test model:
docker run \ ${SAX_UTIL_IMAGE_URL} \ --sax_root=gs://${SAX_ADMIN_STORAGE_BUCKET}/sax-root \ publish \ ${SAX_CELL}/${MODEL_NAME} \ ${MODEL_CONFIG_PATH} \ ${CHECKPOINT_PATH} \ ${REPLICA}
Generate inference results
docker run \ ${SAX_UTIL_IMAGE_URL} \ --sax_root=gs://${SAX_ADMIN_STORAGE_BUCKET}/sax-root \ lm.generate \ ${SAX_CELL}/${MODEL_NAME} \ ""Q: Who is Harry Porter's mother? A\: ""
Note that since this example uses a test model with random weights, the output may not be meaningful.
Clean Up
Stop the docker containers:
docker stop ${SAX_ADMIN_SERVER_DOCKER_NAME} docker stop ${SAX_MODEL_SERVER_DOCKER_NAME}
Delete your Cloud Storage admin storage bucket and any data storage bucket using
gsutilas shown below.
gsutil rm -rf gs://${SAX_ADMIN_STORAGE_BUCKET} gsutil rm -rf gs://${SAX_DATA_STORAGE_BUCKET}",SAX on Cloud TPU v5e | Google Cloud,
id,url,body,title,description
143,https://cloud.google.com/tpu/docs/reference/rest/v2alpha1/projects.locations.operations/get,"Method: projects.locations.operations.get
Stay organized with collections
Save and categorize content based on your preferences.
Gets the latest state of a long-running operation. Clients can use this method to poll the operation result at intervals as recommended by the API service.
HTTP request
GET https://tpu.googleapis.com/v2alpha1/{name=projects/*/locations/*/operations/*}
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
string
The name of the operation resource.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[Operation](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse#Operation)
Authorization scopes
Requires one of the following OAuth scopes:
-
https://www.googleapis.com/auth/cloud.tpu
-
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Method: projects.locations.operations.get | Cloud TPU | Google Cloud,
id,url,body,title,description
20,https://cloud.google.com/tpu/docs/reference/rest/v2/projects.locations.nodes/start,"Method: projects.locations.nodes.start
Stay organized with collections
Save and categorize content based on your preferences.
HTTP request
POST https://tpu.googleapis.com/v2/{name=projects/*/locations/*/nodes/*}:start
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
string
Required. The resource name.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[Operation](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse#Operation)
Authorization scopes
Requires one of the following OAuth scopes:
-
https://www.googleapis.com/auth/cloud.tpu
-
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Method: projects.locations.nodes.start | Cloud TPU | Google Cloud,
id,url,body,title,description
108,https://cloud.google.com/tpu/docs/run-in-container,"Run Cloud TPU applications in a Docker container
[Docker containers](https://www.docker.com/resources/what-container/) make
configuring applications easier by combining your code and all needed
dependencies in one distributable package. You can run Docker containers within
TPU VMs to simplify configuring and sharing your Cloud TPU applications. This
document describes how to set up a Docker container for each ML framework
supported by Cloud TPU.
Train a TensorFlow model in a Docker container
TPU device
Create a file named
Dockerfilein your current directory and paste the following text
FROM python:3.8 RUN pip install https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/tensorflow/tf-2.12.0/tensorflow-2.12.0-cp38-cp38-linux_x86_64.whl RUN curl -L https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/libtpu/1.6.0/libtpu.so -o /lib/libtpu.so RUN git clone https://github.com/tensorflow/models.git WORKDIR ./models RUN pip install -r official/requirements.txt ENV PYTHONPATH=/models
Create Cloud Storage bucket
gsutil mb -c standard -l europe-west4 gs://your-bucket-name
Create a TPU VM
gcloud compute tpus tpu-vm create your-tpu-name \ --zone=europe-west4-a \ --accelerator-type=v2-8 \ --version=tpu-vm-tf-2.15.0-pjrt
Copy the Dockerfile to your TPU VM
gcloud compute tpus tpu-vm scp ./Dockerfile your-tpu-name:
SSH into the TPU VM
gcloud compute tpus tpu-vm ssh your-tpu-name \ --zone=europe-west4-a
Build the Docker image
sudo docker build -t your-image-name .
Start the Docker container
sudo docker run -ti --rm --net=host --name your-container-name --privileged your-image-name bash
Set environment variables
export STORAGE_BUCKET=gs://your-bucket-name export DATA_DIR=gs://cloud-tpu-test-datasets/fake_imagenet export MODEL_DIR=${STORAGE_BUCKET}/resnet-2x
Train ResNet
python3 official/vision/train.py \ --tpu=local \ --experiment=resnet_imagenet \ --mode=train_and_eval \ --config_file=official/vision/configs/experiments/image_classification/imagenet_resnet50_tpu.yaml \ --model_dir=${MODEL_DIR} \ --params_override=""task.train_data.input_path=${DATA_DIR}/train*, task.validation_data.input_path=${DATA_DIR}/validation*,trainer.train_steps=100""
When the training script completes, make sure you clean up the resources.
- Type
exitto exit from the Docker container
- Type
exitto exit from the TPU VM
- Delete the TPU VM
$ gcloud compute tpus tpu-vm delete your-tpu-name --zone=europe-west4-a
TPU Pod
Create a file named
Dockerfilein your current directory and paste the following text
FROM python:3.8 RUN pip install https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/tensorflow/tf-2.12.0/tensorflow-2.12.0-cp38-cp38-linux_x86_64.whl RUN curl -L https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/libtpu/1.6.0/libtpu.so -o /lib/libtpu.so RUN git clone https://github.com/tensorflow/models.git WORKDIR ./models RUN pip install -r official/requirements.txt ENV PYTHONPATH=/models
Create a TPU VM
gcloud compute tpus tpu-vm create your-tpu-name \ --zone=europe-west4-a \ --accelerator-type=v3-32 \ --version=tpu-vm-tf-2.15.0-pod-pjrt
Copy the Dockerfile to your TPU VM
gcloud compute tpus tpu-vm scp ./Dockerfile your-tpu-name:
SSH into the TPU VM
gcloud compute tpus tpu-vm ssh your-tpu-name \ --zone=europe-west4-a
Build the Docker image
sudo docker build -t your-image-name .
Start a Docker container
sudo docker run -ti --rm --net=host --name your-container-name --privileged your-image-name bash
Train ResNet
python3 official/vision/train.py \ --tpu=local \ --experiment=resnet_imagenet \ --mode=train_and_eval \ --config_file=official/vision/configs/experiments/image_classification/imagenet_resnet50_tpu.yaml \ --model_dir=${MODEL_DIR} \ --params_override=""task.train_data.input_path=${DATA_DIR}/train*, task.validation_data.input_path=${DATA_DIR}/validation*,task.train_data.global_batch_size=2048,task.validation_data.global_batch_size=2048,trainer.train_steps=100""
When the training script completes, make sure you clean up the resources.
- Type
exitto exit from the Docker container
- Type
exitto exit from the TPU VM
- Delete the TPU VM
$ gcloud compute tpus tpu-vm delete your-tpu-name --zone=europe-west4-a
Train a PyTorch model in a Docker container
TPU device
Create Cloud TPU VM
gcloud compute tpus tpu-vm create your-tpu-name \ --zone=europe-west4-a \ --accelerator-type=v2-8 \ --version=tpu-ubuntu2204-base
SSH into TPU VM
gcloud compute tpus tpu-vm ssh your-tpu-name \ --zone=europe-west4-a
Start a container in the TPU VM using the nightly PyTorch/XLA image.
sudo docker run -ti --rm --name your-container-name --privileged gcr.io/tpu-pytorch/xla:r2.0_3.8_tpuvm bash
Configure TPU runtime
There are two PyTorch/XLA runtime options: PJRT and XRT. We recommend you use PJRT unless you have a reason to use XRT. To learn more about the different runtime configurations, see you have a reason to use XRT. To learn more about the different runtime configurations, see
[the PJRT runtime documentation](https://github.com/pytorch/xla/blob/master/docs/pjrt.md).
PJRT
export PJRT_DEVICE=TPU
XRT
export XRT_TPU_CONFIG=""localservice;0;localhost:51011""
Clone the PyTorch XLA repo
git clone --recursive https://github.com/pytorch/xla.git
Train ResNet50
python3 xla/test/test_train_mp_imagenet.py --fake_data --model=resnet50 --num_epochs=1
When the training script completes, make sure you clean up the resources.
- Type
exitto exit from the Docker container
- Type
exitto exit from the TPU VM
- Delete the TPU VM
$ gcloud compute tpus tpu-vm delete your-tpu-name --zone=europe-west4-a
TPU Pod
When you run PyTorch code on a TPU Pod, you must run your code on all TPU
workers at the same time. One way to do this is to use the
gcloud compute tpus tpu-vm ssh command with the
--worker=all and
--command flags. The following procedure shows you how create a Docker
image to make setting up each TPU worker easier.
Create a TPU VM
gcloud compute tpus tpu-vm create your-tpu-name \ --zone=us-central2-b \ --accelerator-type=v4-32 \ --version=tpu-ubuntu2204-base
Add the current user to the docker group
gcloud compute tpus tpu-vm ssh your-tpu-name \ --zone=us-central2-b \ --worker=all \ --command=""sudo usermod -a -G docker $USER""
Run the training script in a container on all TPU workers.
gcloud compute tpus tpu-vm ssh your-tpu-name --worker=all \ --zone=us-central2-b \ --command=""docker run --rm --privileged --net=host -e PJRT_DEVICE=TPU gcr.io/tpu-pytorch/xla:r2.0_3.8_tpuvm python /pytorch/xla/test/test_train_mp_imagenet.py --fake_data --model=resnet50 --num_epochs=1""
Docker command flags:
--rmremove the container after its process terminates.
--privilegedexposes the TPU device to the container.
--net=hostbinds all of the container's ports to the TPU VM to allow communication between the hosts in the Pod.
-eset environment variables.
-
When the training script completes, make sure you clean up the resources.
Delete the TPU VM using the following command:
$ gcloud compute tpus tpu-vm delete your-tpu-name \
--zone=us-central2-b
Train a JAX model in a Docker container
TPU Device
Create the TPU VM
gcloud compute tpus tpu-vm create your-tpu-name \ --zone=europe-west4-a \ --accelerator-type=v2-8 \ --version=tpu-ubuntu2204-base
SSH into TPU VM
gcloud compute tpus tpu-vm ssh your-tpu-name --zone=europe-west4-a
Start Docker daemon in TPU VM
sudo systemctl start docker
Start Docker container
sudo docker run -ti --rm --name your-container-name --privileged --network=host python:3.8 bash
Install JAX
pip install jax[tpu] -f https://storage.googleapis.com/jax-releases/libtpu_releases.html
Install FLAX
pip install --upgrade clu git clone https://github.com/google/flax.git pip install --user -e flax
Run the FLAX MNIST training script
cd flax/examples/mnist python3 main.py --workdir=/tmp/mnist \ --config=configs/default.py \ --config.learning_rate=0.05 \ --config.num_epochs=5
When the training script completes, make sure you clean up the resources.
- Type
exitto exit from the Docker container
- Type
exitto exit from the TPU VM
Delete the TPU VM
$ gcloud compute tpus tpu-vm delete your-tpu-name --zone=europe-west4-a
TPU Pod
When you run JAX code on a TPU Pod, you must run your JAX code on all TPU
workers at the same time. One way to do this is to use the
gcloud compute tpus tpu-vm ssh
command with the
--worker=all and
--command flags. The following
procedure shows you how create a Docker image to make setting up each TPU
worker easier.
Create a file named
Dockerfilein your current directory and paste the following text
FROM python:3.8 RUN pip install ""jax[tpu]"" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html RUN pip install --upgrade clu RUN git clone https://github.com/google/flax.git RUN pip install --user -e flax WORKDIR ./flax/examples/mnist
Build the Docker image
docker build -t your-image-name .
Add a tag to your Docker image before pushing it to the Artifact Registry. For more information on working with Artifact Registry, see
[Work with container images](/artifact-registry/docs/docker).
docker tag your-image-name europe-west-docker.pkg.dev/your-project/your-repo/your-image-name:your-tag
Push your Docker image to the Artifact Registry
docker push europe-west4-docker.pkg.dev/your-project/your-repo/your-image-name:your-tag
Create a TPU VM
gcloud compute tpus tpu-vm create your-tpu-name \ --zone=europe-west4-a \ --accelerator-type==v2-8 \ --version=tpu-ubuntu2204-base
Pull the Docker image from the Artifact Registry on all TPU workers.
gcloud compute tpus tpu-vm ssh your-tpu-name --worker=all \ --zone=europe-west4-a \ --command=""sudo usermod -a -G docker ${USER}""
gcloud compute tpus tpu-vm ssh your-tpu-name --worker=all \ --zone=europe-west4-a \ --command=""gcloud auth configure-docker europe-west4-docker.pkg.dev --quiet""
gcloud compute tpus tpu-vm ssh your-tpu-name --worker=all \ --zone=europe-west4-a \ --command=""docker pull europe-west4-docker.pkg.dev/your-project/your-repo/your-image-name:your-tag""
Run the container on all TPU workers.
gcloud compute tpus tpu-vm ssh your-tpu-name --worker=all \ zone=europe-west4-a \ --command=""docker run -ti -d --privileged --net=host --name your-container-name europe-west4-docker.pkg.dev/your-project/your-repo/your-image:your-tag bash""
Run the training script on all TPU workers:
gcloud compute tpus tpu-vm ssh your-tpu-name --worker=all \ --zone=europe-west4-a \ --command=""docker exec --privileged your-container-name python3 main.py --workdir=/tmp/mnist \ --config=configs/default.py \ --config.learning_rate=0.05 \ --config.num_epochs=5""
When the training script completes, make sure you clean up the resources.
Shut down the container on all workers:
gcloud compute tpus tpu-vm ssh your-tpu-name --worker=all \ --zone=europe-west4-a \ --command=""docker kill your-container-name""
Delete the TPU VM using the following command:
$ gcloud compute tpus tpu-vm delete your-tpu-name \ --zone=europe-west4-a
What's next
[Cloud TPU Tutorials](/tpus/docs/tutorials) [Manage TPUs](/tpu/docs/managing-tpus-tpu-vm) [Cloud TPU System Architecture](/tpu/docs/system-architecture-tpu-vm) [Run TensorFlow code on TPU Pod slices](/tpu/docs/tensorflow-pods) [Run JAX code on TPU Pod slices](/tpu/docs/jax-pods)",Run Cloud TPU applications in a Docker container | Google Cloud,
id,url,body,title,description
176,https://cloud.google.com/tpu/docs/reference/rest/v1/projects.locations.acceleratorTypes/list,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Query parameters](#body.QUERY_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [Try it!](#try-it)
Lists accelerator types supported by this API.
HTTP request
GET https://tpu.googleapis.com/v1/{parent=projects/*/locations/*}/acceleratorTypes
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
parent
|
Required. The parent resource name.
Query parameters
|Parameters
|
pageSize
|
The maximum number of items to return.
|
pageToken
|
The nextPageToken value returned from a previous List request, if any.
|
filter
|
List filter.
|
orderBy
|
Sort results.
Request body
The request body must be empty.
Response body
Response for
.
[acceleratorTypes.list](/tpu/docs/reference/rest/v1/projects.locations.acceleratorTypes/list#google.cloud.tpu.v1.Tpu.ListAcceleratorTypes)
If successful, the response body contains data with the following structure:
|JSON representation
|
{
""acceleratorTypes"": [
{
object (
|Fields
|
acceleratorTypes[]
|
The listed nodes.
|
nextPageToken
|
The next page token or empty if none.
|
unreachable[]
|
Locations that could not be reached.
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).",Method: projects.locations.acceleratorTypes.list | Cloud TPU | Google Cloud,
id,url,body,title,description
171,https://cloud.google.com/tpu/docs/reference/rest/Shared.Types/GetLocationRequest,"GetLocationRequest
Stay organized with collections
Save and categorize content based on your preferences.
|JSON representation
|
{
""name"": string
}
|Fields
|
name
|
string
Resource name for the location.
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2022-11-28 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",GetLocationRequest | Cloud TPU | Google Cloud,
id,url,body,title,description
157,https://cloud.google.com/tpu/docs/reference/rest/v2/projects.locations.operations/cancel,"Starts asynchronous cancellation on a long-running operation. The server makes a best effort to cancel the operation, but success is not guaranteed. If the server doesn't support this method, it returns
google.rpc.Code.UNIMPLEMENTED. Clients can use
or other methods to check whether the cancellation succeeded or whether the operation completed despite cancellation. On successful cancellation, the operation is not deleted; instead, it becomes an operation with an
[Operations.GetOperation](/tpu/docs/reference/rest/v1alpha1/projects.locations.operations/get#google.longrunning.Operations.GetOperation)
value with a
[Operation.error](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse#Operation.FIELDS.error)
of 1, corresponding to
[google.rpc.Status.code](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse#Status.FIELDS.code)
Code.CANCELLED.
HTTP request
POST https://tpu.googleapis.com/v2/{name=projects/*/locations/*/operations/*}:cancel
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
The name of the operation resource to be cancelled.
Request body
The request body must be empty.
Response body
If successful, the response body is empty.
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).",Method: projects.locations.operations.cancel | Cloud TPU | Google Cloud,
id,url,body,title,description
118,https://cloud.google.com/tpu/docs/profile-tpu-vm,"Profile your model on Cloud TPU VMs
Profiling enables you to optimize your model's training performance on Cloud TPUs.
You use
[TensorBoard](https://www.tensorflow.org/tensorboard) and the
[Cloud TPU TensorBoard plug-in](#install-plugin) to profile your model.
For more information about using TensorBoard with one of the supported frameworks, see the following documents:
Prerequisites to profiling a training script
Before you use the TPU profiling tools, you need to:
Start a model training session
[Set up a v4-8 TPU](/tpu/docs/v4-users-guide#train_resnet_on_a_tpu_pod_slice)to train a model. The profiling procedure described in this document uses a ResNet model, but you can use another model provided it trains on a v4 TPU.
In your TPU VM, add a line to start the profiler server to the training script.
For the ResNET training, the training script is at:
/usr/share/tpu/tensorflow/resnet50_keras/resnet50.py.
Insert the highlighted lines into resnet50.py. At the top of the file, add the following import:
import tensorflow.compat.v2 as tf2
Right before the scripts starts the training loop, add the highlighted line:
if name == 'main': tf.logging.set_verbosity(tf.logging.INFO) tf2.profiler.experimental.server.start(6000) app.run(main)
The TensorFlow profiler server starts on your TPU VM when you run the script.
Start the model training.
Run your training script and wait until you see output indicating your model is actively training. The output depends on your code and model. Look for output like
Epoch 1/100. Alternatively, you can navigate to the Cloud TPU page in the
[Google Cloud console](https://console.cloud.google.com/compute/tpus), select your TPU, and view the CPU utilization graph. While the CPU utilization graph does not show TPU utilization, it's a good indication that the TPU is training your model.
-
Start profiling the model training
When the model is training, open a separate terminal window or Cloud Shell. Use the following steps to begin profiling the model training.
In the new window or shell, connect to your TPU VM with port forwarding.
gcloud compute tpus tpu-vm ssh your-vm --zone=us-central2-b --ssh-flag=""-4 -L 9001:localhost:9001""
Port forwarding allows your local browser to communicate with the TensorBoard server running on your TPU VM.
Install TensorFlow requirements {: id=""install-tensorboard""}.
Your TPU VM has TensorBoard installed by default. You can also
[install TensorFlow manually](https://www.tensorflow.org/install). Either way, some additional dependencies may be required. Install these dependencies on your TPU VM by running:
pip3 install -r /usr/share/tpu/models/official/requirements.txt
Install the Cloud TPU TensorBoard Plugin {: id=""install-plugin""}.
From the TPU VM, run the following commands:
pip3 install --upgrade ""cloud-tpu-profiler>=2.3.0"" pip3 install tensorflow pip3 install tensorboard_plugin_profile
Start the TensorBoard server
Run TensorBoard and create a log directory (
logdir) on the TPU VM where TensorBoard can write profiling data. Specify the log directory using the
--logdirflag. For example:
mkdir log-directory TPU_LOAD_LIBRARY=0 tensorboard --logdir log-directory --port 9001
TensorBoard starts a web server and displays its URL:
Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all TensorBoard 2.3.0 at http://localhost:9001 (Press CTRL+C to quit)
Open a web browser and go to the URL displayed in the TensorBoard output. Select Profile from the drop-down menu in the upper right of the TensorBoard page. The list of available profiling tools is shown in the tools pulldown menu on the left sidebar.
Capture a profile on TPU VMs
- Select the CAPTURE PROFILE button.
- Select the IP address radio button.
- Type HOSTNAME:6000 in the
Profile Service URLfield.
- Select the CAPTURE button.
View profile data with TensorBoard
After you capture a profile, TensorBoard displays the overview_page. The list of profiling tools you can use is displayed in the left pane.
Profile
The Profile tab is displayed after you have captured some model data. You may need to click the refresh button on the TensorBoard page. Once data is available, clicking the Profile tab presents a selection of tools to help with performance analysis. You can use any of the following tools to profile your model.
[Overview page](#overview_page) [Input pipeline analyzer](#input_pipeline_analyzer) [XLA Op profile](#op_profile) [Trace viewer](#trace_viewer)(Chrome browser only) [Memory viewer](#memory_viewer)
Profile overview page
The overview page (overview_page), available in the Profile page, provides a top level view of how your model performed during a capture run. The page shows you an aggregated overview for all your TPUs and an overall input pipeline analysis. There is an option for selecting individual TPUs in the Host drop-down.
The page displays data in the following panels:
Performance summary
- FLOPS Utilization - The percentage utilization of the TPU matrix units
Top ten TensorFlow operations on TPU Displays the TensorFlow operations that consumed the most time:
Each row displays the self-time of an operation (as the percentage of time taken by all operations), cumulative time, category, name, and the FLOPS rate achieved.
Run environment
- The number of hosts used
- The type of TPU used
- The number of TPU cores
Input pipeline analyzer
The input pipeline analyzer provides insights into your performance results. The tool tells you immediately whether your program is input bound and can walk you through device and host-side analysis to debug whatever stage of the pipeline is creating bottlenecks.
See the guidance on
[input pipeline performance](https://www.tensorflow.org/versions/master/performance/datasets_performance)
for deeper insight into optimizing pipeline performance.
Input pipeline
When a TensorFlow program reads data from a file, the read process is divided into multiple data processing stages connected in series. The output of one stage is the input to the next one. This system of reading is called the input pipeline.
A typical pipeline for reading records from files has the following stages:
- File reading
- File preprocessing (optional)
- File transfer from the host machine to the device
An inefficient input pipeline can severely slow down your application. An
application is considered input bound when it spends a significant portion
of time in its input pipeline. Use the
[Input pipeline analyzer](#input_pipeline_analyzer) to understand
where the input pipeline is inefficient.
Input pipeline dashboard
To open the input pipeline analyzer, select Profile, then select input_pipeline_analyzer from the Tools drop-down.
The dashboard shows device-side and host-side analysis details.
Device-side analysis - Shows details on device step times.
- Device step time statistics
- % of device step time waiting for input data
Host-side analysis
This section shows the details of host-side analysis broken into several categories:
- Enqueuing data to be transferred to device Time spent putting data into an infeed queue before transferring the data to the device.
- Data preprocessing Time spent on preprocessing operations, such as image decompression.
- Reading data from files in advance Time spent reading files, including caching, prefetching, and interleaving.
- Reading data from files on demand Time spent on reading data from files without caching, prefetching, and interleaving.
- Other data reading or processing Time spent on other input related operations
not using
tf.data.
To see the statistics for individual input operations and their categories
broken down by execution time, expand the
Show Input Op statistics section.
A source data table like the following appears:
Each table entry contains the following information:
- Input Op Shows the TensorFlow operation name of the input operation.
- Count Shows the total number of instances of the operation executed during the profiling period.
- Total Time (in ms) Shows the cumulative sum of time spent on each of the operation instances.
- Total Time % Shows the total time spent on an operation as a fraction of the total time spent in input processing.
- Total Self-time (in ms) Shows the accumulated time
over all instances of the function. The self-time measures the time spent
inside the function body, excluding the time spent in any functions it calls.
For example, the
Iterator::PaddedBatch::Filter::ForeverRepeat::Mapis called by
Iterator::PaddedBatch::Filter, therefore its total self-time is excluded from the total self-time of the latter.
- Total self-time % Shows the total self-time as a fraction of the total time spent on input processing.
- Category Shows the processing category of the input operation.
Op profile
Op profile is a Cloud TPU tool that displays the performance
statistics of
[XLA](https://www.tensorflow.org/performance/xla/) operations
executed during a profiling period. The operation profile shows:
- How well your application uses the Cloud TPU as a percentage of time spent on operations by category and of TPU FLOPS utilization.
- The most time-consuming operations. Those operations are potential targets for optimization.
- Details of individual operations, including shape, padding and expressions that use the operation.
You can use op profile to find targets for optimization. For example, you can use operation profile to identify which XLA operations are taking the longest time to run and how many TPU FLOPS they consume.
Using op profile
The Op Profile tool contains performance statistics of XLA operations. You can view Op Profile data in TensorBoard by clicking on the Profile tab at the top of the screen and then selecting op_profile from the Tools drop-down. You will see a display like this:
- Overview section Shows Cloud TPU utilization and provides suggestions for optimization.
- Control panel Contains controls that let you set the number of operations displayed in the table, which operations are displayed, and how they are sorted.
[Op table](#op_table)Lists the top TensorFlow operation categories associated with the XLA ops. These operations are sorted by percentage of Cloud TPU usage. [Op details cards](#op_details_cards)Displays details about the operations that appear when you point to an operation in the table. These details include the FLOPS utilization, the expression in which the operation is used, and the operation layout (fit).
XLA Op table
The operation table lists XLA operation categories in order from the highest to lowest percentage of Cloud TPU usage. The table shows the percentage of time taken, the operation category name, the associated TensorFlow op name, and the percentage of FLOPS utilization for the category. To display (or hide) the ten most time-consuming XLA operations for a category, click the triangle next to the category name in the table.
- Time Shows the total percentage of time spent by all the operations in that category. You can click to expand the entry and see the breakdown of time spent by each individual operation.
- Top ten Ops The toggle next to a category name displays/hides the top ten time-consuming operations within the category. If a fusion operation entry is displayed in the operations list, you can expand it to see the non-fusion, element wise operations it contains.
- TensorFlow Op Shows the TensorFlow operation name associated with the XLA operation.
- FLOPS Shows the FLOPS utilization, which is the measured number of FLOPS expressed as a percentage of the Cloud TPU peak FLOPS. The higher the FLOPS utilization percentage, the faster operations run. The table cell is color coded: green for high FLOPS utilization (good) and red for low FLOPS utilization (bad).
Op details cards
When you select a table entry, a card appears displaying details about the XLA operation or the operation category. A typical card looks like this:
- Name and Category Shows the highlighted XLA operation name and category.
- FLOPS utilization Displays FLOPS utilization as a percentage of total FLOPS possible.
- Expression Shows the
[XLA expression](https://www.tensorflow.org/performance/xla/operation_semantics)containing the operation.
- Memory Utilization Displays the percentage of peak memory usage by your program.
- Layout (Convolution operations only) Shows the
[shape and layout](https://www.tensorflow.org/performance/xla/shapes)of a tensor, including a description of any padding performed by the XLA compiler.
Interpreting results
For convolution operations, low TPU FLOPS utilization may be due to one or both of the following reasons:
- padding (matrix units are partially used)
- convolution operation is memory bound
This section gives an interpretation of some performance metrics from a model with low FLOP utilization. In this example, output fusion and convolution dominated the execution time. There were many vector or scalar operations that had low FLOP utilization.
One optimization strategy for this type of profile is to transform the vector or scalar operations to convolution operations.
In the following example, %convolution.399 shows lower FLOPS and memory utilization than %convolution.340 in the previous example.
In this example, the batch size is being padded to 128 and feature size is being padded to 8. In this case, only 5% of the matrix units are being used effectively. Utilization is calculated by (((batch_time * num_of_features) / padding_size ) / num_of_cores). Compare the FLOPS in this example to the %convolution.340 in the previous example which uses no padding.
Trace viewer
Trace viewer is a Cloud TPU performance analysis tool available
on the Profile page. The tool uses the
[Chrome trace event profiling viewer](https://github.com/catapult-project/catapult/tree/master/tracing)
so it only works in the Chrome browser.
Trace viewer displays a timeline that shows:
- Durations for the operations that were executed by your TensorFlow model.
- Which part of the system (TPU or host machine) executed an operation. Typically, the host machine executes infeed operations, which preprocess training data and transfers it to the TPU, whereas the TPU executes the actual model training.
Trace viewer lets you identify performance problems in your model, then take steps to resolve them. For example, at a high level, you can identify whether infeed or model training is taking most of the time. Drilling down, you can identify which TensorFlow operations are taking the longest to execute.
Trace viewer is limited to 1M events for each Cloud TPU. If
you need to assess more events, use the
[streaming trace viewer](#stream_tr_viewer)
instead.
Trace viewer interface
To open trace viewer, go to TensorBoard, click the Profile tab at the top of the screen, and choose trace_viewer from the Tools drop-down. The viewer appears displaying your most recent run:
This screen contains the following main elements (marked with numbers in the preceding screen shot):
- Runs drop-down Contains all runs for which you've captured trace information. The default view is your most recent run, but you can open the drop-down to select a different run.
- Tools drop-down Selects different profiling tools.
- Host drop-down Selects a host that contains a Cloud TPU set.
- Timeline pane Shows operations that Cloud TPU and the host machine executed over time.
- Details pane Shows additional information for operations selected in the Timeline pane.
Here's a closer look at the timeline pane:
The Timeline pane contains the following elements:
- Top bar Contains various auxiliary controls.
- Time axis Shows time relative to the beginning of the trace.
- Section and track labels Each section contains multiple tracks and has a triangle on the left that you can click to expand and collapse the section. There is one section for every processing element in the system.
- Tool selector Contains various tools for interacting with the trace viewer.
- Events Shows the time during which an operation was executed or the duration of meta-events, such as training steps.
- Vertical tab bar This bar does not have a useful purpose for Cloud TPU. The bar is part of the general purpose trace viewer tool provided by Chrome that is used for a various performance analysis tasks.
Sections and tracks
Trace viewer contains the following sections:
- One section for each TPU node, labeled with the number of the TPU chip
and the TPU node within the chip (for example, ""Chip 2: TPU Core 1""). Each
TPU node section contains the following tracks:
- Step Shows the duration of the training steps that were running on the TPU.
- TensorFlow Ops Shows TensorFlow operations executed on the TPU.
- XLA Ops Shows
[XLA](https://www.tensorflow.org/performance/xla/)operations that ran on the TPU. (Each operation is translated into one or several XLA operations. The XLA compiler translates the XLA operations into code that runs on the TPU.)
- One section for threads running on the host CPU, labeled ""Host Threads"". The section contains one track for each CPU thread. Note: You can ignore the information displayed alongside the section labels.
Timeline tool selector
You can interact with the timeline view using the timeline tool selector in
TensorBoard. You can click a timeline tool or use the following
[keyboard shortcuts](#keyboard_shortcuts) to activate and highlight a tool.
To move the timeline tool selector, click in the dotted area at the top and then
drag the selector to where you want it.
Use the timeline tools as follows:
|Selection tool
click an event to select it or drag to select multiple events. Additional information about the selected event or events (name, start time, and duration) will be displayed in the details pane.
|Pan tool
Drag to pan the timeline view horizontally and vertically.
|Zoom tool
Drag up to zoom in or drag down to zoom out along the horizontal (time) axis. The horizontal position of the mouse cursor determines the center around which the zoom takes place.
Note: If the zoom tool remains active after you release the mouse button, click the timeline view to deactivate the zoom tool.
|Timing tool
Drag horizontally to mark a time interval. The length of the interval appears on the time axis. To adjust the interval, drag its ends. To clear the interval, click anywhere inside the timeline view.
If you select another tool, the interval remains marked.
Memory viewer
Memory viewer lets you visualize the peak memory usage and memory usage trends for your program.
The memory viewer user interface looks like this:
- Host drop-down Selects a TPU host and XLA High Level Optimizer (HLO) modules to visualize.
- Memory overview Displays peak memory allocation and size without padding.
- Working space chart Displays peak memory use and a plot of memory usage trends for your program. Point to a buffer in one of the buffer charts to display additional information in the buffer allocation card.
- Buffer charts Two charts that display buffer allocation at peak memory usage. Point to a buffer in one of the buffer charts to display additional information in the buffer details card.
- Buffer allocation details card Displays allocation details for a buffer.
Memory overview panel
The memory overview (top) panel shows you the module name and the peak memory allocation set when the total buffer allocation size reaches the maximum. The unpadded peak allocation size is also shown for comparison.
Working space chart
This chart displays peak memory use and a plot of memory usage trends for your program. The vertical line indicates peak memory utilization for the program. This charts shows if your program can fit into the available global memory space.
Each point in the graph represents a ""program point"" in the XLA HLO program. The line shows you how memory usage of your program changes over time.
Interaction with buffer chart elements
When you point to a buffer in abuffer charts, a horizontal line showing the lifetime of the buffer appears in the working space chart.
The thickness of the horizontal line indicates the relative magnitude of the buffer size relative to the peak memory allocation. The length of the line indicates the lifetime of the buffer.
Buffer charts
Two charts show the breakdown of memory usage at the peak usage.
By Program Order Displays the buffers from left to right in the order in which they were active during program execution.
By Size Displays the buffers that were active during program execution in order of decreasing size.
Buffer allocation details card
When you point to a buffer displayed in one of the buffer charts, a buffer allocation details card appears. A typical details card looks like this:
- Name - Name of the XLA operation.
- Category - The operation category.
- Size - The size of the buffer allocation (including padding).
- Unpadded size - The size of the buffer allocation without padding.
- Expansion - The relative magnitude of padded buffer size versus the unpadded size.
- Extra memory - Indicates how much extra memory is used for padding.
- Shape - Describes the rank, size, and data type of the N-dimensional array.
- TensorFlow op name - Shows the name of the TensorFlow operation associated with the buffer allocation.
- Allocation type - Indicates buffer allocation category: Parameter, Output, Thread-local, and Temporary (for example, buffer allocation within a fusion).
Out of memory errors
If you run a model and get an ""out of memory"" error, use the guidelines in this document to capture a profile. Wait until your script is training your model before starting the profiler. The profiling output can help you understand what caused the error.",Profile your model on Cloud TPU VMs | Google Cloud,
id,url,body,title,description
149,https://cloud.google.com/tpu/docs/reference/rest/v1alpha1/projects.locations/list,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Query parameters](#body.QUERY_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [Try it!](#try-it)
Lists information about the supported locations for this service.
HTTP request
GET https://tpu.googleapis.com/v1alpha1/{name=projects/*}/locations
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
The resource that owns the locations collection, if applicable.
Query parameters
|Parameters
|
filter
|
A filter to narrow down results to a preferred subset. The filtering language accepts strings like
|
pageSize
|
The maximum number of results to return. If not set, the service selects a default.
|
pageToken
|
A page token received from the
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[ListLocationsResponse](/tpu/docs/reference/rest/Shared.Types/ListLocationsResponse)
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).",Method: projects.locations.list | Cloud TPU | Google Cloud,
id,url,body,title,description
195,https://cloud.google.com/tpu/docs/reference/rest/v2alpha1/projects.locations,"Resource: Location
A resource that represents a Google Cloud location.
|JSON representation
|
{ ""name"": string, ""locationId"": string, ""displayName"": string, ""labels"": { string: string, ... }, ""metadata"": { ""@type"": string, field1: ..., ... } }
|Fields
|
name
|
Resource name for the location, which may vary between implementations. For example:
|
locationId
|
The canonical id for this location. For example:
|
displayName
|
The friendly name for this location, typically a nearby city name. For example, ""Tokyo"".
|
labels
|
Cross-service attributes for the location. For example
An object containing a list of
|
metadata
|
Service-specific metadata. For example the available capacity at the given location.
An object containing fields of an arbitrary type. An additional field
|
Methods
|
|Generates the Cloud TPU service identity for the project.
|
|Gets information about a location.
|
|Lists information about the supported locations for this service.",REST Resource: projects.locations | Cloud TPU | Google Cloud,
id,url,body,title,description
132,https://cloud.google.com/tpu/docs/v5e-inference-converter,"Cloud TPU v5e Inference Converter introduction
Introduction
Cloud TPU Inference Converter prepares and optimizes a TensorFlow 2 (TF2) model
for TPU inference. The converter runs in a local or TPU VM shell.
The TPU VM shell is recommended because it comes preinstalled with the command
line tools needed for the converter. It takes an exported
[SavedModel](https://www.tensorflow.org/guide/saved_model)
and performs the following steps:
- TPU Conversion: It adds
TPUPartitionedCalland other TPU ops to the model to make it servable on the TPU. By default, a model exported for inference doesn't have such ops and cannot be served on the TPU, even if it was trained on the TPU.
- Batching: It adds batching ops to the model to enable in-graph batching for better throughput.
- BFloat16 Conversion: It converts the data format of the model from
float32to
bfloat16for better computational performance and lower High Bandwidth Memory (HBM) usage on the TPU.
- IO Shape Optimization: It optimizes the tensor shapes for data transferred between the CPU and TPU to improve bandwidth utilization.
When exporting a model, users create function aliases for any functions they would like to run on the TPU. They pass these functions to the Converter and the Converter places them on the TPU and optimizes them.
The Cloud TPU Inference Converter is available as a Docker image which can be executed in any environment with Docker installed.
Estimated time to complete the steps shown above: ~20 min - 30 min
Prerequisites
- The model must be a TF2 model and exported in the
[SavedModel](https://www.tensorflow.org/guide/saved_model)format.
- The model must have a function alias for the TPU function. See the
[code example](#function-alias)for how to do this. The following examples uses
tpu_funcas the TPU function alias.
- Make sure your machine's CPU supports Advanced Vector eXtensions (AVX)
instructions, as the Tensorflow library (the dependency of the Cloud TPU
Inference Converter) is compiled to use AVX instructions.
[Most CPUs](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions#CPUs_with_AVX)have the AVX support.
- You can run
lscpu | grep avxto check whether the AVX instruction set is supported.
- You can run
Before you begin
Before you begin setup, do the following:
[Create a new project](https://console.cloud.google.com/projectselector2/home/dashboard): In the Google Cloud console, on the project selector page, select or create a Cloud project. [Set up a TPU VM](/tpu/docs/managing-tpus-tpu-vm#creating_a_cloud_tpu): Create a new TPU VM using Google Cloud console or
gcloud, or use an existing TPU VM to run inference with the converted model on the TPU VM.
- Make sure the TPU VM image is TensorFlow based. For example,
--version=tpu-vm-tf-2.11.0.
- The converted model will be loaded and served on this TPU VM.
- Make sure the TPU VM image is TensorFlow based. For example,
Ensure you have the command line tools you need to use Cloud TPU Inference Converter. You can install the Google Cloud SDK and Docker locally or use a TPU VM which has this software installed by default. You use these tools to interact with the Converter image.
Connect to the instance with SSH using the following command:
gcloud compute tpus tpu-vm ssh ${tpu-name} --zone ${zone} --project ${project-id}
Environment Setup
Set up your environment from your TPU VM shell or from your local shell.
TPU VM Shell
In your TPU VM shell, run the following commands to allow non-root docker usage:
sudo usermod -a -G docker ${USER} newgrp docker
Initialize your Docker Credential helpers:
gcloud auth configure-docker \ us-docker.pkg.dev
Local Shell
In your local shell, set up the environment using the following steps:
Install the
[Cloud SDK](https://cloud.google.com/sdk/install), which includes the
gcloudcommand-line tool.
Install
[Docker](https://docs.docker.com/engine/install/):
Allow non-root Docker usage:
sudo usermod -a -G docker ${USER} newgrp docker
Login in to your environment:
gcloud auth login
Initialize your Docker Credential helpers:
gcloud auth configure-docker \ us-docker.pkg.dev
Pull the Inference Converter Docker image:
CONVERTER_IMAGE=us-docker.pkg.dev/cloud-tpu-images/inference/tpu-inference-converter-cli:2.13.0 docker pull ${CONVERTER_IMAGE}
Converter Image
The Image is for doing one-time model conversions. Set the model paths
and adjust the
[converter options](#converter-options)
to fit your needs. The
[Usage Examples](#usage-examples)
section provides several common use cases.
docker run \ --mount type=bind,source=${MODEL_PATH},target=/tmp/input,readonly \ --mount type=bind,source=${CONVERTED_MODEL_PATH},target=/tmp/output \ ${CONVERTER_IMAGE} \ --input_model_dir=/tmp/input \ --output_model_dir=/tmp/output \ --converter_options_string=' tpu_functions { function_alias: ""tpu_func"" } batch_options { num_batch_threads: 2 max_batch_size: 8 batch_timeout_micros: 5000 allowed_batch_sizes: 2 allowed_batch_sizes: 4 allowed_batch_sizes: 8 max_enqueued_batches: 10 } '
Inference with the converted model in TPU VM
# Initialize the TPU resolver = tf.distribute.cluster_resolver.TPUClusterResolver(""local"") tf.config.experimental_connect_to_cluster(resolver) tf.tpu.experimental.initialize_tpu_system(resolver) # Load the model model = tf.saved_model.load(${CONVERTED_MODEL_PATH}) # Find the signature function for serving serving_signature = 'serving_default' # Change the serving signature if needed serving_fn = model.signatures[serving_signature] # Run the inference using requests. results = serving_fn(**inputs) logging.info(""Serving results: %s"", str(results))
Usage Examples
Add a function alias for the TPU function
- Find or create a function in your model that wraps everything you want
to run on the TPU. If
@tf.functiondoesn't exist, add it.
- When saving the model, provide SaveOptions like below to give
model.tpu_funcan alias
func_on_tpu.
- You can pass this function alias to the converter.
class ToyModel(tf.keras.Model): @tf.function( input_signature=[tf.TensorSpec(shape=[None, 10], dtype=tf.float32)]) def tpu_func(self, x): return x * 1.0 model = ToyModel() save_options = tf.saved_model.SaveOptions(function_aliases={ 'func_on_tpu': model.tpu_func, }) tf.saved_model.save(model, model_dir, options=save_options)
Convert a model with multiple TPU functions
You can put multiple functions on the TPU. Simply create multiple function
aliases and pass them in
converter_options_string to the converter.
tpu_functions { function_alias: ""tpu_func_1"" } tpu_functions { function_alias: ""tpu_func_2"" }
Quantization
Quantization is a technique that reduces the precision of the numbers used to represent a model's parameters. This results in a smaller model size and faster computation. A quantized model provides gains in inference throughput as well as smaller memory usage and storage size, at the cost of small accuracy drops.
The new Post-Training Quantization feature in TensorFlow that targets
TPU, is developed from the similar existing feature in TensorFlow Lite
that is used to target mobile and edge devices. To learn more about
quantization in general, you can take a look at
[TensorFlow Lite's document](https://www.tensorflow.org/lite/performance/model_optimization#quantization).
Quantization concepts
This section defines concepts specifically related to quantization with the Inference Converter.
Concepts related to other TPU configurations (for example,
slices, hosts, chips, and TensorCores) are described in the
[TPU System Architecture](/tpu/docs/system-architecture-tpu-vm) page. [Post-training quantization (PTQ)](https://www.tensorflow.org/model_optimization/guide/quantization/post_training): PTQ is a technique that reduces the size and computational complexity of a neural network model without significantly affecting its accuracy. PTQ works by converting the floating-point weights and activations of a trained model to lower-precision integers, such as 8-bit or 16-bit integers. This can cause a significant reduction in model size and inference latency, while only incurring a small loss in accuracy.
Calibration: The calibration step for quantization is the process of collecting statistics on the range of values that the weights and activations of a neural network model take. This information is used to determine the quantization parameters for the model, which are the values that will be used to convert the floating-point weights and activations to integers.
Representative Dataset: A representative dataset for quantization is a small dataset that represents the actual input data for the model. It is used during the calibration step of quantization to collect statistics on the range of values that the weights and activations of the model will take. The representative dataset should satisfy the following properties:
- It should properly represent the actual inputs to the model during inference. This means that it should cover the range of values that the model is likely to see in the real world.
- It should collectively flow through each branch of conditionals
(such as
tf.cond), if there are any. This is important because the quantization process needs to be able to handle all possible inputs to the model, even if they are not explicitly represented in the representative dataset.
- It should be large enough to collect enough statistics and reduce error. As a rule of thumb, it is recommended to use more than 200 representative samples.
The representative dataset can be a subset of the training dataset, or it can be a separate dataset that is specifically designed to be representative of the real-world inputs to the model. The choice of which dataset to use depends on the specific application.
Static Range Quantization (SRQ): SRQ determines the range of values for the weights and activations of a neural network model once, during the calibration step. This means that the same range of values is used for all inputs to the model. This can be less accurate than dynamic range quantization, especially for models with a wide range of input values. However, static range quantization requires less computation at run time than dynamic range quantization.
Dynamic Range Quantization (DRQ): DRQ determines the range of values for the weights and activations of a neural network model for each input. This allows the model to adapt to the range of values of the input data, which can improve accuracy. However, dynamic range quantization requires more computation at run time than static range quantization.
Feature Static range quantization Dynamic range quantization Range of values Determined once, during calibration Determined for each input Accuracy Can be less accurate, especially for models with a wide range of input values Can be more accurate, especially for models with a wide range of input values Complexity Simpler More complex Computation at run time Less computation More computation
Weight-only Quantization: Weight-only quantization is a type of quantization that only quantizes the weights of a neural network model, while leaving the activations in floating point. This can be a good option for models that are sensitive to accuracy, as it can help to preserve the accuracy of the model.
How to use
[quantization](https://www.tensorflow.org/model_optimization/guide/quantization/post_training)
Quantization can be applied by configuring and setting
[QuantizationOptions](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/quantization/tensorflow/quantization_options.proto) to the converter options. Notable options are:
- tags: Collection of tags identifying the
[within the](https://www.tensorflow.org/guide/saved_model#details_of_the_savedmodel_command_line_interface)
MetaGraphDef
[to quantize. No need to specify if you have only one](https://www.tensorflow.org/guide/saved_model#running_a_savedmodel_in_tensorflow_serving)
SavedModel
MetaGraphDef.
- signature_keys: Sequence of keys identifying
containing inputs and outputs. If not specified, [""serving_default""] is used.
[SignatureDef](https://www.tensorflow.org/guide/saved_model#details_of_the_savedmodel_command_line_interface)
- quantization_method: Quantization method to apply. If not
specified,
STATIC_RANGEquantization will be applied.
- op_set: Should be kept as XLA. It is currently the default option, no need to specify.
- representative_datasets: Specify the dataset used for calibrating the quantization parameters.
Building the representative dataset
A representative dataset is essentially an iterable of samples.
Where a sample is a map of:
{input_key: input_value}. For example:
representative_dataset = [{""x"": tf.random.uniform(shape=(3, 3))}
for _ in range(256)]
The representative datasets should be saved as
TFRecord
files using the
TfRecordRepresentativeDatasetSaver class
currently available in the tf-nightly pip package. For example:
# Assumed tf-nightly installed.
import tensorflow as tf
representative_dataset = [{""x"": tf.random.uniform(shape=(3, 3))}
for _ in range(256)]
tf.quantization.experimental.TfRecordRepresentativeDatasetSaver(
path_map={'serving_default': '/tmp/representative_dataset_path'}
).save({'serving_default': representative_dataset})
Examples
The following example quantizes the model with the signature key
of
serving_default and function alias of
tpu_func:
docker run \ --mount type=bind,source=${MODEL_PATH},target=/tmp/input,readonly \ --mount type=bind,source=${CONVERTED_MODEL_PATH},target=/tmp/output \ ${CONVERTER_IMAGE} \ --input_model_dir=/tmp/input \ --output_model_dir=/tmp/output \ --converter_options_string=' \ tpu_functions { \ function_alias: ""tpu_func"" \ } \ external_feature_configs { \ quantization_options { \ signature_keys: ""serving_default"" \ representative_datasets: { \ key: ""serving_default"" \ value: { \ tfrecord_file_path: ""${TF_RECORD_FILE}"" \ } \ } \ } \ } '
Add batching
The Converter can be used to add batching to a model. For a description of the
batching options that can be tuned, see
[Definition of batching options](#batching-options).
By default, the Converter will batch any TPU functions in the model. It can also
batch user-provided
[signatures](#signature-batching)
and
[functions](#function-batching)
which can further improve performance. Any TPU function, user-provided function
or signature that is batched, must meet the batching op's
[strict shape requirements](#strict-shape-requirements).
The Converter can also
[update](#update-batching-options)
existing batching options.
The following is an example of how to add batching to a model. For more
information on batching, see
[Batching deep dive](#batching-deep-dive).
batch_options { num_batch_threads: 2 max_batch_size: 8 batch_timeout_micros: 5000 allowed_batch_sizes: 2 allowed_batch_sizes: 4 allowed_batch_sizes: 8 max_enqueued_batches: 10 }
Disable bfloat16 and IO shape optimizations
BFloat16 and IO Shape Optimizations are enabled by default. If they don't work well with your model, they can be disabled.
# Disable both optimizations disable_default_optimizations: true # Or disable them individually io_shape_optimization: DISABLED bfloat16_optimization: DISABLED
Conversion Report
You can find this conversion report from the log after running the Inference Converter. Below is an example.
-------- Conversion Report -------- TPU cost of the model: 96.67% (2034/2104) CPU cost of the model: 3.33% (70/2104) Cost breakdown ================================ % Cost Name -------------------------------- 3.33 70 [CPU cost] 48.34 1017 tpu_func_1 48.34 1017 tpu_func_2 --------------------------------
This report estimates the computational cost of the output model on CPU and TPU, and further breaks down the TPU cost to each function, which should reflect your selection of the TPU functions in the converter options.
If you want to better utilize the TPU, you may want to experiment with the model structure and adjust the converter options.
FAQs
Which function(s) should I place on the TPU?
It is best to put as much of your model on the TPU as possible, because the vast majority of ops execute faster on the TPU.
If your model does not contain any TPU-incompatible op, strings or sparse tensors, putting the entire model on the TPU is usually the best strategy. And you can do it by finding or creating a function that wraps the entire model, creating a function alias for it, and passing that to the Converter.
If your model contains parts that cannot work on the TPU (e.g.,TPU-incompatible ops, strings or sparse tensors), the choice of TPU functions depends on where the incompatible part is.
- If it's at the beginning or the end of the model, you can refactor the
model to keep it on the CPU. Examples are string pre- and post-processing
stages. For more information about moving code to the CPU, see,
[""How do I move a part of the model to CPU?""](#move-model-to-cpu)It shows a typical way to refactor the model.
- If it's in the middle of the model it's better to split the model into three parts and contain all the TPU-incompatible ops in the middle part, and make it run on the CPU.
- If it is a sparse tensor, consider calling
tf.sparse.to_denseon the CPU and passing the resulting dense tensor to the TPU portion of the model.
Another factor to consider is the HBM usage. Embedding tables can use a lot of HBM. If they grow beyond the hardware limitation of the TPU, they have to be put on the CPU, along with the lookup ops.
Whenever possible, only one TPU function should exist under one signature. If the structure of your model requires calling multiple TPU functions per incoming inference request, you should be aware of the added latency of sending tensors between CPU and TPU.
A good way to evaluate the selection of TPU functions is to check the
[Conversion Report](#conversion-report).
It shows the percentage of computation that was placed on the TPU, and a
breakdown of the cost of each TPU function.
How do I move a part of the model to CPU?
If your model contains parts that cannot be served on the TPU, you need to refactor the model to move them to the CPU. Here is a toy example. The model is a language model with a preprocessing stage. The code for layer definitions and functions are omitted for simplicity.
class LanguageModel(tf.keras.Model): @tf.function def model_func(self, input_string): word_ids = self.preprocess(input_string) return self.bert_layer(word_ids)
This model cannot be directly served on the TPU for two reasons. First, the
parameter is a string. Second, the
preprocess function may contain many string
ops. Both are not TPU-compatible.
To refactor this model, you can create another function called
tpu_func to
host the computational-intensive
bert_layer. Then create a function alias for
tpu_func and pass it to the Converter. In this way, everything inside
tpu_func will run on the TPU, and everything left in
model_func will run on
the CPU.
class LanguageModel(tf.keras.Model): @tf.function def tpu_func(self, word_ids): return self.bert_layer(word_ids) @tf.function def model_func(self, input_string): word_ids = self.preprocess(input_string) return self.tpu_func(word_ids)
What should I do if the model has TPU-incompatible ops, strings or sparse tensors?
Most of the standard TensorFlow ops are supported on the TPU, but a few including sparse tensors and strings are not supported. The Converter doesn't check for TPU-incompatible ops. So a model containing such ops can pass the conversion. But when running it for inference, errors like below will occur.
'tf.StringToNumber' op isn't compilable for TPU device.
If your model has TPU-incompatible ops, they should be put outside the TPU function. Moreover, string is an unsupported data format on the TPU. So string-typed variables shouldn't be placed in the TPU function. And the parameters and return values of the TPU function shouldn't be string-typed as well. Similarly, avoid placing sparse tensors in the TPU function including in its parameters and return values.
It's usually not hard to refactor out the incompatible part of the model and
move it to the CPU. Here is an
[example](#move-model-to-cpu).
How to support custom ops in the model?
If custom ops are used in your model, the Converter may not recognize them and fail to convert the model. This is because the op library of the custom op, which contains the complete definition of the op, isn't linked to the Converter.
As currently the converter code is not open-sourced yet, the converter cannot be built with custom op.
What should I do if I have a TensorFlow 1 model?
The Converter does not support TensorFlow 1 models. TensorFlow 1 models should be migrated to TensorFlow 2.
Do I need to enable the MLIR bridge when running my model?
Most converted models can be run with either the newer TF2XLA MLIR bridge or the original TF2XLA bridge.
How do I convert a model that has already been exported without a function alias?
If a model was exported without a function alias, the easiest way is to export
it again and
[create a function alias](#function-alias).
If reexporting is not an option, it is still possible to convert the model by
providing a
concrete_function_name. However, identifying the correct
concrete_function_name does require some detective work.
Function aliases are a mapping from a user defined string to a concrete function name. They make it easier to refer to a specific function in the model. The Converter accepts both function aliases and raw concrete function names.
Concrete function names can be found by examining the
saved_model.pb.
The following example shows how to put a concrete function called
__inference_serve_24 on the TPU.
sudo docker run \ --mount type=bind,source=${MODEL_PATH},target=/tmp/input,readonly \ --mount type=bind,source=${CONVERTED_MODEL_PATH},target=/tmp/output \ ${CONVERTER_IMAGE} \ --input_model_dir=/tmp/input \ --output_model_dir=/tmp/output \ --converter_options_string=' tpu_functions { concrete_function_name: ""__inference_serve_24"" }'
How do I resolve a compile time constant constraint error?
For both training and inference, XLA requires the inputs to certain ops have a known shape at TPU compile time. This means that when XLA compiles the TPU portion of the program, the inputs to these ops must have a statically known shape.
There are two ways to resolve this issue.
- The best option is to update the op's inputs to have a statically known
shape by the time XLA compiles the TPU program. This compilation happens
right before the TPU portion of the model is run. This means that the shape
should be statically known by the time the
TpuFunctionis about to run.
- Another option is to modify the
TpuFunctionto no longer include the problematic op.
Why am I getting a batching shape error?
Batching has
[strict shape requirements](#strict-shape-requirements)
that allow incoming requests to be batched along their 0th dimension (aka the
batching dimension). These shape requirements come from the TensorFlow batching
op and cannot be relaxed.
Failure to meet these requirements will result in errors like:
- Batching input tensors must have at least one dimension.
- Dimensions of inputs should match.
- Batching input tensors supplied in a given op invocation must have equal 0th-dimension size.
- Batched output tensor's 0th dimension does not equal the sum of the 0th dimension sizes of the input tensors.
To meet these requirements, consider providing a different
[function](#function-batching)
or
[signature](#signature-batching)
to batch. It may also be necessary to modify existing functions to meet these
requirements.
If a function
is being batched, make sure its
@tf.function's input_signature's shapes all
have None in the 0th dimension. If a signature
is being batched, make sure that all its inputs have -1 in the 0th dimension.
For a complete explanation on why these errors are happening and how to resolve
them, see
[Batching Deep Dive](#batching-deep-dive).
Known Issues
TPU function cannot indirectly call another TPU function
While the Converter can handle most function calling scenarios across the CPU-TPU boundary, there is one rare edge case it would fail. It is when a TPU function indirectly calls another TPU function.
This is because the Converter modifies the direct caller of a TPU function from calling the TPU function itself to calling a TPU call stub. The call stub contains ops that can only work on the CPU. When a TPU function calls any function that eventually calls the direct caller, those CPU ops could be brought on the TPU to execute, which will generate missing kernel errors. Note this case is different from a TPU function directly calling another TPU function. In this case, the Converter doesn't modify either function to call the call stub, so it can work.
In the Converter, we have implemented the detection of this scenario. If you see the following error, that means your model has hit this edge case:
Unable to place both ""__inference_tpu_func_2_46"" and ""__inference_tpu_func_4_68"" on the TPU because ""__inference_tpu_func_2_46"" indirectly calls ""__inference_tpu_func_4_68"". This behavior is unsupported because it can cause invalid graphs to be generated.
The general solution is to refactor the model to avoid such a function calling scenario. If you find that difficult to do, contact the Google support team to discuss more.
Reference
Converter Options in Protobuf format
message ConverterOptions { // TPU conversion options. repeated TpuFunction tpu_functions = 1; // The state of an optimization. enum State { // When state is set to default, the optimization will perform its // default behavior. For some optimizations this is disabled and for others // it is enabled. To check a specific optimization, read the optimization's // description. DEFAULT = 0; // Enabled. ENABLED = 1; // Disabled. DISABLED = 2; } // Batch options to apply to the TPU Subgraph. // // At the moment, only one batch option is supported. This field will be // expanded to support batching on a per function and/or per signature basis. // // // If not specified, no batching will be done. repeated BatchOptions batch_options = 100; // Global flag to disable all optimizations that are enabled by default. // When enabled, all optimizations that run by default are disabled. If a // default optimization is explicitly enabled, this flag will have no affect // on that optimization. // // This flag defaults to false. bool disable_default_optimizations = 202; // If enabled, apply an optimization that reshapes the tensors going into // and out of the TPU. This reshape operation improves performance by reducing // the transfer time to and from the TPU. // // This optimization is incompatible with input_shape_opt which is disabled. // by default. If input_shape_opt is enabled, this option should be // disabled. // // This optimization defaults to enabled. State io_shape_optimization = 200; // If enabled, apply an optimization that updates float variables and float // ops on the TPU to bfloat16. This optimization improves performance and // throughtput by reducing HBM usage and taking advantage of TPU support for // bfloat16. // // This optimization may cause a loss of accuracy for some models. If an // unacceptable loss of accuracy is detected, disable this optimization. // // This optimization defaults to enabled. State bfloat16_optimization = 201; BFloat16OptimizationOptions bfloat16_optimization_options = 203; // The settings for XLA sharding. If set, XLA sharding is enabled. XlaShardingOptions xla_sharding_options = 204; } message TpuFunction { // The function(s) that should be placed on the TPU. Only provide a given // function once. Duplicates will result in errors. For example, if // you provide a specific function using function_alias don't also provide the // same function via concrete_function_name or jit_compile_functions. oneof name { // The name of the function alias associated with the function that // should be placed on the TPU. Function aliases are created during model // export using the tf.saved_model.SaveOptions. // // This is a recommended way to specify which function should be placed // on the TPU. string function_alias = 1; // The name of the concrete function that should be placed on the TPU. This // is the name of the function as it found in the GraphDef and the // FunctionDefLibrary. // // This is NOT the recommended way to specify which function should be // placed on the TPU because concrete function names change every time a // model is exported. string concrete_function_name = 3; // The name of the signature to be placed on the TPU. The user must make // sure there is no TPU-incompatible op under the entire signature. string signature_name = 5; // When jit_compile_functions is set to True, all jit compiled functions // are placed on the TPU. // // To use this option, decorate the relevant function(s) with // @tf.function(jit_compile=True), before exporting. Then set this flag to // True. The converter will find all functions that were tagged with // jit_compile=True and place them on the TPU. // // When using this option, all other settings for the TpuFunction // will apply to all functions tagged with // jit_compile=True. // // This option will place all jit_compile=True functions on the TPU. // If only some jit_compile=True functions should be placed on the TPU, // use function_alias or concrete_function_name. bool jit_compile_functions = 4; } } message BatchOptions { // Number of scheduling threads for processing batches of work. Determines // the number of batches processed in parallel. This should be roughly in line // with the number of TPU cores available. int32 num_batch_threads = 1; // The maximum allowed batch size. int32 max_batch_size = 2; // Maximum number of microseconds to wait before outputting an incomplete // batch. int32 batch_timeout_micros = 3; // Optional list of allowed batch sizes. If left empty, // does nothing. Otherwise, supplies a list of batch sizes, causing the op // to pad batches up to one of those sizes. The entries must increase // monotonically, and the final entry must equal max_batch_size. repeated int32 allowed_batch_sizes = 4; // Maximum number of batches enqueued for processing before requests are // failed fast. int32 max_enqueued_batches = 5; // If set, disables large batch splitting which is an efficiency improvement // on batching to reduce padding inefficiency. bool disable_large_batch_splitting = 6; // Experimental features of batching. Everything inside is subject to change. message Experimental { // The component to be batched. // 1. Unset if it's for all TPU subgraphs. // 2. Set function_alias or concrete_function_name if it's for a function. // 3. Set signature_name if it's for a signature. oneof batch_component { // The function alias associated with the function. Function alias is // created during model export using the tf.saved_model.SaveOptions, and is // the recommended way to specify functions. string function_alias = 1; // The concreate name of the function. This is the name of the function as // it found in the GraphDef and the FunctionDefLibrary. This is NOT the // recommended way to specify functions, because concrete function names // change every time a model is exported. string concrete_function_name = 2; // The name of the signature. string signature_name = 3; } } Experimental experimental = 7; } message BFloat16OptimizationOptions { // Indicates where the BFloat16 optimization should be applied. enum Scope { // The scope currently defaults to TPU. DEFAULT = 0; // Apply the bfloat16 optimization to TPU computation. TPU = 1; // Apply the bfloat16 optimization to the entire model including CPU // computations. ALL = 2; } // This field indicates where the bfloat16 optimization should be applied. // // The scope defaults to TPU. Scope scope = 1; // If set, the normal safety checks are skipped. For example, if the model // already contains bfloat16 ops, the bfloat16 optimization will error because // pre-existing bfloat16 ops can cause issues with the optimization. By // setting this flag, the bfloat16 optimization will skip the check. // // This is an advanced feature and not recommended for almost all models. // // This flag is off by default. bool skip_safety_checks = 2; // Ops that should not be converted to bfloat16. // Inputs into these ops will be cast to float32, and outputs from these ops // will be cast back to bfloat16. repeated string filterlist = 3; } message XlaShardingOptions { // num_cores_per_replica for TPUReplicateMetadata. // // This is the number of cores you wish to split your model into using XLA // SPMD. int32 num_cores_per_replica = 1; // (optional) device_assignment for TPUReplicateMetadata. // // This is in a flattened [x, y, z, core] format (for // example, core 1 of the chip // located in 2,3,0 will be stored as [2,3,0,1]). // // If this is not specified, then the device assignments will utilize the same // topology as specified in the topology attribute. repeated int32 device_assignment = 2; // A serialized string of tensorflow.tpu.TopologyProto objects, used for // the topology attribute in TPUReplicateMetadata. // // You must specify the mesh_shape and device_coordinates attributes in // the topology object. // // This option is required for num_cores_per_replica > 1 cases due to // ambiguity of num_cores_per_replica, for example, // pf_1x2x1 with megacore and df_1x1 // both have num_cores_per_replica = 2, but topology is (1,2,1,1) for pf and // (1,1,1,2) for df. // - For pf_1x2x1, mesh shape and device_coordinates looks like: // mesh_shape = [1,2,1,1] // device_coordinates=flatten([0,0,0,0], [0,1,0,0]) // - For df_1x1, mesh shape and device_coordinates looks like: // mesh_shape = [1,1,1,2] // device_coordinates=flatten([0,0,0,0], [0,0,0,1]) // - For df_2x2, mesh shape and device_coordinates looks like: // mesh_shape = [2,2,1,2] // device_coordinates=flatten( // [0,0,0,0],[0,0,0,1],[0,1,0,0],[0,1,0,1] // [1,0,0,0],[1,0,0,1],[1,1,0,0],[1,1,0,1]) bytes topology = 3; }
Batching Deep Dive
Batching is used to improve the throughput and TPU utilization. It allows
multiple requests to be processed at the same time. During training, batching
can be done using
tf.data. During inference, it is typically done by adding an
op in the graph that batches incoming requests. The op waits until it has enough
requests or a timeout is reached before it generates a large batch from the
individual requests. See
[Definition of batching options](#batching-options)
for more information about the different batching options that can be tuned,
including batch sizes and timeouts.
By default, the Converter inserts the batching op directly before the TPU
computation. It wraps the user-provided TPU function(s) and any preexisting TPU
computation in the model with batching op(s). It is possible to override this
default behavior by telling the Converter which
[functions](#function-batching)
and/or
[signatures](#signature-batching)
should be batched.
The following example shows how to add the default batching.
batch_options { num_batch_threads: 2 max_batch_size: 8 batch_timeout_micros: 5000 allowed_batch_sizes: 2 allowed_batch_sizes: 4 allowed_batch_sizes: 8 max_enqueued_batches: 10 }
Signature batching
Signature batching batches the entire model starting at the signature's inputs and going to the signature's outputs. Unlike the Converter's default batching behavior, signature batching batches both the TPU computation and the CPU computation. This gives 10% to 20% performance gain during inference on some models.
Like all batching, Signature batching does have
[strict shape requirements](#strict-shape-requirements).
To help ensure these shape requirements are met, signature inputs should have
shapes that have at least two dimensions. The first dimension is batch size
and should have a size of -1. For example,
(-1, 4),
(-1) or
(-1,
128, 4, 10) are all valid input shapes. If this is not possible, consider using
the default batching behavior or
[function batching](#function-batching).
To use signature batching provide the signature name(s) as
signature_name(s)
using the
BatchOptions.
batch_options { num_batch_threads: 2 max_batch_size: 8 batch_timeout_micros: 5000 allowed_batch_sizes: 2 allowed_batch_sizes: 4 allowed_batch_sizes: 8 max_enqueued_batches: 10 experimental { signature_name: ""serving_default"" } }
Function batching
Function batching can be used to tell the Converter which function(s) should be batched. By default the Converter will batch all TPU functions. Function batching overrides this default behavior.
Function batching can be used to batch CPU computation. Many models see a performance improvement when their CPU computation is batched. The best way to batch CPU computation is using signature batching however it may not work for some models. In those cases, function batching can be used to batch part of the CPU computation in addition to the TPU computation. Note that the batching op cannot run on the TPU so any batching function that is provided must be called on the CPU.
Function batching can also be used to satisfy the
[strict shape requirements](#strict-shape-requirements)
imposed by the batching op. In cases when the TPU function(s) don't meet the
batching op's shape requirements, function batching can be used to tell the
Converter to batch different function(s).
To use this, generate a
function_alias for the function that should be
batched. You can do this by finding or creating a function in your model
that wraps everything you want batched. Make sure this function meets the
[strict shape requirements](#strict-shape-requirements)
imposed by the batching op. Add
@tf.function if it doesn't have one already.
It is important to provide the
input_signature to the
@tf.function. The 0th
dimension should be
None because it is the batch dimension so it cannot be a
fixed size. For example,
[None, 4],
[None] or
[None, 128, 4, 10] are all
valid input shapes. When saving the model, provide
SaveOptions like those shown
below to give
model.batch_func an alias ""
batch_func"". Then you can pass this
function alias to the converter.
class ToyModel(tf.keras.Model): @tf.function(input_signature=[tf.TensorSpec(shape=[None, 10], dtype=tf.float32)]) def batch_func(self, x): return x * 1.0 ... model = ToyModel() save_options = tf.saved_model.SaveOptions(function_aliases={ 'batch_func': model.batch_func, }) tf.saved_model.save(model, model_dir, options=save_options)
Next, pass the
function_alias(s) using the BatchOptions.
batch_options { num_batch_threads: 2 max_batch_size: 8 batch_timeout_micros: 5000 allowed_batch_sizes: 2 allowed_batch_sizes: 4 allowed_batch_sizes: 8 max_enqueued_batches: 10 experimental { function_alias: ""batch_func"" } }
Definition of batching options
num_batch_threads: (integer) Number of scheduling threads for processing batches of work. Determines the number of batches processed in parallel. This should be roughly in line with the number of TPU cores available.
max_batch_size: (integer) Maximum allowed batch size. Can be larger than
allowed_batch_sizesto utilize large batch splitting.
batch_timeout_micros: (integer) Maximum number of microseconds to wait before outputting an incomplete batch.
allowed_batch_sizes: (list of integers) If the list is not empty, it will pad batches up to the nearest size in the list. The list must be monotonically increasing and the final element must be lower than or equal to
max_batch_size.
max_enqueued_batches: (integer) Maximum number of batches enqueued for processing before requests are failed fast.
Updating existing batching options
You can add or update batching options by running the Docker image specifying
batch_options and setting
disable_default_optimizations to true using the
--converter_options_string flag. The batch options will be applied to every
TPU function or pre-existing batching op.
batch_options { num_batch_threads: 2 max_batch_size: 8 batch_timeout_micros: 5000 allowed_batch_sizes: 2 allowed_batch_sizes: 4 allowed_batch_sizes: 8 max_enqueued_batches: 10 } disable_default_optimizations=True
Batching shape requirements
Batches are created by concatenating input tensors across requests along their batch (0th) dimension. The output tensors are split along their 0th dimension. In order to perform these operations, the batching op has strict shape requirements for its inputs and outputs.
Walkthrough
To understand these requirements, it is helpful to first understand how
batching is performed. In the example below, we are batching a simple
tf.matmul op.
def my_func(A, B) return tf.matmul(A, B)
The first inference request produces the inputs A and B with the shapes
(1, 3,
2) and
(1, 2, 4) respectively. The second inference request produces the
inputs A and B with the shapes
(2, 3, 2) and
(2, 2, 4).
The batching timeout is reached. The model supports a batch size of 3 so
inference requests #1 and #2 are batched together without any padding. The
batched tensors are formed by concatenating the requests #1 and #2 along the batch (0th)
dimension. Since #1's A has a shape of
(1, 3, 2) and #2's A has a shape of
(2, 3, 2), when they are concatenated along the batch (0th) dimension, the
resulting shape is
(3, 3, 2).
The
tf.matmul is executed and it produces an output with the shape
(3, 3,
4).
The output of the
tf.matmul is batched so it needs to be split back into
separate requests. The batching op does this by splitting along the batch (0th)
dimension of each output tensor. It decides how to split the 0th dimension based
on the shape of the original inputs. Since request #1's shapes have a 0th
dimension of 1, its output has a 0th dimension of 1 for a shape of
(1, 3, 4).
Since request #2's shapes have a 0th dimension of 2, its output has a 0th
dimension of 2 for a shape of
(2, 3, 4).
Shape Requirements
In order to perform the input concatenating and output splitting described above, the batching op has the following shape requirements:
Inputs to batching cannot be scalars. In order to concatenate along the 0th dimension, the tensors have to have at least two dimensions.
In the walkthrough above. Neither A nor B are scalars.
Failure to meet this requirement will cause an error like:
Batching input tensors must have at least one dimension. A simple fix for this error is to make the scalar a vector.
Across different inference requests (for example, different Session run invocations), input tensors with the same name have the same size for each dimension except the 0th dimension. This allows inputs to be cleanly concatenated along their 0th dimension.
In the walkthrough above, request #1's A has a shape of
(1, 3, 2). This means that any future request must produce a shape with the pattern
(X, 3, 2). Request #2 meets this requirement with
(2, 3, 2). Similarly, request #1's B has a shape of
(1, 2, 4)so all future requests must produce a shape with the pattern
(X, 2, 4).
Failure to meet this requirement will cause an error like:
Dimensions of inputs should match.
For a given inference request, all inputs must have the same 0th dimension size. If different input tensors to the batching op have different 0th dimensions, the batching op does not know how to split the output tensors.
In the walkthrough above, request #1's tensors all have a 0th dimension size of 1. This lets the batching op know that its output should have a 0th dimension size of 1. Similarly request #2's tensors have a 0th dimension size of 2, so its output will have a 0th dimension size of 2. When the batching op splits the final shape of
(3, 3, 4), it produces
(1, 3, 4)for request #1 and
(2, 3, 4)for request #2.
Failure to meet this requirement will result in errors like:
Batching input tensors supplied in a given op invocation must have equal 0th-dimension size.
The 0th dimension size of each output tensor's shape must be the sum of all the input tensors' 0th dimension size (plus any padding introduced by the batching op to meet the next largest
allowed_batch_size). This allows the batching op to split the output tensors along their 0th dimension based on the 0th dimension of the input tensors.
In the walkthrough above, the input tensors have a 0th dimension of 1 from request #1 and 2 from request #2. Therefore, each output tensor must have a 0th dimension of 3 because 1+2=3. The output tensor
(3, 3, 4)meets this requirement. If 3 had not been a valid batch size but 4 was, the batching op would have had to pad the 0th dimension of the inputs from 3 to 4. In this case, each output tensor would have to have a 0th dimension size of 4.
Failure to meet this requirement will result in an error like:
Batched output tensor's 0th dimension does not equal the sum of the 0th dimension sizes of the input tensors.
Resolving Shape Requirement Errors
To meet these requirements, consider providing a different
[function](#function-batching)
or
[signature](#signature-batching)
to batch. It may also be necessary to modify existing functions to meet these
requirements.
If a
[function](#function-batching)
is being batched, make sure its
@tf.function's input_signature's shapes all
have
None in the 0th dimension (aka the batch dimension). If a
[signature](#signature-batching)
is being batched, make sure that all its inputs have -1 in the 0th dimension.
The BatchFunction op does not support
SparseTensors as inputs or outputs.
Internally, each sparse tensor is represented as three separate tensors that can
have different 0th dimension sizes.",Cloud TPU v5e Inference Converter introduction | Google Cloud,
id,url,body,title,description
144,https://cloud.google.com/tpu/docs/troubleshooting/troubleshooting,"Troubleshooting Cloud TPU errors and performance issues
Stay organized with collections
Save and categorize content based on your preferences.
These troubleshooting documents describe error conditions and performance issues
you might see while training with Cloud TPUs using TensorFlow, JAX, and PyTorch.
If you cannot tell whether the problem you are seeing is specific to a
particular framework, start with Troubleshooting TensorFlow - TPU.
Monitoring with Google Cloud Monitoring
Troubleshooting TensorFlow - TPU
Troubleshooting PyTorch - TPU
Troubleshooting JAX - TPU
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Troubleshooting Cloud TPU errors and performance issues | Google Cloud,
id,url,body,title,description
158,https://cloud.google.com/tpu/docs/tutorials/hf-gpt2,"If you are not familiar with Cloud TPU, we recommend that you go through the
[quickstart](https://cloud.google.com/tpu/docs/quickstart) to learn how to
create a TPU VM.
This tutorial shows you how to train the HuggingFace GPT2 model on Cloud TPU.
Objectives
- Create a Cloud TPU
- Install dependencies
- Run the training job
Costs
In this document, you use the following billable components of Google Cloud:
- Compute Engine
- Cloud TPU
To generate a cost estimate based on your projected usage,
use the
[pricing calculator](/products/calculator).
[free trial](/free-trial).
Before you begin
Before starting this tutorial, check that your Google Cloud project is correctly set up.
-
Sign in to your Google Cloud account. If you're new to
Google Cloud,
[create an account](https://console.cloud.google.com/freetrial)to evaluate how our products perform in real-world scenarios. New customers also get $300 in free credits to run, test, and deploy workloads.
-
In the Google Cloud console, on the project selector page, select or
[create a Google Cloud project](/resource-manager/docs/creating-managing-projects).
-
[Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console).
-
In the Google Cloud console, on the project selector page, select or
[create a Google Cloud project](/resource-manager/docs/creating-managing-projects).
-
[Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console).
This walkthrough uses billable components of Google Cloud. Check the
[Cloud TPU pricing page](/tpu/docs/pricing) to
estimate your costs. Be sure to
[clean up](#clean-up)
resources you create when you've finished with them to avoid unnecessary
charges.
Train HuggingFace GPT2 with Cloud TPUs
Open a Cloud Shell window.
Create an environment variable for your project ID.
export PROJECT_ID=your-project-id
Configure Google Cloud CLI to use the your Google Cloud project where you want to create a Cloud TPU.
gcloud config set project ${PROJECT_ID}
The first time you run this command in a new Cloud Shell VM, an
Authorize Cloud Shellpage is displayed. Click
Authorizeat the bottom of the page to allow
gcloudto make Google Cloud API calls with your credentials.
Create a Service Account for the Cloud TPU project.
Service accounts allow the Cloud TPU service to access other Google Cloud services.
$ gcloud beta services identity create --service tpu.googleapis.com --project $PROJECT_ID
The command returns a Cloud TPU Service Account with following format:
service-PROJECT_NUMBER@cloud-tpu.iam.gserviceaccount.com
Create a Cloud TPU
Create a Cloud TPU VM using the
gcloudcommand. The following command creates a
v4-8TPU. You can also create a TPU Podslice by setting the
--accelerator-typeflag to a Podslice type, for example
v4-32.
$ gcloud compute tpus tpu-vm create hf-gpt2 \ --zone=us-central2-b \ --accelerator-type=v4-8 \ --version=tpu-vm-tf-2.15.0-pjrt
Command flag descriptions
zone
- The
[zone](/tpu/docs/types-zones)where you plan to create your Cloud TPU.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
version
- The Cloud TPU
[software version](/tpu/docs/supported-tpu-versions#tpu_software_versions).
-
Connect to the Cloud TPU VM by running the following
sshcommand.
gcloud compute tpus tpu-vm ssh hf-gpt2 --zone=us-central2-b
Install dependencies
Clone the HuggingFace Transformers repo:
(vm)$ cd /tmp (vm)$ git clone https://github.com/huggingface/transformers.git (vm)$ cd transformers
Install dependencies:
(vm)$ pip install . (vm)$ pip install -r examples/tensorflow/_tests_requirements.txt (vm)$ cd /tmp/transformers/examples/tensorflow/language-modeling (vm)$ pip install -r requirements.txt
Create temp directory:
(vm)$ mkdir /tmp/gpt2-wikitext
- If you set
--version=tpu-vm-tf-2.15.0-pjrtwhen creating your TPU, set the following environment variables to enable the PJRT runtime:
(vm)$ export NEXT_PLUGGABLE_DEVICE_USE_C_API=true (vm)$ export TF_PLUGGABLE_DEVICE_LIBRARY_PATH=/lib/libtpu.so
Run training script
(vm)$ python3 run_clm.py \
--model_name_or_path distilgpt2 \
--max_train_samples 1000 \
--max_eval_samples 100 \
--num_train_epochs 1 \
--output_dir /tmp/gpt2-wikitext \
--dataset_name wikitext \
--dataset_config_name wikitext-103-raw-v1
Command flag descriptions
model_name_or_path
- The name of the model to train.
max_train_samples
- The maximum number of samples to use for training.
max_eval_samples
- The maximum number of samples to use for evaluation.
num_train_epochs
- The number of epochs to train the model.
output_dir
- The output directory for the training script.
dataset_name
- The name of the dataset to use.
dataset_config_name
- The dataset configuration name
When the training is complete, a message similar to the following is displayed:
125/125 [============================] - ETA: 0s - loss: 3.61762023-07-07 21:38:17.902850: W tensorflow/core/framework/dataset.cc:956] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations. 125/125 [============================] - 763s 6s/step - loss: 3.6176 - val_loss: 3.4233 Configuration saved in /tmp/gpt2-wikitext/config.json Configuration saved in /tmp/gpt2-wikitext/generation_config.json Model weights saved in /tmp/gpt2-wikitext/tf_model.h5 D0707 21:38:45.640973681 12027 init.cc:191] grpc_shutdown starts clean-up now
Clean up
Disconnect from the TPU VM instance:
(vm)$ exit
Your prompt should now be
username@projectname, showing you are in the Cloud Shell.
Delete the TPU resource.
$ gcloud compute tpus tpu-vm delete hf-gpt2 \ --zone=us-central2-b
What's next
Try one of the other
[supported reference models](/tpu/docs/tutorials/supported-models).",Training HuggingFace GPT2 on Cloud TPU (TF 2.x) | Google Cloud,
id,url,body,title,description
14,https://cloud.google.com/tpu/docs/reference/rest,"TPU API provides customers with access to Google TPU technology.
[REST Resource: v2alpha1.projects.locations](#v2alpha1.projects.locations) [REST Resource: v2alpha1.projects.locations.acceleratorTypes](#v2alpha1.projects.locations.acceleratorTypes) [REST Resource: v2alpha1.projects.locations.nodes](#v2alpha1.projects.locations.nodes) [REST Resource: v2alpha1.projects.locations.operations](#v2alpha1.projects.locations.operations) [REST Resource: v2alpha1.projects.locations.queuedResources](#v2alpha1.projects.locations.queuedResources) [REST Resource: v2alpha1.projects.locations.reservations](#v2alpha1.projects.locations.reservations) [REST Resource: v2alpha1.projects.locations.runtimeVersions](#v2alpha1.projects.locations.runtimeVersions) [REST Resource: v2.projects.locations](#v2.projects.locations) [REST Resource: v2.projects.locations.acceleratorTypes](#v2.projects.locations.acceleratorTypes) [REST Resource: v2.projects.locations.nodes](#v2.projects.locations.nodes) [REST Resource: v2.projects.locations.operations](#v2.projects.locations.operations) [REST Resource: v2.projects.locations.runtimeVersions](#v2.projects.locations.runtimeVersions) [REST Resource: v1alpha1.projects.locations](#v1alpha1.projects.locations) [REST Resource: v1alpha1.projects.locations.acceleratorTypes](#v1alpha1.projects.locations.acceleratorTypes) [REST Resource: v1alpha1.projects.locations.nodes](#v1alpha1.projects.locations.nodes) [REST Resource: v1alpha1.projects.locations.operations](#v1alpha1.projects.locations.operations) [REST Resource: v1alpha1.projects.locations.tensorflowVersions](#v1alpha1.projects.locations.tensorflowVersions) [REST Resource: v1.projects.locations](#v1.projects.locations) [REST Resource: v1.projects.locations.acceleratorTypes](#v1.projects.locations.acceleratorTypes) [REST Resource: v1.projects.locations.nodes](#v1.projects.locations.nodes) [REST Resource: v1.projects.locations.operations](#v1.projects.locations.operations) [REST Resource: v1.projects.locations.tensorflowVersions](#v1.projects.locations.tensorflowVersions)
Service: tpu.googleapis.com
To call this service, we recommend that you use the Google-provided
[client libraries](https://cloud.google.com/apis/docs/client-libraries-explained). If your application needs to use your own libraries to call this service, use the following information when you make the API requests.
Discovery document
A
[Discovery Document](https://developers.google.com/discovery/v1/reference/apis) is a machine-readable specification for describing and consuming REST APIs. It is used to build client libraries, IDE plugins, and other tools that interact with Google APIs. One service may provide multiple discovery documents. This service provides the following discovery documents:
Service endpoint
A
[service endpoint](https://cloud.google.com/apis/design/glossary#api_service_endpoint) is a base URL that specifies the network address of an API service. One service might have multiple service endpoints. This service has the following service endpoint and all URIs below are relative to this service endpoint:
https://tpu.googleapis.com
REST Resource:
[v2alpha1.projects.locations](/tpu/docs/reference/rest/v2alpha1/projects.locations)
|Methods
|
|
POST /v2alpha1/{parent=projects/*/locations/*}:generateServiceIdentity
Generates the Cloud TPU service identity for the project.
|
|
GET /v2alpha1/{name=projects/*/locations/*}
Gets information about a location.
|
|
GET /v2alpha1/{name=projects/*}/locations
Lists information about the supported locations for this service.
REST Resource:
[v2alpha1.projects.locations.acceleratorTypes](/tpu/docs/reference/rest/v2alpha1/projects.locations.acceleratorTypes)
|Methods
|
|
GET /v2alpha1/{name=projects/*/locations/*/acceleratorTypes/*}
Gets AcceleratorType.
|
|
GET /v2alpha1/{parent=projects/*/locations/*}/acceleratorTypes
Lists accelerator types supported by this API.
REST Resource:
[v2alpha1.projects.locations.nodes](/tpu/docs/reference/rest/v2alpha1/projects.locations.nodes)
|Methods
|
|
POST /v2alpha1/{parent=projects/*/locations/*}/nodes
Creates a node.
|
|
DELETE /v2alpha1/{name=projects/*/locations/*/nodes/*}
Deletes a node.
|
|
GET /v2alpha1/{name=projects/*/locations/*/nodes/*}
Gets the details of a node.
|
|
POST /v2alpha1/{name=projects/*/locations/*/nodes/*}:getGuestAttributes
Retrieves the guest attributes for the node.
|
|
GET /v2alpha1/{parent=projects/*/locations/*}/nodes
Lists nodes.
|
|
PATCH /v2alpha1/{node.name=projects/*/locations/*/nodes/*}
Updates the configurations of a node.
|
|
POST /v2alpha1/{name=projects/*/locations/*/nodes/*}:simulateMaintenanceEvent
Simulates a maintenance event.
|
|
POST /v2alpha1/{name=projects/*/locations/*/nodes/*}:start
Starts a node.
|
|
POST /v2alpha1/{name=projects/*/locations/*/nodes/*}:stop
Stops a node.
REST Resource:
[v2alpha1.projects.locations.operations](/tpu/docs/reference/rest/v2alpha1/projects.locations.operations)
|Methods
|
|
POST /v2alpha1/{name=projects/*/locations/*/operations/*}:cancel
Starts asynchronous cancellation on a long-running operation.
|
|
DELETE /v2alpha1/{name=projects/*/locations/*/operations/*}
Deletes a long-running operation.
|
|
GET /v2alpha1/{name=projects/*/locations/*/operations/*}
Gets the latest state of a long-running operation.
|
|
GET /v2alpha1/{name=projects/*/locations/*}/operations
Lists operations that match the specified filter in the request.
REST Resource:
[v2alpha1.projects.locations.queuedResources](/tpu/docs/reference/rest/v2alpha1/projects.locations.queuedResources)
|Methods
|
|
POST /v2alpha1/{parent=projects/*/locations/*}/queuedResources
Creates a QueuedResource TPU instance.
|
|
DELETE /v2alpha1/{name=projects/*/locations/*/queuedResources/*}
Deletes a QueuedResource TPU instance.
|
|
GET /v2alpha1/{name=projects/*/locations/*/queuedResources/*}
Gets details of a queued resource.
|
|
GET /v2alpha1/{parent=projects/*/locations/*}/queuedResources
Lists queued resources.
|
|
POST /v2alpha1/{name=projects/*/locations/*/queuedResources/*}:reset
Resets a QueuedResource TPU instance
REST Resource:
[v2alpha1.projects.locations.reservations](/tpu/docs/reference/rest/v2alpha1/projects.locations.reservations)
|Methods
|
|
GET /v2alpha1/{parent=projects/*/locations/*}/reservations
Retrieves the reservations for the given project in the given location.
REST Resource:
[v2alpha1.projects.locations.runtimeVersions](/tpu/docs/reference/rest/v2alpha1/projects.locations.runtimeVersions)
|Methods
|
|
GET /v2alpha1/{name=projects/*/locations/*/runtimeVersions/*}
Gets a runtime version.
|
|
GET /v2alpha1/{parent=projects/*/locations/*}/runtimeVersions
Lists runtime versions supported by this API.
REST Resource:
[v2.projects.locations](/tpu/docs/reference/rest/v2/projects.locations)
|Methods
|
|
POST /v2/{parent=projects/*/locations/*}:generateServiceIdentity
Generates the Cloud TPU service identity for the project.
|
|
GET /v2/{name=projects/*/locations/*}
Gets information about a location.
|
|
GET /v2/{name=projects/*}/locations
Lists information about the supported locations for this service.
REST Resource:
[v2.projects.locations.acceleratorTypes](/tpu/docs/reference/rest/v2/projects.locations.acceleratorTypes)
|Methods
|
|
GET /v2/{name=projects/*/locations/*/acceleratorTypes/*}
Gets AcceleratorType.
|
|
GET /v2/{parent=projects/*/locations/*}/acceleratorTypes
Lists accelerator types supported by this API.
REST Resource:
[v2.projects.locations.nodes](/tpu/docs/reference/rest/v2/projects.locations.nodes)
|Methods
|
|
POST /v2/{parent=projects/*/locations/*}/nodes
Creates a node.
|
|
DELETE /v2/{name=projects/*/locations/*/nodes/*}
Deletes a node.
|
|
GET /v2/{name=projects/*/locations/*/nodes/*}
Gets the details of a node.
|
|
POST /v2/{name=projects/*/locations/*/nodes/*}:getGuestAttributes
Retrieves the guest attributes for the node.
|
|
GET /v2/{parent=projects/*/locations/*}/nodes
Lists nodes.
|
|
PATCH /v2/{node.name=projects/*/locations/*/nodes/*}
Updates the configurations of a node.
|
|
POST /v2/{name=projects/*/locations/*/nodes/*}:start
Starts a node.
|
|
POST /v2/{name=projects/*/locations/*/nodes/*}:stop
Stops a node.
REST Resource:
[v2.projects.locations.operations](/tpu/docs/reference/rest/v2/projects.locations.operations)
|Methods
|
|
POST /v2/{name=projects/*/locations/*/operations/*}:cancel
Starts asynchronous cancellation on a long-running operation.
|
|
DELETE /v2/{name=projects/*/locations/*/operations/*}
Deletes a long-running operation.
|
|
GET /v2/{name=projects/*/locations/*/operations/*}
Gets the latest state of a long-running operation.
|
|
GET /v2/{name=projects/*/locations/*}/operations
Lists operations that match the specified filter in the request.
REST Resource:
[v2.projects.locations.runtimeVersions](/tpu/docs/reference/rest/v2/projects.locations.runtimeVersions)
|Methods
|
|
GET /v2/{name=projects/*/locations/*/runtimeVersions/*}
Gets a runtime version.
|
|
GET /v2/{parent=projects/*/locations/*}/runtimeVersions
Lists runtime versions supported by this API.
REST Resource:
[v1alpha1.projects.locations](/tpu/docs/reference/rest/v1alpha1/projects.locations)
|Methods
|
|
GET /v1alpha1/{name=projects/*/locations/*}
Gets information about a location.
|
|
GET /v1alpha1/{name=projects/*}/locations
Lists information about the supported locations for this service.
REST Resource:
[v1alpha1.projects.locations.acceleratorTypes](/tpu/docs/reference/rest/v1alpha1/projects.locations.acceleratorTypes)
|Methods
|
|
GET /v1alpha1/{name=projects/*/locations/*/acceleratorTypes/*}
Gets AcceleratorType.
|
|
GET /v1alpha1/{parent=projects/*/locations/*}/acceleratorTypes
Lists accelerator types supported by this API.
REST Resource:
[v1alpha1.projects.locations.nodes](/tpu/docs/reference/rest/v1alpha1/projects.locations.nodes)
|Methods
|
|
POST /v1alpha1/{parent=projects/*/locations/*}/nodes
Creates a node.
|
|
DELETE /v1alpha1/{name=projects/*/locations/*/nodes/*}
Deletes a node.
|
|
GET /v1alpha1/{name=projects/*/locations/*/nodes/*}
Gets the details of a node.
|
|
GET /v1alpha1/{parent=projects/*/locations/*}/nodes
Lists nodes.
|
|
POST /v1alpha1/{name=projects/*/locations/*/nodes/*}:reimage
Reimages a node's OS.
|
|
POST /v1alpha1/{name=projects/*/locations/*/nodes/*}:start
Starts a node.
|
|
POST /v1alpha1/{name=projects/*/locations/*/nodes/*}:stop
Stops a node.
REST Resource:
[v1alpha1.projects.locations.operations](/tpu/docs/reference/rest/v1alpha1/projects.locations.operations)
|Methods
|
|
POST /v1alpha1/{name=projects/*/locations/*/operations/*}:cancel
Starts asynchronous cancellation on a long-running operation.
|
|
DELETE /v1alpha1/{name=projects/*/locations/*/operations/*}
Deletes a long-running operation.
|
|
GET /v1alpha1/{name=projects/*/locations/*/operations/*}
Gets the latest state of a long-running operation.
|
|
GET /v1alpha1/{name=projects/*/locations/*}/operations
Lists operations that match the specified filter in the request.
REST Resource:
[v1alpha1.projects.locations.tensorflowVersions](/tpu/docs/reference/rest/v1alpha1/projects.locations.tensorflowVersions)
|Methods
|
|
GET /v1alpha1/{name=projects/*/locations/*/tensorflowVersions/*}
Gets TensorFlow Version.
|
|
GET /v1alpha1/{parent=projects/*/locations/*}/tensorflowVersions
Lists TensorFlow versions supported by this API.
REST Resource:
[v1.projects.locations](/tpu/docs/reference/rest/v1/projects.locations)
|Methods
|
|
GET /v1/{name=projects/*/locations/*}
Gets information about a location.
|
|
GET /v1/{name=projects/*}/locations
Lists information about the supported locations for this service.
REST Resource:
[v1.projects.locations.acceleratorTypes](/tpu/docs/reference/rest/v1/projects.locations.acceleratorTypes)
|Methods
|
|
GET /v1/{name=projects/*/locations/*/acceleratorTypes/*}
Gets AcceleratorType.
|
|
GET /v1/{parent=projects/*/locations/*}/acceleratorTypes
Lists accelerator types supported by this API.
REST Resource:
[v1.projects.locations.nodes](/tpu/docs/reference/rest/v1/projects.locations.nodes)
|Methods
|
|
POST /v1/{parent=projects/*/locations/*}/nodes
Creates a node.
|
|
DELETE /v1/{name=projects/*/locations/*/nodes/*}
Deletes a node.
|
|
GET /v1/{name=projects/*/locations/*/nodes/*}
Gets the details of a node.
|
|
GET /v1/{parent=projects/*/locations/*}/nodes
Lists nodes.
|
|
POST /v1/{name=projects/*/locations/*/nodes/*}:reimage
Reimages a node's OS.
|
|
POST /v1/{name=projects/*/locations/*/nodes/*}:start
Starts a node.
|
|
POST /v1/{name=projects/*/locations/*/nodes/*}:stop
Stops a node, this operation is only available with single TPU nodes.
REST Resource:
[v1.projects.locations.operations](/tpu/docs/reference/rest/v1/projects.locations.operations)
|Methods
|
|
POST /v1/{name=projects/*/locations/*/operations/*}:cancel
Starts asynchronous cancellation on a long-running operation.
|
|
DELETE /v1/{name=projects/*/locations/*/operations/*}
Deletes a long-running operation.
|
|
GET /v1/{name=projects/*/locations/*/operations/*}
Gets the latest state of a long-running operation.
|
|
GET /v1/{name=projects/*/locations/*}/operations
Lists operations that match the specified filter in the request.
REST Resource:
[v1.projects.locations.tensorflowVersions](/tpu/docs/reference/rest/v1/projects.locations.tensorflowVersions)
|Methods
|
|
GET /v1/{name=projects/*/locations/*/tensorflowVersions/*}
Gets TensorFlow Version.
|
|
GET /v1/{parent=projects/*/locations/*}/tensorflowVersions
List TensorFlow versions supported by this API.",Cloud TPU API | Google Cloud,
id,url,body,title,description
161,https://cloud.google.com/tpu/docs/reference/rest/Shared.Types/DeleteOperationRequest,"DeleteOperationRequest
Stay organized with collections
Save and categorize content based on your preferences.
|JSON representation
|
{
""name"": string
}
|Fields
|
name
|
string
The name of the operation resource to be deleted.
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2022-11-28 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",DeleteOperationRequest | Cloud TPU | Google Cloud,
id,url,body,title,description
164,https://cloud.google.com/tpu/docs/tutorials/mnist-2.x,"This tutorial contains a high-level description of the MNIST model, instructions on downloading the MNIST TensorFlow TPU code sample, and a guide to running the code on Cloud TPU.
Disclaimer
This tutorial uses a third-party dataset. Google provides no representation, warranty, or other guarantees about the validity, or any other aspects of this dataset.
Model description
The
[MNIST dataset](http://yann.lecun.com/exdb/mnist/) contains a large number of images of
hand-written digits in the range 0 to 9, as well as the labels identifying the
digit in each image.
This tutorial trains a machine learning model to classify images based on the MNIST dataset. After training, the model classifies incoming images into 10 categories (0 to 9) based on what it's learned about handwritten images from the MNIST dataset. You can then send the model an image that it hasn't seen before, and the model identifies the digit in the image based on what the model has learned during training.
The MNIST dataset has been split into three parts:
- 60,000 examples of training data
- 10,000 examples of test data
- 5,000 examples of validation data
The model has a mixture of seven layers:
- 2 x convolution
- 2 x max pooling
- 2 x dense (fully connected)
- 1 x dropout
Loss is computed via categorical cross entropy.
This version of the MNIST model uses the Keras API, a recommended way to build and run a machine learning model on a Cloud TPU.
Keras simplifies the model development process by hiding most of the low-level implementation, which also makes it easy to switch between TPU and other test platforms such as GPUs or CPUs.
Objectives
- Create a Cloud Storage bucket to hold your dataset and model output.
- Run the training job.
- Verify the output results.
Costs
In this document, you use the following billable components of Google Cloud:
- Compute Engine
- Cloud TPU
- Cloud Storage
To generate a cost estimate based on your projected usage,
use the
[pricing calculator](/products/calculator).
[free trial](/free-trial).
Before you begin
This section provides information on setting up Cloud Storage bucket and a Compute Engine VM.
Open a Cloud Shell window.
Create a variable for your project's ID.
export PROJECT_ID=project-id
Configure Google Cloud CLI to use the project where you want to create Cloud TPU.
gcloud config set project ${PROJECT_ID}
The first time you run this command in a new Cloud Shell VM, an
Authorize Cloud Shellpage is displayed. Click
Authorizeat the bottom of the page to allow
gcloudto make GCP API calls with your credentials.
Create a Service Account for the Cloud TPU project.
gcloud beta services identity create --service tpu.googleapis.com --project $PROJECT_ID
The command returns a Cloud TPU Service Account with following format:
service-PROJECT_NUMBER@cloud-tpu.iam.gserviceaccount.com
Create a Cloud Storage bucket using the following command:
gsutil mb -p ${PROJECT_ID} -c standard -l us-central1 gs://bucket-name
This Cloud Storage bucket stores the data you use to train your model and the training results. The
gcloudcommand used in this tutorial sets up default permissions for the Cloud TPU Service Account you set up in the previous step. If you want finer-grain permissions, review the
[access level permissions](/tpu/docs/storage-buckets).
Launch a Compute Engine VM and Cloud TPU using the
gcloudcommand. The command you use depends on whether you are using TPU VMs or TPU nodes. For more information, see
[System Architecture](/tpu/docs/system-architecture-tpu-vm).
TPU VM
$ gcloud compute tpus tpu-vm create mnist-tutorial \ --zone=us-central1-b \ --accelerator-type=v3-8 \ --version=tpu-vm-tf-2.15.0-pjrt \ --preemptible
Command flag descriptions
zone
- The
[zone](/tpu/docs/types-zones)where you plan to create your Cloud TPU.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
version
- The Cloud TPU
[software version](/tpu/docs/supported-tpu-versions#tpu_software_versions).
preemptible
- The
[preemptible](/tpu/docs/preemptible)TPUs cost less, but might be shut down at any time.
TPU Node
$ gcloud compute tpus execution-groups create \ --name=mnist-tutorial \ --zone=us-central1-b \ --tf-version=2.12.0 \ --machine-type=n1-standard-1 \ --accelerator-type=v3-8 \ --preemptible
Command flag descriptions
name
- The name of the Cloud TPU to create.
zone
- The
[zone](/tpu/docs/types-zones)where you plan to create your Cloud TPU.
tf-version
- The version of TensorFlow the
gcloudcommand installs on your VM.
machine-type
- The
[machine type](/compute/docs/machine-types)of the Compute Engine VM to create.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
preemptible
- The
[preemptible](https://cloud.google.com/tpu/docs/preemptible)TPUs cost less, but might be shut down at any time.
For more information on the
gcloudcommand, see the
[gcloud Reference](/sdk/gcloud/reference).
-
When the
gcloud compute tpuscommand has finished executing, verify that your shell prompt has changed from
username@projectnameto
username@vm-name. This change shows that you are now logged into your Compute Engine VM.
If you are not connected to the Compute Engine instance, you can connect by running the following command:
TPU VM
gcloud compute tpus tpu-vm ssh mnist-tutorial --zone=us-central1-b
TPU Node
gcloud compute ssh mnist-tutorial --zone=us-central1-b
As you continue these instructions, run each command that begins with
(vm)$in your VM session window.
Create an environment variable for the TPU name.
TPU VM
(vm)$ export TPU_NAME=local
TPU Node
(vm)$ export TPU_NAME=mnist-tutorial
Install TensorFlow requirements.
The command you use depends on whether you are using TPU VMs or TPU Nodes.
TPU VM
(vm)$ pip3 install -r /usr/share/tpu/models/official/requirements.txt
TPU Node
(vm)$ pip3 install --user tensorflow-model-optimization>=0.1.3
Train the model
The source code for the MNIST TPU model is available on
[GitHub](https://github.com/tensorflow/models/blob/master/official/legacy/image_classification/mnist_main.py).
Set the following variables. Replace bucket-name with your bucket name:
(vm)$ export STORAGE_BUCKET=gs://bucket-name (vm)$ export MODEL_DIR=${STORAGE_BUCKET}/mnist (vm)$ export DATA_DIR=${STORAGE_BUCKET}/data
Set the
PYTHONPATHenvironment variable.
TPU VM
(vm)$ export PYTHONPATH=""${PYTHONPATH}:/usr/share/tpu/models""
TPU Node
(vm)$ export PYTHONPATH=""${PYTHONPATH}:/usr/share/models""
- If you set
--version=tpu-vm-tf-2.15.0-pjrtwhen creating your TPU, set the following environment variables to enable the PJRT runtime:
(vm)$ export NEXT_PLUGGABLE_DEVICE_USE_C_API=true (vm)$ export TF_PLUGGABLE_DEVICE_LIBRARY_PATH=/lib/libtpu.so
Change to directory that stores the model:
TPU VM
(vm)$ cd /usr/share/tpu/models/official/legacy/image_classification
TPU Node
(vm)$ cd /usr/share/models/official/legacy/image_classification
Run the MNIST training script:
(vm)$ python3 mnist_main.py \ --tpu=${TPU_NAME} \ --model_dir=${MODEL_DIR} \ --data_dir=${DATA_DIR} \ --train_epochs=10 \ --distribution_strategy=tpu \ --download
Command flag descriptions
tpu
- The name of the Cloud TPU. If not specified when setting up the Compute Engine VM and Cloud TPU, defaults to your username.
model_dir
- The Cloud Storage bucket where checkpoints and summaries are stored during training. You can use an existing folder to load previously generated checkpoints created on a TPU of the same size and TensorFlow version.
data_dir
- The Cloud Storage path of training input. It is set to the fake_imagenet dataset in this example.
train_epochs
- The number of epochs to train the model.
distribution_strategy
- To train the ResNet model on a Cloud TPU, set
distribution_strategyto
tpu.
download
- When set to
true, the script downloads and preprocesses the MNIST dataset, if it hasn't been downloaded already.
-
The training script runs in under 5 minutes on a v3-8 Cloud TPU and displays output similar to:
Run stats: { 'accuracy_top_1': 0.9762369990348816, 'eval_loss': 0.07863274961709976, 'loss': 0.1111728847026825, 'training_accuracy_top_1': 0.966645359992981 }
Clean up
To avoid incurring charges to your Google Cloud account for the resources used in this tutorial, either delete the project that contains the resources, or keep the project and delete the individual resources.
Disconnect from the Compute Engine instance, if you have not already done so:
(vm)$ exit
Your prompt should now be
username@projectname, showing you are in the Cloud Shell.
Delete your Cloud TPU and Compute Engine resources. The command you use to delete your resources depends upon whether you are using TPU VMs or TPU Nodes. For more information, see
[System Architecture](/tpu/docs/system-architecture-tpu-vm).
TPU VM
$ gcloud compute tpus tpu-vm delete mnist-tutorial \ --zone=us-central1-b
TPU Node
$ gcloud compute tpus execution-groups delete mnist-tutorial \ --zone=us-central1-b
Verify the resources have been deleted by running
gcloud compute tpus execution-groups list. The deletion might take several minutes. The output from the following command should not include any of the resources created in this tutorial:
TPU VM
$ gcloud compute tpus tpu-vm list --zone=us-central1-b
TPU Node
$ gcloud compute tpus execution-groups list --zone=us-central1-b
Delete your Cloud Storage bucket using
gsutilas shown below. Replace bucket-name with the name of your Cloud Storage bucket.
$ gsutil rm -r gs://bucket-name
What's next
The TensorFlow Cloud TPU tutorials generally train the model using a
sample dataset. The results of this training are not usable
for inference. To use a model for inference, you can train the data on a
publicly available dataset or your own data set. TensorFlow models
trained on Cloud TPUs generally require datasets to be in
[TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) format.
You can use the
[dataset conversion tool sample](https://cloud.google.com/tpu/docs/classification-data-conversion) to convert an image classification dataset into TFRecord format. If you are not using an image classification model, you will have to convert your dataset to [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) format yourself. For more information, see [TFRecord and tf.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord).
Hyperparameter tuning
To improve the model's performance with your dataset, you can tune
the model's hyperparameters. You can find information about hyperparameters
common to all TPU supported models on
[GitHub](https://github.com/tensorflow/tpu/tree/master/models/hyperparameters).
Information about model-specific hyperparameters can be found in the
[source code](https://github.com/tensorflow/tpu/tree/master/models/official)
for each model. For more information on hyperparameter tuning, see
[Overview of hyperparameter tuning](https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview),
[Using the Hyperparameter tuning service](https://cloud.google.com/ai-platform/training/docs/using-hyperparameter-tuning),
and [Tune hyperparameters](https://developers.google.com/machine-learning/guides/text-classification/step-5).
Inference
Once you have trained your model you can use it for inference
(also called prediction).
[AI Platform](https://cloud.google.com/ai-platform/docs/technical-overview)
is a cloud-based solution for developing,
[training](https://cloud.google.com/ai-platform/training/docs),
and [deploying](https://cloud.google.com/ai-platform/prediction/docs/deploying-models)
machine learning models. Once a model is deployed, you can use the
[AI Platform Prediction service](https://cloud.google.com/ai-platform/prediction/docs).
- Run a Cloud TPU
[colab](https://colab.sandbox.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/03_Flower_pictures_to_TFRecords.ipynb)that demonstrates how to train an image classification model using your own image data.
- Explore the other
[Cloud TPU tutorials](/tpu/docs/tutorials).
- Learn to use the
[TPU monitoring tools in TensorBoard](/tpu/docs/cloud-tpu-tools).
- Verify performance on a large-scale model by
[running the ResNet sample](/tpu/docs/tutorials/resnet).",Running MNIST on Cloud TPU (TF 2.x) | Google Cloud,
id,url,body,title,description
152,https://cloud.google.com/tpu/docs/cloud-tpu-tools,"Profile your model on Cloud TPU Nodes
Profiling your model enables you to optimize training performance on Cloud TPUs.
To profile your model you use
[TensorBoard](https://www.tensorflow.org/tensorboard) and the
[Cloud TPU TensorBoard plug-in](#install-plugin). For installation
instructions, see [TensorBoard installation instructions](#install-tensorboard).
For more information about using TensorBoard with one of the supported frameworks, see the following documents:
Prerequisites
TensorBoard is installed as part of TensorFlow. TensorFlow is installed by
default in Cloud TPU Nodes. You can also
[install TensorFlow manually](https://www.tensorflow.org/install).
Either way, some additional dependencies may be required. Install these by
running:
(vm)$ pip3 install --user -r /usr/share/models/official/requirements.txt
Install Cloud TPU TensorBoard Plugin
SSH into your TPU Node:
$ gcloud compute ssh your-vm --zone=your-zone
Run the following commands:
pip3 install --upgrade ""cloud-tpu-profiler>=2.3.0"" pip3 install --user --upgrade -U ""tensorboard>=2.3"" pip3 install --user --upgrade -U ""tensorflow>=2.3""
Capturing a profile
You can capture a profile using the TensorBoard UI or programmatically.
Capture a profile using TensorBoard
When you start TensorBoard, it starts a web server. When you point your browser to the TensorBoard URL, it displays a web page. The web page enables you to manually capture a profile and view the profile data.
Start the TensorFlow Profiler server
tf.profiler.experimental.server.start(6000)
This starts up the TensorFlow profiler server on your TPU VM.
Start your training script
Run your training script and wait until you see output indicating your model is
actively training. What this looks like depends on your code and model. Look for
output like
Epoch 1/100. Alternatively, you can navigate to the Cloud TPU page
in the
[Google Cloud console](https://console.cloud.google.com/compute/tpus), select your TPU and view the CPU utilization graph. While
this does not show TPU utilization, it is a good indication that the TPU is
training your model.
Start the TensorBoard server
Open a new terminal window and ssh into your TPU VM with port forwarding. This allows your local browser to communicate with the TensorBoard server running on your TPU VM.
gcloud compute tpus execution-groups ssh your-vm --zone=us-central1-a --ssh-flag=""-4 -L 9001:localhost:9001""
Run TensorBoard in the terminal window you just opened and specify the directory
where TensorBoard can write profiling data with the
--logdir flag. For example:
TPU_LOAD_LIBRARY=0 tensorboard --logdir your-model-dir --port 9001
TensorBoard starts a web server and displays its URL:
Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all TensorBoard 2.3.0 at http://localhost:9001/ (Press CTRL+C to quit)
Open a web browser and go to the URL displayed in the TensorBoard output. Make sure TensorBoard has fully loaded the profiling data by clicking the reload button in the upper right hand corner of the TensorBoard page. By default the TensorBoard page comes up with the Scalars tab selected.
Capture a profile on TPU Nodes
- Select PROFILE from the dropdown menu at the top of the screen
- Select the CAPTURE PROFILE button
- Select the TPU Name radio button
- Type your TPU name
- Select the CAPTURE button
Capture a profile programmatically
How you programmatically capture a profile depends on what ML framework you are using.
If you are using TensorFlow, you start and stop the profiler using
tf.profiler.experimental.start() and
tf.profiler.experimental.stop()
respectively. For more information, see
[TensorFlow performance guide](/tpu/docs/tensorflow-performance-guide).
If you are using JAX, use
jax.profiler.start_trace() and
jax.profiler.stop_trace()
to start and stop the profiler respectively. For more information, see
[Profiling JAX programs](https://jax.readthedocs.io/en/latest/profiling.html).
Capture profile common problems
Sometimes when you try to capture a trace you might see messages like the following:
No trace event is collected after xx attempt(s). Perhaps, you want to try again
(with more attempts?).Tip: increase number of attempts with --num_tracing_attempts.
Failed to capture profile: empty trace result
This can occur if the TPU is not actively performing calculations, a training step is taking too long, or other reasons. If you are seeing this message, try the following:
- Try to capture a profile after a few epochs have run.
- Try increasing the profiling duration in the TensorBoard Capture Profile dialog. It's possible that a training step is taking too long.
- Make sure both VM and TPU have the same TF version.
View profile data with TensorBoard
The Profile tab is displayed after you have captured some model data. You may need to click the reload button in the upper right hand corner of the TensorBoard page. Once data is available, clicking on the Profile tab presents a selection of tools to help with performance analysis:
[Overview page](#overview_page) [Trace viewer](#trace_viewer)(Chrome browser only) [Streaming trace viewer](#stream_tr_viewer)(Chrome browser only)
Trace viewer
Trace viewer is a Cloud TPU performance analysis tool available
under Profile. The tool uses the
[Chrome trace event profiling viewer](https://github.com/catapult-project/catapult/tree/master/tracing)
so it only works in the Chrome browser.
Trace viewer displays a timeline that shows:
- Durations for the operations that were executed by your TensorFlow model .
- Which part of the system (TPU or host machine) executed an operation. Typically, the host machine executes infeed operations, which preprocesses training data and transfers it to the TPU, whereas the TPU executes the actual model training.
Trace viewer allows you to identify performance problems in your model, then take steps to resolve them. For example, at a high level, you can identify whether infeed or model training is taking the majority of the time. Drilling down, you can identify which TensorFlow operations are taking the longest to execute.
Note that trace viewer is limited to 1M events per Cloud TPU. If
you need to assess more events, use the
[streaming trace viewer](#stream_tr_viewer)
instead.
Trace viewer interface
To open trace viewer, go to TensorBoard, click on the Profile tab at the top of the screen, and choose trace_viewer from the Tools dropdown. The viewer appears displaying your most recent run:
This screen contains the following main elements (marked with numbers above):
- Runs dropdown. Contains all of the runs for which you've captured trace information. The default view is your most recent run, but you can open the dropdown to select a different run.
- Tools dropdown. Selects different profiling tools.
- Host dropdown. Selects a host that contains a Cloud TPU set.
- Timeline pane. Shows operations that Cloud TPU and the host machine executed over time.
- Details pane. Shows additional information for operations selected in the Timeline pane.
Here's a closer look at the timeline pane:
The Timeline pane contains the following elements:
- Top bar. Contains various auxiliary controls.
- Time axis. Shows time relative to the beginning of the trace.
- Section and track labels. Each section contains multiple tracks and has a triangle on the left that you can click to expand and collapse the section. There is one section for every processing element in the system.
- Tool selector. Contains various tools for interacting with the trace viewer.
- Events. These show the time during which an operation was executed or the duration of meta-events, such as training steps.
- Vertical tab bar. This does not have a useful purpose for Cloud TPU. The bar is part of the general purpose trace viewer tool provided by Chrome that is used for a variety of performance analysis tasks.
Sections and tracks
Trace viewer contains the following sections:
- One section for each TPU node, labeled with the number of the TPU chip
and the TPU node within the chip (for example, ""Chip 2: TPU Core 1""). Each
TPU node section contains the following tracks:
- Step. Shows the duration of the training steps that were running on the TPU.
- TensorFlow Ops. Shows TensorFlow operations executed on the TPU.
- XLA Ops. Shows
[XLA](https://www.tensorflow.org/performance/xla/)operations that ran on the TPU. (Each operation is translated into one or several XLA operations. The XLA compiler translates the XLA operations into code that runs on the TPU.)
- One section for threads running on the host machine's CPU, labeled ""Host Threads"". The section contains one track for each CPU thread. Note: You can ignore the information displayed alongside the section labels.
Timeline tool selector
You can interact with the timeline view using the timeline tool selector in
TensorBoard. You can click on a timeline tool or use the following
[keyboard shortcuts](#keyboard_shortcuts) to activate and highlight a tool.
To move the timeline tool selector, click in the dotted area at the top and then
drag the selector to where you want it.
Use the timeline tools as follows:
|Selection tool
Click on an event to select it or drag to select multiple events. Additional information about the selected event or events (name, start time, and duration) will be displayed in the details pane.
|Pan tool
Drag to pan the timeline view horizontally and vertically.
|Zoom tool
Drag up to zoom in or drag down to zoom out along the horizontal (time) axis. The horizontal position of the mouse cursor determines the center around which the zoom takes place.
Note: The zoom tool has a known bug where zoom remains active if you release the mouse button while the mouse cursor is outside the timeline view. If this happens to you, just click briefly on the timeline view to stop zooming.
|Timing tool
Drag horizontally to mark a time interval. The length of the interval appears on the time axis. To adjust the interval, drag its ends. To clear the interval, click anywhere inside the timeline view.
Note that the interval remains marked if you select one of the other tools.
Graphs
TensorBoard provides a number of visualizations, or graphs, of your model and
its performance. Use the graphs together with the
[trace viewer](#trace_viewer)
or [streaming trace viewer](#stream_tr_viewer) to fine tune your models and
improve their performance on Cloud TPU.
Model graph
The modeling framework may generate a
graph from your model. The data for the graph is stored in the
MODEL_DIR
directory in the storage bucket you specify with the
--logdir
parameter. You can view this graph without running
capture_tpu_profile.
To view a model's graph, select the Graphs tab in TensorBoard.
A single node in the structure graph represents a single operation.
TPU compatibility graph
The Graphs tab includes a compatibility checker module which checks for and displays ops that can potentially cause issues when a model is run.
To view a model's TPU compatibility graph, select the Graphs tab in TensorBoard and then select the TPU Compatibility option. The graph presents the compatible (valid) operations in green and the incompatible (invalid) operations in red.
A given node can display both colors, each as a percentage of the
Cloud TPU compatibility operations for that node. See
[Interpreting compatibility results](#compatibility) for an example.
The compatibility summary panel displayed to the right of the graph shows the percentage of all Cloud TPU-compatible operations, their attributes and a list of incompatible operations for a selected node.
Click on any operation in the graph to display its attributes in the summary panel.
Note that compatibility checker does not assess any operations that are
explicitly assigned to a non-TPU device using
[manual device placement](https://www.tensorflow.org/guide/gpu#manual_device_placement).
In addition, the checker does not actually compile the model for execution, so
be sure to interpret the results as an estimate of compatibility.
Interpreting compatibility results
Profile
The Profile tab, is displayed after you have captured some model data. You may need to click the reload button in the upper right hand corner of the TensorBoard page. Once data is available, clicking on the Profile tab presents a selection of tools to help with performance analysis:
[Overview page](#overview_page) [Input pipeline analyzer](#input_pipeline_analyzer) [XLA Op profile](#op_profile) [Trace viewer](#trace_viewer)(Chrome browser only) [Memory viewer](#memory_viewer) [Pod viewer](#pod_viewer) [Streaming trace viewer](#stream_tr_viewer)(Chrome browser only)
Profile overview page
The overview page (overview_page), available under Profile, provides a top level view of how your model performed during a capture run. The page shows you an aggregated overview page for all the TPUs, as well as an overall input pipeline analysis. There is an option for selecting individual TPUs in the Host dropdown.
The page displays data in the following panels:
Performance summary
- Average Step Time - The step time averaged over all sampled steps
- Host Idle Time - The percentage of time the Host was idle
- TPU Idle Time - The percentage of time the TPU was idle
- FLOPS Utilization - The percentage utilization of the TPU matrix units
- Memory Bandwidth Utilization - The percentage of memory bandwidth used
Step-time graph. Displays a graph of device step time (in milliseconds) over all the steps sampled. The blue area corresponds to the portion of the step time the TPUs were sitting idle waiting for input data from the host. The red area shows how much of time the Cloud TPU was actually working.
Top 10 TensorFlow operations on TPU. Displays the TensorFlow operations that consumed the most time:
Each row displays an operation's self time (as the percentage of time taken by all operations), cumulative time, category, name, and the FLOPS rate achieved.
Run environment
- Number of hosts used
- Type of TPU used
- Number of TPU cores
- Training batch size
Recommendation for next steps. Reports when a model is input bound and whenever issues with Cloud TPU occur. Suggests tools you can use to locate performance bottlenecks in performance.
Input pipeline analyzer
The input pipeline analyzer provides insights into your performance results. The
tool displays performance results from the
input_pipeline.json file
that is collected by the
[capture_tpu_profile](#capture_trace) tool.
The tool tells you immediately whether your program is input bound and can walk you through device and host-side analysis to debug whatever stage(s) of the pipeline are creating bottlenecks.
See the guidance on
[input pipeline performance](https://www.tensorflow.org/guide/profiler#input_pipeline_analyzer)
for deeper insight into optimizing pipeline performance.
Input pipeline
When a TensorFlow program reads data from a file it begins at the top of the TensorFlow graph in a pipelined manner. The read process is divided into multiple data processing stages connected in series, where the output of one stage is the input to the next one. This system of reading is called the input pipeline.
A typical pipeline for reading records from files has the following stages:
- File reading
- File preprocessing (optional)
- File transfer from the host machine to the device
An inefficient input pipeline can severely slow down your application. An application is considered input bound when it spends a significant portion of time in its input pipeline. Use the input pipeline analyzer to understand where the input pipeline is inefficient.
Input pipeline dashboard
To open the input pipeline analyzer, select Profile, then select input_pipeline_analyzer from the Tools dropdown.
The dashboard contains three sections:
[Summary.](#ip_summary)Summarizes the overall input pipeline with information on whether your application is input bound and, if so, by how much. [Device-side analysis.](#ip_device_side)Displays detailed, device-side analysis results, including the device step-time and the range of device time spent waiting for input data across cores at each step. [Host-side analysis.](#ip_host_side)Shows a detailed analysis on the host side, including a breakdown of input processing time on the host.
Input pipeline summary
The first section reports if your program is input bound by presenting the percentage of device time spent on waiting for input from the host. If you are using a standard input pipeline that has been instrumented, the tool reports where most of the input processing time is spent. For example:
Device-side analysis
The second section details the device-side analysis, providing insights on time spent on the device versus on the host and how much device time was spent waiting for input data from the host.
- Device step time statistics. Reports the average, standard deviation, and range (minimum, maximum) of the device step time.
- Step time. Displays a graph of device step time (in milliseconds) over all the steps sampled. The blue area corresponds to the part of the step time Cloud TPUs sat idle waiting for input data from the host. The red area shows how much of time the Cloud TPU was actually working.
- Percentage of time waiting for input data. Reports the average, standard deviation and the range (minimum, maximum) of the fraction of time spent on a device waiting for the input data normalized to the total device step time.
- Range of device time across cores spent waiting for input data, by step number. Displays a line chart showing the amount of device time (expressed as a percentage of total device step time) spent waiting for input data processing. The fraction of time spent varies from core to core, so the range of fractions for each core is also plotted for each step. Since the time a step takes is determined by the slowest core, you want the range to be as small as possible.
Host-side analysis
Section 3 shows the details of host-side analysis, reporting of the input processing time (the time spent on Dataset API operations) on the host broken into several categories:
- Enqueuing data to be transferred to device Time spent putting data into an infeed queue before transferring the data to the device.
- Data preprocessing. Time spent on preprocessing operations, such as image decompression.
- Reading data from files in advance. Time spent reading files, including caching, prefetching, and interleaving.
- Reading data from files on demand. Time spent on reading data from files without caching, prefetching, and interleaving.
- Other data reading or processing. Time spent on other input related operations
not using
tf.data.
To see the statistics for individual input operations and their categories broken down by execution time, expand the ""Show Input Op statistics"" section.
A source data table like the following appears:
Each table entry contains the following information:
- Input Op. Shows the TensorFlow op name of the input operation.
- Count. Shows the total number of instances of the operation executed during the profiling period.
- Total Time (in ms). Shows the cumulative sum of time spent on each of operation instances.
- Total Time %. Shows the total time spent on an operation as a fraction of the total time spent in input processing.
- Total Self Time (in ms). Shows the accumulative sum of the self time
spent on each of those instances. The self time measures the time spent
inside the function body, excluding the time spent in the function it calls.
For example, the
Iterator::PaddedBatch::Filter::ForeverRepeat::Mapis called by
Iterator::PaddedBatch::Filter, therefore it's total self time is excluded from the total self time of the latter.
- Total Self Time %. Shows the total self time as a fraction of the total time spent on input processing.
- Category. Shows the processing category of the input operation.
Op profile
Op profile is a Cloud TPU tool that displays the performance
statistics of
[XLA](https://www.tensorflow.org/performance/xla/) operations
executed during a profiling period. The op profile shows:
- How well your application uses the Cloud TPU as a percentage of time spent on operations by category and of TPU FLOPS utilization.
- The most time-consuming operations. Those operations are potential targets for optimization.
- Details of individual operations, including shape, padding and expressions that use the operation.
You can use op profile to find good targets for optimization. For example, if your model achieves only 5% of the TPU peak FLOPS, you can use the tool to identify which XLA operations are taking the longest time to execute and how many TPU FLOPS they consume.
Using op profile
During profile collection,
[ also creates
a capture_tpu_profile](#capture_trace)
op_profile.json file that contains performance statistics of XLA operations.
You can view the data from op_profile in TensorBoard by clicking on the Profile tab at the top of the screen and then selecting op_profile from the Tools dropdown. You will see a display like this:
- Overview section. Shows Cloud TPU utilization and provides suggestions for optimization.
- Control panel. Contains controls that allow you to set the number of operations displayed in the table, which operations are displayed, and how they are sorted.
[Op table](#op_table). A table that lists the top TensorFlow operation categories associated with the XLA ops. These operations are sorted by percentage of Cloud TPU usage. [Op details cards](#op_details_cards). Details about the op that appear when you hover over an op in the table. These include the FLOPS utilization, the expression in which the op is used, and the op layout (fit).
XLA Op table
The Op table lists XLA operation categories in order from the highest to lowest percentage of Cloud TPU usage. Initially, the table shows the percentage of time taken, the op category name, the associated TensorFlow op name, and the percentage of FLOPS utilization for the category. To display (or hide) the 10 most time-consuming XLA operations for a category, click the triangle next to the category name in the table.
- Time. Shows the total percentage of time spent by all the operations in that category. You can click to expand the entry and see the breakdown of time spent by each individual operation.
- Top10 Ops. The toggle next to a category's name displays/hides the top 10 time-consuming operations within the category. If a fusion operation entry is displayed in the operations list, you can expand it to see the non-fusion, elementwise operations it contains.
- TensorFlow Op. Shows the TensorFlow op name associated with the XLA operation.
- FLOPS. Shows the FLOPS utilization, which is the measured number of FLOPS expressed as a percentage of the Cloud TPU peak FLOPS. The higher the FLOPS utilization percentage, the faster operations run. The table cell is color coded: green for high FLOPS utilization (good) and red for low FLOPS utilization (bad).
Op details cards
When you select a table entry, a card appears on the left displaying details about the XLA op or the operation category. A typical card looks like this:
- Name and Category. Shows the highlighted XLA operation name and category.
- FLOPS utilization. Displays FLOPS utilization as a percentage of total FLOPS possible.
- Expression. Shows the
[XLA expression](https://www.tensorflow.org/performance/xla/operation_semantics)containing the operation.
- Memory Utilization. Displays the percentage of peak memory usage by your program.
- Layout (Convolution operations only.) Shows the
[shape and layout](https://www.tensorflow.org/performance/xla/shapes)of a tensor, including whether the shape of the tensor is an exact fit for the matrix units and how the matrix is padded.
Interpreting results
For convolution operations, TPU FLOPS utilization can be low due to one or both of the following reasons:
- padding (matrix units are partially used)
- convolution op is memory bound
This section gives an interpretation of some numbers from a different model in which FLOPs were low. In this example, output fusion and convolution dominated the execution time and there was a long tail of vector or scalar operations that had very low FLOPS.
One optimization strategy for this type of profile is to transform the vector or scalar operations to convolution operations.
In the following example, %convolution.399 shows lower FLOPS and memory utilization than %convolution.340 in the previous example.
Examine the layout and note that batch size 16 is being padded to 128 and feature size 3 is being padded to 8, which indicates that only 5% of the matrix units are being effectively used. (The calculation for this instance of percent utilization is (((batch_time * num_of_features) / padding_size ) / num_of_cores). Compare the FLOPS in this example to the %convolution.340 in the previous example which has an exact fit to the matrix.
Pod viewer
The Pod viewer tool provides performance visualizations for every core in a Pod and displays the status of the communications channels across the cores in a Pod. Pod viewer can identify and highlight potential bottlenecks and areas that need optimization. The tool works for full Pods and all v2 and v3 Pod slices.
To display the Pod viewer tool:
- Select Profile from the menu button at the top right side of the TensorBoard window.
- Click the Tools menu on the left side of the window and select pod_viewer.
The Pod viewer user interface includes:
- A step slider, which allows you to select which step you want to examine.
- A topology graph, which interactively visualizes your TPU cores in the whole TPU system.
- A communication links chart, which visualizes the send and receive (recv) channels in the topology graph.
- A latency of send and recv channels bar chart. Hovering over a bar in this chart activates the communication links in the communication links chart. A channel details card appears on the left-hand bar, providing detailed information of the channel, such as the size of data transferred, latency, and bandwidth.
- A step breakdown chart, which visualizes a breakdown of a step for all cores. This can be used to track system bottlenecks and whether a particular core is slowing down the system.
Step slider
Use the slider to select a step. The rest of the tool displays statistics, such as step breakdown and communication links, for that step.
Topology graph
The topology graph is organized hierarchically by host, chip and core. The smallest rectangles are TPU cores. Two cores together indicate a TPU chip and four chips together indicate a host.
The topology graph is also a heatmap, color coded by the percentage of time a particular breakdown (for example, High flops compute, infeed, send, etc.) takes in the selected step. The bar just below the topology graph (shown in the following graphic) shows a color coding for core and chip usage. The color of the cores show the utilization ranging from yellow to blue. For High flops compute, larger numbers (darker color) indicate more time spent doing compute. For all other breakdowns, smaller numbers (lighter colors) indicate smaller wait times. Potential problem areas, or hotspots, are indicated when a core is darker than the others.
Click on the pulldown menu selector next to the system name (circled in the diagram) to choose the particular type of breakdown you want to examine.
Hover your mouse over any of the small rectangles (single cores) to display a techtip showing the core's position in the system, its global chip ID, and its host name. The techtip also includes the duration of the selected breakdown category, for example High flops, and its utilization percentage out of a step.
Communication channels
This tool helps visualize send and recv links if your model uses them to communicate between cores. When your model contains send and recv ops, you can use a channel ID selector to select a channel ID. A link from the source (src) core and destination (dst) core, represents the communication channel. It is rendered on the topology graph by hovering your mouse over the bars on the chart showing the latency of send and recv channels.
A card appears on the left-hand bar giving you more details about the communication channel. A typical card looks like this:
- Data Transferred, which shows the data transferred by the send and recv channel in memibytes (MiB).
- Latency, which shows the duration, in microseconds, from the start of the send event to the end of the recv-done event.
- BW, which shows the amount of data transferred, in gibibites (GiB), from the source core to the destination core in the duration of time.
- Send Delay, which is the duration from the beginning of the recv-done to the beginning of send in microseconds. If the recv-done op starts after the beginning of the send op, the delay is zero.
- Hlo Names, which displays the XLA hlo ops names associated with this channel. These hlo names are associated with the statistics displayed in other TensorBoard tools such as op_profile and memory_viewer.
Step breakdown chart
This chart provides details for each training or evaluation step.
The x-axis is the global chip ID and the y-axis is the time in microseconds. From this chart, you can see where the time is used in a particular training step, where any bottlenecks are, and whether there is a load imbalance across all chips.
A card appears on the left-hand bar giving you more details about the step breakdown. A typical card looks like this:
The fields in the card specify the following:
- High Flops Compute, which is the time spent on convolution or output fusion operations (ops).
- Low flops compute, which is calculated by deducting all other breakdowns from the total duration.
- Infeed, which is the time the TPU spends waiting on the host.
- Outfeed, which is the time the host spends waiting on output from the TPU.
- AllReduce sync, which is the portion of time spent on
[CrossReplicaSum ops](https://www.tensorflow.org/xla/operation_semantics#crossreplicasum)that is waiting to synchronize with other cores. CrossReplicaSum ops computes the sum across replicas.
- AllReduce compute, which is the actual compute time spent on CrossReplicaSum ops.
- Chip to chip send ops, which is the time spent on send operations.
- Chip to chip recv-done ops, which is the time spent on recv operations.
Trace viewer
Trace viewer is a Cloud TPU performance analysis tool available
under Profile. The tool uses the
[Chrome trace event profiling viewer](https://github.com/catapult-project/catapult/tree/master/tracing)
so it only works in the Chrome browser.
Trace viewer displays a timeline that shows:
- Durations for the operations that were executed by your TensorFlow model .
- Which part of the system (TPU or host machine) executed an operation. Typically, the host machine executes infeed operations, which preprocesses training data and transfers it to the TPU, whereas the TPU executes the actual model training.
Trace viewer allows you to identify performance problems in your model, then take steps to resolve them. For example, at a high level, you can identify whether infeed or model training is taking the majority of the time. Drilling down, you can identify which TensorFlow operations are taking the longest to execute.
Note that trace viewer is limited to 1M events per Cloud TPU. If
you need to assess more events, use the
[streaming trace viewer](#stream_tr_viewer)
instead.
Trace viewer interface
To open trace viewer, go to TensorBoard, click on the Profile tab at the top of the screen, and choose trace_viewer from the Tools dropdown. The viewer appears displaying your most recent run:
This screen contains the following main elements (marked with numbers above):
- Runs dropdown. Contains all of the runs for which you've captured trace information. The default view is your most recent run, but you can open the dropdown to select a different run.
- Tools dropdown. Selects different profiling tools.
- Host dropdown. Selects a host that contains a Cloud TPU set.
- Timeline pane. Shows operations that Cloud TPU and the host machine executed over time.
- Details pane. Shows additional information for operations selected in the Timeline pane.
Here's a closer look at the timeline pane:
The Timeline pane contains the following elements:
- Top bar. Contains various auxiliary controls.
- Time axis. Shows time relative to the beginning of the trace.
- Section and track labels. Each section contains multiple tracks and has a triangle on the left that you can click to expand and collapse the section. There is one section for every processing element in the system.
- Tool selector. Contains various tools for interacting with the trace viewer.
- Events. These show the time during which an operation was executed or the duration of meta-events, such as training steps.
- Vertical tab bar. This does not have a useful purpose for Cloud TPU. The bar is part of the general purpose trace viewer tool provided by Chrome that is used for a variety of performance analysis tasks.
Sections and tracks
Trace viewer contains the following sections:
- One section for each TPU node, labeled with the number of the TPU chip
and the TPU node within the chip (for example, ""Chip 2: TPU Core 1""). Each
TPU node section contains the following tracks:
- Step. Shows the duration of the training steps that were running on the TPU.
- TensorFlow Ops. Shows TensorFlow operations executed on the TPU.
- XLA Ops. Shows
[XLA](https://www.tensorflow.org/performance/xla/)operations that ran on the TPU. (Each operation is translated into one or several XLA operations. The XLA compiler translates the XLA operations into code that runs on the TPU.)
- One section for threads running on the host machine's CPU, labeled ""Host Threads"". The section contains one track for each CPU thread. Note: You can ignore the information displayed alongside the section labels.
Timeline tool selector
You can interact with the timeline view using the timeline tool selector in
TensorBoard. You can click on a timeline tool or use the following
[keyboard shortcuts](#keyboard_shortcuts) to activate and highlight a tool.
To move the timeline tool selector, click in the dotted area at the top and then
drag the selector to where you want it.
Use the timeline tools as follows:
|Selection tool
Click on an event to select it or drag to select multiple events. Additional information about the selected event or events (name, start time, and duration) will be displayed in the details pane.
|Pan tool
Drag to pan the timeline view horizontally and vertically.
|Zoom tool
Drag up to zoom in or drag down to zoom out along the horizontal (time) axis. The horizontal position of the mouse cursor determines the center around which the zoom takes place.
Note: The zoom tool has a known bug where zoom remains active if you release the mouse button while the mouse cursor is outside the timeline view. If this happens to you, just click briefly on the timeline view to stop zooming.
|Timing tool
Drag horizontally to mark a time interval. The length of the interval appears on the time axis. To adjust the interval, drag its ends. To clear the interval, click anywhere inside the timeline view.
Note that the interval remains marked if you select one of the other tools.
Events
Events within the timeline are displayed in different colors; the colors themselves have no specific meaning.
Timeline top bar
The top bar of the Timeline pane contains several auxiliary controls:
- Metadata display. Not used for TPUs.
- View Options. Not used for TPUs.
- Search box. Enter text to search for all events whose name contains the text. Click the arrow buttons to the right of the search box to move forwards and backwards through the matching events, selecting each event in turn.
- Console button. Not used for TPUs.
- Help button. Click to display a help summary.
Keyboard shortcuts
Following are the keyboard shortcuts you can use in trace viewer. Click the help button (?) in the top bar to see more keyboard shortcuts.
w Zoom in s Zoom out a Pan left d Pan right f Zoom to selected event(s) m Mark time interval for selected event(s) 1 Activate selection tool 2 Activate pan tool 3 Activate zoom tool 4 Activate timing tool
The f shortcut can be highly useful. Try selecting a step and pressing f to zoom into the step quickly.
Characteristic events
Following are some of the event types that can be very useful when analyzing TPU performance.
InfeedDequeueTuple. This TensorFlow operation runs on a TPU and receives input data coming from the host. When infeed takes a long time, it can mean that the TensorFlow operations which preprocess the data on the host machine cannot keep up with the TPU data consumption rate. You can see corresponding events in the host traces called InfeedEnqueueTuple. To view a more detailed input-pipeline analysis, use the
[Input Pipeline Analyzer](#input_pipeline_analyzer)tool.
CrossReplicaSum. This TensorFlow operation runs on a TPU and computes a sum across replicas. Because each replica corresponds to a different TPU node, the operation must wait for all TPU nodes to be finished with a step. If this operation is taking a long time, it might not mean that the summing operation itself is slow but that a TPU node is waiting for another TPU node with a slow data infeed.
- Dataset Ops. Trace viewer visualizes dataset operations performed when
data is loaded using the
[Dataset API](https://www.tensorflow.org/programmers_guide/datasets). The
Iterator::Filter::Batch::ForeverRepeat::Memoryin the example is compiled and it corresponds to the
dataset.map()operation. Use trace viewer to examine the loading operations as you work through debugging and mitigating input pipeline bottlenecks.
- Prefetch Threads. Using
dataset.prefetch()to buffer input data can prevent sporadic slowdowns in file access that create bottlenecks in the input pipeline.
What can go wrong
Here are some potential issues to be aware of when using trace viewer:
- Event display limit. Trace viewer displays a maximum of 1 million
events. If you captured more events, only the earliest 1 million events are
displayed; later events are dropped. To capture more TPU events, you can
use the
--include_dataset_ops=Falseflag to explicitly require
capture_tpu_profileto exclude the dataset ops.
- Very long events. Events that begin before a capture starts or that end after a capture is finished are not visible in trace viewer. Consequently, very long events can be missed.
When to start trace capture. Be sure to start trace capture after you know the Cloud TPU is running. If you start before then, you may see only a few events or no events at all in trace viewer. You can increase the profile time using the
--duration_msflag and you can set automatic retries using the
--num_tracing_attemptsflag. For example:
(vm)$ capture_tpu_profile --tpu=$TPU_NAME --logdir=${MODEL_DIR} --duration_ms=60000 --num_tracing_attempts=10
Memory viewer
Memory viewer allows you to visualize the peak memory usage for your program, and memory usage trends over the program's lifetime.
The memory viewer UI looks like this:
- Host dropdown. Selects for a TPU host and XLA High Level Optimizer (HLO) modules to visualize.
- Memory overview. Displays peak memory allocation and size without padding.
- Working space chart. Displays peak memory use and a plot of memory usage trends over the program's lifetime. Hovering over a buffer in one of the buffer charts adds an annotation for the buffer lifetime and the buffer details card.
- Buffer charts. Two charts that display buffer allocation at the point of peak memory usage, as indicated by the vertical line in the working space plot. Hovering over a buffer in one of the buffer charts displays the buffer's lifetime bar in the working space chart and a details card on the left.
- Buffer allocation details card. Displays allocation details for a buffer.
Memory overview panel
The memory overview (top) panel shows you the module name and the peak memory allocation set when the total buffer allocation size reaches the maximum. The unpadded peak allocation size is also shown for comparison.
Working space chart
This chart displays peak memory use and a plot of memory usage trends over the program's lifetime. The line drawn from top to bottom of the plot indicates peak memory utilization for the program. This point determines whether or not a program can fit into the available global memory space.
Each point on the overlying line plot represents a ""program point"" in XLA's HLO program as scheduled by the compiler. The line provides a sense of the spikiness leading to and from the peak usage.
Interaction with buffer chart elements
When you hover over a buffer displayed in one of the buffer charts below the working space chart, a horizontal lifetime line for that buffer appears in the working space chart. The horizontal line is the same color as the highlighted buffer.
The horizontal line thickness indicates the relative magnitude of the buffer size relative to the peak memory allocation. The line length corresponds to the life of the buffer, starting at the point in the program where buffer space was allocated and ending where the space was freed.
Buffer charts
Two charts show the breakdown of memory usage at the peak usage point (indicated by the vertical line in the plot above the charts).
By Program Order. Displays the buffers from left to right in the order in which they were active during program execution. Buffers active for the longest time are on the left side of the chart.
By Size. Displays the buffers that were active during program execution in descending size order. Buffers that had the largest impact at the point of peak memory usage are on the left.
Buffer allocation details card
When you hover over a buffer displayed in one of the buffer charts, a buffer allocation details card appears (in addition to the lifetime line displayed in the working chart). A typical details card looks like this:
- Name. Name of the XLA operation.
- Category. Operation category.
- Size. Size of the buffer allocation (including padding).
- Unpadded size. Size of the buffer allocation without padding.
- Expansion. Relative magnitude of padded buffer size versus the unpadded size.
- Extra memory. Indicates how much extra memory is used for padding.
- Shape. Describes the rank, size, and data type of the N-dimensional array.
- TensorFlow op name. Shows the name of the TensorFlow operation associated with the buffer allocation.
- Allocation type. Indicates buffer allocation category. Types are: Parameter, Output, Thread-local, and Temporary (for example, buffer allocation within a fusion).
""Out of memory"" errors
If you run a model and get an ""out of memory error"", use the following command to capture a memory profile and view it in the memory viewer. Make sure to set appropriate duration_ms so that the profiling period overlaps with your program compilation time. The output can help you understand what caused the error:
(vm)$ capture_tpu_profile --tpu=$TPU_NAME --logdir=${MODEL_DIR} --duration_ms=60000
Streaming trace viewer
Streaming trace viewer (
trace_viewer) is a Cloud TPU
performance analysis tool, available for
TensorFlow 2.15.0 or later, that provides dynamic trace
renderings. The tool uses the
[Chrome trace event profiling viewer](https://github.com/catapult-project/catapult/tree/master/tracing)
so it works only in the Chrome browser.
When you use
capture_tpu_profile to
capture a profile, a .tracetable file is saved to your Google Cloud storage
bucket. The file contains a large number of trace events that can be viewed in
in both trace viewer and streaming trace viewer.
Using streaming trace viewer
To use the streaming trace viewer,
trace_viewer, you must shut down your
existing TensorBoard session and then relaunch TensorBoard using the IP address
of the TPU you want to examine. Streaming trace viewer requires TensorBoard to
make a Google Remote Procedure Call (GRPC) to an IP address for the
Cloud TPU. The GRPC channel is not encrypted.
You can find the IP address for a Cloud TPU host on the
[Cloud TPU page](https://console.cloud.google.com/compute/tpu). Find your
Cloud TPU and look in
the Internal IP column for the IP address.
In your VM, run TensorBoard as follows replacing tpu-ip with your TPU's IP address:
(vm)$ tensorboard --logdir=${MODEL_DIR} \
--master_tpu_unsecure_channel=tpu-ip
The in TensorBoard tool appears in the Tools dropdown list.
In the timeline, you can zoom in and out to see trace events load dynamically into your browser.
Monitoring your Cloud TPU job
This section describes how to use
capture_tpu_profile to capture a
single profile or continuously monitor your Cloud TPU job on the
command-line interface in real time. By setting the
--monitoring_level
option to
0 (the default),
1, or
2, you
get a single profile, basic monitoring, or detailed monitoring, respectively.
Open a new Cloud Shell and ssh to your VM (replace vm-name in the command with your VM name):
(vm)$ gcloud compute ssh vm-name \
--ssh-flag=-L6006:localhost:6006
In the new Cloud Shell, run
capture_tpu_profile with the
--monitoring_levelflag set to either 1 or 2, such as:
(vm)$ capture_tpu_profile --tpu=$TPU_NAME \
--monitoring_level=1
Setting monitoring_level=1 produces output similar to the following:
TPU type: TPU v2 Utilization of TPU Matrix Units is (higher is better): 10.7%
Setting monitoring_level=2 displays more detailed information:
TPU type: TPU v2 Number of TPU Cores: 8 TPU idle time (lower is better): 0.091% Utilization of TPU Matrix Units is (higher is better): 10.7% Step time: 1.95 kms (avg), 1.90kms (minute), 2.00 kms (max) Infeed percentage: 87.5% (avg). 87.2% (min), 87.8 (max)
Monitoring flags
--tpu(required) specifies the name of the Cloud TPU you want to monitor.
--monitoring_level. Change the behavior of
capture_tpu_profilefrom producing a single profile, to basic or detailed continuous monitoring. There are three available levels: Level 0 (the default): Produces a single profile, then exits. Level 1: Shows TPU version and TPU utilization. Level 2: Shows the TPU utilization, TPU idle time, and number of TPU cores used. Also provides min, avg, and max step times along with the infeed percentage contribution.
--duration_ms(optional, default is 1000ms) specifies how long to profile the TPU host during each cycle. Generally, this should be long enough to capture at least one training step worth of data. 1 second captures a training step in most models but if your model step time is very large, you can set the value to 2x
step_time(in ms).
--num_queriesspecifies how many cycles to run
capture_tpu_profile. To continuously monitor your TPU job, set the value to a high number. To quickly check your model's step time set the value to a low number.",Profile your model on Cloud TPU Nodes | Google Cloud,
id,url,body,title,description
112,https://cloud.google.com/tpu/docs/reference/rest/v2/projects.locations.runtimeVersions,"REST Resource: projects.locations.runtimeVersions
Stay organized with collections
Save and categorize content based on your preferences.
Resource: RuntimeVersion
A runtime version that a Node can be configured with.
|JSON representation
|
{
""name"": string,
""version"": string
}
|Fields
|
name
|
string
The resource name.
|
version
|
string
The runtime version.
|
Methods
|
|
Gets a runtime version.
|
|
Lists runtime versions supported by this API.
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2022-11-28 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",REST Resource: projects.locations.runtimeVersions | Cloud TPU | Google Cloud,
id,url,body,title,description
58,https://cloud.google.com/tpu/docs/reference/rest/v1/projects.locations/get,"Method: projects.locations.get
Stay organized with collections
Save and categorize content based on your preferences.
Gets information about a location.
HTTP request
GET https://tpu.googleapis.com/v1/{name=projects/*/locations/*}
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
string
Resource name for the location.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[Location](/tpu/docs/reference/rest/Shared.Types/ListLocationsResponse#Location)
Authorization scopes
Requires one of the following OAuth scopes:
-
https://www.googleapis.com/auth/cloud.tpu
-
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Method: projects.locations.get | Cloud TPU | Google Cloud,
id,url,body,title,description
197,https://cloud.google.com/tpu/docs/internal-ip-blocks,"Internal IP address ranges for TPU Nodes
TPU VM architecture
When you use TPU VMs, you SSH directly into the TPU VM.
TPU Node architecture
TPU Nodes run on a
[peer VPC network](/vpc/docs/vpc-peering) that Google
manages. When you create a TPU Node, the system automatically creates a
peer connection to a [VPC network](/vpc/docs/vpc) from your project. The peer
connection provides access to the TPU Node from your instances, GKE clusters,
and other Cloud Platform services that run on the same VPC network.
When you are writing applications to automatically create and manage TPU Nodes,
you can
[reserve allocated IP address ranges](/vpc/docs/configure-private-services-access#allocating-range)
to automatically identify open IP address ranges on your VPC network and reserve
them to be used for your TPU Nodes.
Automatic IP address configuration
When you create a TPU VM or TPU Node using the Google Cloud Platform (GCP)
[Google Cloud CLI](https://cloud.google.com/sdk/gcloud/reference/compute/tpus/),
or the [Google Cloud console](https://console.cloud.google.com/), you do not need to
specify an internal IP address or address range since the IP network address
is automatically configured.
Manual IP address configuration
You can also manually specify an internal IP address range for your TPU Node. You can use any valid address in your user-specified network or VPC internal network as long as it does not conflict with other addresses already used on that network.
Manually configured IP address ranges for TPU Nodes must meet the following requirements:
The IP address range must be from within the internal IP address ranges:
10.0.0.0 - 10.255.255.255 (10/8 prefix) 172.16.0.0 - 172.31.255.255 (172.16/12 prefix) 192.168.0.0 - 192.168.255.255 (192.168/16 prefix)
Your range must be one of the following formats:
W.X.Y.Z/N W.X.Y.Z
where:
W,X,Y,Zare integers in the range 0-255 with no leading zeros.
/Nis an optional address range size. If you do not specify a range size Cloud TPU selects the correct range size for your TPU type.
-
Your range must have enough addresses to accommodate the size of your TPU type. If you omit the range size, Cloud TPU selects the correct range size for your TPU type. If you must specify a range size, select a range size that provides enough addresses for 1/4th the number of TPU cores in your node with at least 8 addresses at a minimum. For example, if you select a
v3-512TPU type with 512 cores, your range must have a size of
/25to provide 128 addresses for the TPU peer network. For both
v3-8and
v3-32TPU types, you must specify the minimum range size of
/29to provide at least 8 addresses.
If you must select an address range, select one that does not conflict with another network resource on the network that you are using. For example, if you deploy your Cloud TPU on the ""default"" network that is created on standard Compute Engine projects, then this network will already have a subnetwork for
us-central1using the
10.128.0.0/20range and you cannot place any of your subnetworks inside this range. This is true for all other regions.",Internal IP address ranges for TPU Nodes | Google Cloud,
id,url,body,title,description
53,https://cloud.google.com/tpu/docs/tpu-iap,"Connect to a TPU VM without a public IP address
If your organization has a
constraints/compute.vmExternalIpAccess organization
policy constraint, you need to create TPU VMs that do not have an external IP
address. To connect to a TPU VM without an external IP address, you need to:
- Enable
[Private Google Access](/vpc/docs/private-google-access)for the subnet where you will create a TPU VM.
- Grant
roles/iap.tunnelResourceAccessorand
roles/tpu.adminto users who will connect to the TPU VMs.
- Create a TPU VM without a public IP address.
- SSH into your TPU VM using
--tunnel-through-iap.
Enable Private Service Access
To use an IAP, you must enable Private Google Access which allows you to connect to VMs that do not have external IP addresses. In the following command replace your-subnet with the name of the subnet where you will create the TPU VM and your-region with the region where the TPU VM will be located.
gcloud compute networks subnets update your-subnet \ --region=your-region \ --enable-private-ip-google-access
Grant permissions
Users that need to SSH into TPU VMs that do not have public IP addresses must be granted the iap.tunnelResourceAccessor role. For more information about granting a role, see
[Granting an IAM Role](/iam/docs/granting-changing-revoking-access#grant-single-role).
Create a TPU VM without a public IP address
The following command shows how to create a TPU VM with no public IP address.
gcloud compute tpus tpu-vm create tpu-vm-name \ --zone $ZONE \ --project your-project \ --internal-ips \ --version tpu-vm-tf-2.15.0-pjrt \ --accelerator-type v2-8 \ --subnetwork your-subnet \
SSH into your TPU VM using IAP tunneling
The following command shows how to SSH into a TPU VM using IAP tunneling.
gcloud alpha compute tpus tpu-vm ssh tpu-vm-name --tunnel-through-iap",Connect to a TPU VM without a public IP address | Google Cloud,
id,url,body,title,description
128,https://cloud.google.com/tpu/docs/reference/rest/Shared.Types/CancelOperationRequest,"CancelOperationRequest
Stay organized with collections
Save and categorize content based on your preferences.
|JSON representation
|
{
""name"": string
}
|Fields
|
name
|
string
The name of the operation resource to be cancelled.
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2022-11-28 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",CancelOperationRequest | Cloud TPU | Google Cloud,
id,url,body,title,description
198,https://cloud.google.com/tpu/docs/reference/rest/v2alpha1/projects.locations.queuedResources/create,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Query parameters](#body.QUERY_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [Try it!](#try-it)
Creates a QueuedResource TPU instance.
HTTP request
POST https://tpu.googleapis.com/v2alpha1/{parent=projects/*/locations/*}/queuedResources
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
parent
|
Required. The parent resource name.
Query parameters
|Parameters
|
queuedResourceId
|
The unqualified resource name. Should follow the
|
requestId
|
Idempotent request UUID.
Request body
The request body contains an instance of
.
[QueuedResource](/tpu/docs/reference/rest/v2alpha1/projects.locations.queuedResources#QueuedResource)
Response body
If successful, the response body contains a newly created instance of
.
[Operation](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse#Operation)
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).",Method: projects.locations.queuedResources.create | Cloud TPU | Google Cloud,
id,url,body,title,description
173,https://cloud.google.com/tpu/docs/reference/rest/v2/AcceleratorConfig,"AcceleratorConfig
Stay organized with collections
Save and categorize content based on your preferences.
A TPU accelerator configuration.
|JSON representation
|
{
""type"": enum (
),
""topology"": string
}
[Type](/tpu/docs/reference/rest/v2/AcceleratorConfig#Type)
|Fields
|
type
|
enum (
)
[Type](/tpu/docs/reference/rest/v2/AcceleratorConfig#Type)
Required. Type of TPU.
|
topology
|
string
Required. Topology of TPU in chips.
Type
|Enums
|
TYPE_UNSPECIFIED
|Unspecified version.
|
V2
|TPU v2.
|
V3
|TPU v3.
|
V4
|TPU v4.
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",AcceleratorConfig | Cloud TPU | Google Cloud,
id,url,body,title,description
48,https://cloud.google.com/tpu/docs/tutorials/transformer-2.x,"If you are not familiar with Cloud TPU, it is strongly recommended that you
go through the
[quickstart](https://cloud.google.com/tpu/docs/quickstart) to learn how to
create a TPU VM.
This tutorial shows you how to train a Transformer model on Cloud TPU. Transformer is a neural network architecture that solves sequence to sequence problems using attention mechanisms. Unlike traditional neural seq2seq models, Transformer does not involve recurrent connections. The attention mechanism learns dependencies between tokens in two sequences. Since attention weights apply to all tokens in the sequences, the Transformer model is able to easily capture long-distance dependencies.
Transformer's overall structure follows the standard encoder-decoder pattern. The encoder uses self-attention to compute a representation of the input sequence. The decoder generates the output sequence one token at a time, taking the encoder output and previous decoder-output tokens as inputs.
The model also applies embeddings on the input and output tokens, and adds a constant positional encoding. The positional encoding adds information about the position of each token.
Objectives
- Create a Cloud Storage bucket to hold your dataset and model output.
- Download and pre process the dataset used to train the model.
- Run the training job.
- Verify the output results.
Costs
In this document, you use the following billable components of Google Cloud:
- Compute Engine
- Cloud TPU
To generate a cost estimate based on your projected usage,
use the
[pricing calculator](/products/calculator).
[free trial](/free-trial).
Before you begin
Before starting this tutorial, check that your Google Cloud project is correctly set up.
-
Sign in to your Google Cloud account. If you're new to
Google Cloud,
[create an account](https://console.cloud.google.com/freetrial)to evaluate how our products perform in real-world scenarios. New customers also get $300 in free credits to run, test, and deploy workloads.
-
In the Google Cloud console, on the project selector page, select or
[create a Google Cloud project](/resource-manager/docs/creating-managing-projects).
-
[Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console).
-
In the Google Cloud console, on the project selector page, select or
[create a Google Cloud project](/resource-manager/docs/creating-managing-projects).
-
[Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console).
This walkthrough uses billable components of Google Cloud. Check the
[Cloud TPU pricing page](/tpu/docs/pricing) to
estimate your costs. Be sure to
[clean up](#clean-up)
resources you create when you've finished with them to avoid unnecessary
charges.
Train with a single Cloud TPU device
This section provides information on setting up a Cloud Storage bucket and a TPU VM for single device training.
Open a Cloud Shell window.
Create an environment variable for your project ID.
export PROJECT_ID=project-id
Configure Google Cloud CLI to use the your Google Cloud project where you want to create a Cloud TPU.
gcloud config set project ${PROJECT_ID}
The first time you run this command in a new Cloud Shell VM, an
Authorize Cloud Shellpage is displayed. Click
Authorizeat the bottom of the page to allow
gcloudto make Google Cloud API calls with your credentials.
Create a Service Account for the Cloud TPU project.
Service accounts allow the Cloud TPU service to access other Google Cloud services.
$ gcloud beta services identity create --service tpu.googleapis.com --project $PROJECT_ID
The command returns a Cloud TPU Service Account with following format:
service-PROJECT_NUMBER@cloud-tpu.iam.gserviceaccount.com
Create a Cloud Storage bucket using the following command:
$ gsutil mb -p ${PROJECT_ID} -c standard -l us-central2 gs://bucket-name
This Cloud Storage bucket stores the data you use to train your model and the training results. The
gcloudcommand used in this tutorial to set up the TPU also sets up default permissions for the Cloud TPU Service Account you set up in the previous step. If you want finer-grain permissions, review the
[access level permissions](/tpu/docs/storage-buckets).
Train the Transformer model on a single Cloud TPU
Launch a Cloud TPU VM using the
gcloudcommand.
$ gcloud compute tpus tpu-vm create transformer-tutorial \ --zone=us-central2-b \ --accelerator-type=v4-8 \ --version=tpu-vm-tf-2.15.0-pjrt
Command flag descriptions
zone
- The
[zone](/tpu/docs/types-zones)where you plan to create your Cloud TPU.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
version
- The Cloud TPU
[software version](/tpu/docs/supported-tpu-versions#tpu_software_versions).
-
Connect to the Cloud TPU VM by running the following
sshcommand.
gcloud compute tpus tpu-vm ssh transformer-tutorial --zone=us-central2-b
Export environment variables.
(vm)$ export STORAGE_BUCKET=gs://bucket-name (vm)$ export SENTENCEPIECE_MODEL=sentencepiece (vm)$ export SENTENCEPIECE_MODEL_PATH=${STORAGE_BUCKET}/${SENTENCEPIECE_MODEL}.model (vm)$ export TFDS_DIR=${STORAGE_BUCKET}/tfds (vm)$ export PARAM_SET=big (vm)$ export TPU_NAME=local (vm)$ export MODEL_DIR=${STORAGE_BUCKET}/transformer/model_${PARAM_SET} (vm)$ export PYTHONPATH=""/usr/share/tpu/models:$PYTHONPATH""
- If you set
--version=tpu-vm-tf-2.15.0-pjrtwhen creating your TPU, set the following environment variables to enable the PJRT runtime:
(vm)$ export NEXT_PLUGGABLE_DEVICE_USE_C_API=true (vm)$ export TF_PLUGGABLE_DEVICE_LIBRARY_PATH=/lib/libtpu.so
Install Tensorflow requirements.
(vm)$ pip3 install -r /usr/share/tpu/models/official/requirements.txt
Download and preprocess the dataset
(vm)$ python3 -c ""import tensorflow_datasets as tfds; tfds.load('wmt14_translate/de-en', split='train+validation', shuffle_files=True, download=True)"" (vm)$ python3 /usr/share/tpu/models/official/nlp/data/train_sentencepiece.py --output_model_path=${SENTENCEPIECE_MODEL}
Copy the dataset to the Cloud Storage bucket
(vm)$ gsutil -m cp ${SENTENCEPIECE_MODEL}.model ${STORAGE_BUCKET} (vm)$ gsutil -m cp ${SENTENCEPIECE_MODEL}.vocab ${STORAGE_BUCKET} (vm)$ gsutil -m cp -r tensorflow_datasets/wmt14_translate ${TFDS_DIR}/wmt14_translate
Navigate to the training directory
(vm)$ cd /usr/share/tpu/models/
Run the training script
(vm)$ python3 official/nlp/train.py \ --tpu=${TPU_NAME} \ --experiment=wmt_transformer/large \ --mode=train_and_eval \ --model_dir=${MODEL_DIR} \ --params_override=""runtime.distribution_strategy=tpu, task.train_data.tfds_data_dir=${TFDS_DIR}, task.validation_data.tfds_data_dir=${TFDS_DIR}, task.sentencepiece_model_path=${SENTENCEPIECE_MODEL_PATH}, trainer.train_steps=10000, trainer.validation_interval=10000""
Command flag descriptions
tpu
- The name of the Cloud TPU. This is set by specifying
the environment variable (
TPU_NAME).
experiment
- The model to train.
mode
- The mode in which to run the script.
model_dir
- The Cloud Storage bucket where checkpoints and summaries are stored during training. You can use an existing folder to load previously generated checkpoints created on a TPU of the same size and TensorFlow version.
params_override
- Set model parameters.
By default, the model will evaluate after every 10,000 steps. You can increase the number of training steps or specify how often to run evaluations by setting these parameters:
train.train_steps: The total number of training steps to run.
trainer.validation_interval: The number of training steps to run between evaluations.
Training and evaluation takes approximately 20 minutes on a v4-8 Cloud TPU. When the training and evaluation complete, a message similar to the following appears:
I0208 20:57:19.309512 140039467895872 controller.py:310] eval | step: 10000 | eval time: 69.2 sec | output: {'bleu_score': 19.204771518707275, 'sacrebleu_score': 18.307039308307356, 'validation_loss': 2.0654342} eval | step: 10000 | eval time: 69.2 sec | output: {'bleu_score': 19.204771518707275, 'sacrebleu_score': 18.307039308307356, 'validation_loss': 2.0654342}
You have now completed single-device training. Use the following steps to delete your single-device TPU resources.
-
Disconnect from the Compute Engine instance:
(vm)$ exit
Your prompt should now be
username@projectname, showing you are in the Cloud Shell.
Delete the TPU resource.
$ gcloud compute tpus tpu-vm delete transformer-tutorial \ --zone=us-central2-b
Command flag descriptions
zone
- The
[zone](https://cloud.google.com/tpu/docs/types-zones)where your Cloud TPU resided.
-
At this point, you can either conclude this tutorial and
[clean up](#cleanup),
or you can continue and explore running the model on Cloud TPU Pods.
Scale your model with Cloud TPU Pods
Training your model on Cloud TPU Pods may require some changes to
your training script. For information, see
[Training on TPU Pods](/tpu/docs/training-on-tpu-pods).
TPU Pod training
Open a Cloud Shell window.
Create a variable for your project ID.
export PROJECT_ID=project-id
Configure Google Cloud CLI to use the project where you want to create a Cloud TPU.
gcloud config set project ${PROJECT_ID}
The first time you run this command in a new Cloud Shell VM, an
Authorize Cloud Shellpage is displayed. Click
Authorizeat the bottom of the page to allow
gcloudto make API calls with your credentials.
Create a Service Account for the Cloud TPU project.
gcloud beta services identity create --service tpu.googleapis.com --project $PROJECT_ID
The command returns a Cloud TPU Service Account with following format:
service-PROJECT_NUMBER@cloud-tpu.iam.gserviceaccount.com
Create a Cloud Storage bucket using the following command or use a bucket you created earlier for your project:
gsutil mb -p ${PROJECT_ID} -c standard -l us-central1 gs://bucket-name
This Cloud Storage bucket stores the data you use to train your model and the training results. The
gcloudcommand used in this tutorial sets up default permissions for the Cloud TPU Service Account you set up in the previous step. If you want finer-grain permissions, review the
[access level permissions](/tpu/docs/storage-buckets).
Launch the TPU VM resources
Launch a TPU VM Pod using the
gcloudcommand. This tutorial specifies a v4-32 Pod. For other Pod options, see
[TPU types](/tpu/docs/types-topologies) [available TPU types page](/tpu/docs/regions-zones).
$ gcloud compute tpus tpu-vm create transformer-tutorial \ --zone=us-central2-b \ --accelerator-type=v3-32 \ --version=tpu-vm-tf-2.15.0-pod-pjrt
Command flag descriptions
zone
- The
[zone](/tpu/docs/types-zones)where you plan to create your Cloud TPU.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
version
- The Cloud TPU
[software version](/tpu/docs/supported-tpu-versions#tpu_software_versions).
-
Connect to the TPU VM by running the following
sshcommand. When you are logged into the VM, your shell prompt changes from
username@projectnameto
username@vm-name:
gcloud compute tpus tpu-vm ssh transformer-tutorial --zone=us-central2-b
Install TensorFlow requirements.
(vm)$ pip3 install -r /usr/share/tpu/models/official/requirements.txt
Set up and start the Pod training
Export Cloud TPU setup variables:
(vm)$ export PYTHONPATH=""/usr/share/tpu/models:$PYTHONPATH"" (vm)$ export STORAGE_BUCKET=gs://bucket-name (vm)$ export SENTENCEPIECE_MODEL=sentencepiece (vm)$ export SENTENCEPIECE_MODEL_PATH=${STORAGE_BUCKET}/${SENTENCEPIECE_MODEL}.model (vm)$ export TFDS_DIR=${STORAGE_BUCKET}/tfds (vm)$ export TPU_NAME=transformer-tutorial (vm)$ export PARAM_SET=big (vm)$ export MODEL_DIR=${STORAGE_BUCKET}/transformer/model_${PARAM_SET} (vm)$ export TPU_LOAD_LIBRARY=0
Download the dataset
(vm)$ python3 -c ""import tensorflow_datasets as tfds; tfds.load('wmt14_translate/de-en', split='train+validation', shuffle_files=True, download=True)"" (vm)$ python3 /usr/share/tpu/models/official/nlp/data/train_sentencepiece.py --output_model_path=${SENTENCEPIECE_MODEL}
Copy the dataset to Cloud Storage bucket
(vm)$ gsutil -m cp ${SENTENCEPIECE_MODEL}.model ${STORAGE_BUCKET} (vm)$ gsutil -m cp ${SENTENCEPIECE_MODEL}.vocab ${STORAGE_BUCKET} (vm)$ gsutil -m cp -r tensorflow_datasets/wmt14_translate ${TFDS_DIR}/wmt14_translate
Change to the training directory:
(vm)$ cd /usr/share/tpu/models/
Run the training script:
(vm)$ python3 official/nlp/train.py
--tpu=${TPU_NAME}
--experiment=wmt_transformer/large
--mode=train_and_eval
--model_dir=${MODEL_DIR}
--params_override=""runtime.distribution_strategy=tpu, task.train_data.tfds_data_dir=${TFDS_DIR}, task.validation_data.tfds_data_dir=${TFDS_DIR}, task.sentencepiece_model_path=${SENTENCEPIECE_MODEL_PATH}, trainer.train_steps=10000, trainer.validation_interval=10000""
Command flag descriptions
tpu
- The name of the Cloud TPU. This is set by specifying
the environment variable (
TPU_NAME).
experiment
- The model to train.
mode
- The mode in which to run the script.
model_dir
- The Cloud Storage bucket where checkpoints and summaries are stored during training. You can use an existing folder to load previously generated checkpoints created on a TPU of the same size and TensorFlow version.
params_override
- Set model parameters.
-
By default, the model will evaluate after every 10000 steps. In order to
train to convergence, change
train_steps to 200000.
You can increase the
number of training steps or specify how often to run evaluations by setting
these parameters:
trainer.train_steps: Sets the total number of training steps to run.
trainer.validation_interval: Sets the number of training steps to run between evaluations.
Training and evaluation takes approximately 14 minutes on a v4-32 Cloud TPU. When the training and evaluation complete, messages similar to the following appear:
I0209 22:19:49.143219 139751309618240 controller.py:310] eval | step: 10000 | eval time: 73.6 sec | output: {'bleu_score': 19.401752948760986, 'sacrebleu_score': 18.442741330886378, 'validation_loss': 2.0558002} eval | step: 10000 | eval time: 73.6 sec | output: {'bleu_score': 19.401752948760986, 'sacrebleu_score': 18.442741330886378, 'validation_loss': 2.0558002}
This training script trains for 20000 steps and runs evaluation every 2000 steps. This particular training and evaluation takes approximately 8 minutes on a v3-32 Cloud TPU Pod. When the training and evaluation complete, a message similar to the following appears:
INFO:tensorflow:Writing to file /tmp/tmpdmlanxcf I0218 21:09:19.100718 140509661046592 translate.py:184] Writing to file /tmp/tmpdmlanxcf I0218 21:09:28.043537 140509661046592 transformer_main.py:118] Bleu score (uncased): 1.799112930893898 I0218 21:09:28.043911 140509661046592 transformer_main.py:119] Bleu score (cased): 1.730366237461567
In order to train to convergence, change
train_steps to 200000. You
can increase the number of training steps or specify how often to run
evaluations by setting these parameters:
--train_steps: Sets the total number of training steps to run.
--steps_between_evals: Number of training steps to run between evaluations.
When the training and evaluation complete, a message similar to the following appears:
0509 00:27:59.984464 140553148962624 translate.py:184] Writing to file /tmp/tmp_rk3m8jp I0509 00:28:11.189308 140553148962624 transformer_main.py:119] Bleu score (uncased): 1.3239131309092045 I0509 00:28:11.189623 140553148962624 transformer_main.py:120] Bleu score (cased): 1.2855342589318752
Clean up
To avoid incurring charges to your Google Cloud account for the resources used in this tutorial, either delete the project that contains the resources, or keep the project and delete the individual resources.
Disconnect from the Compute Engine instance, if you have not already done so:
(vm)$ exit
Your prompt should now be
username@projectname, showing you are in the Cloud Shell.
Delete your Cloud TPU and Compute Engine resources.
$ gcloud compute tpus tpu-vm delete transformer-tutorial \ --zone=us-central2-b
Run
gsutilas shown, replacing bucket-name with the name of the Cloud Storage bucket you created for this tutorial:
$ gsutil rm -r gs://bucket-name
What's next
The TensorFlow Cloud TPU tutorials generally train the model using a
sample dataset. The results of this training are not usable
for inference. To use a model for inference, you can train the data on a
publicly available dataset or your own data set. TensorFlow models
trained on Cloud TPUs generally require datasets to be in
[TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) format.
You can use the
[dataset conversion tool sample](https://cloud.google.com/tpu/docs/classification-data-conversion) to convert an image classification dataset into TFRecord format. If you are not using an image classification model, you will have to convert your dataset to [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) format yourself. For more information, see [TFRecord and tf.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord).
Hyperparameter tuning
To improve the model's performance with your dataset, you can tune
the model's hyperparameters. You can find information about hyperparameters
common to all TPU supported models on
[GitHub](https://github.com/tensorflow/tpu/tree/master/models/hyperparameters).
Information about model-specific hyperparameters can be found in the
[source code](https://github.com/tensorflow/tpu/tree/master/models/official)
for each model. For more information on hyperparameter tuning, see
[Overview of hyperparameter tuning](https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview),
[Using the Hyperparameter tuning service](https://cloud.google.com/ai-platform/training/docs/using-hyperparameter-tuning),
and [Tune hyperparameters](https://developers.google.com/machine-learning/guides/text-classification/step-5).
Inference
Once you have trained your model you can use it for inference
(also called prediction).
[AI Platform](https://cloud.google.com/ai-platform/docs/technical-overview)
is a cloud-based solution for developing,
[training](https://cloud.google.com/ai-platform/training/docs),
and [deploying](https://cloud.google.com/ai-platform/prediction/docs/deploying-models)
machine learning models. Once a model is deployed, you can use the
[AI Platform Prediction service](https://cloud.google.com/ai-platform/prediction/docs).",Training transformer on Cloud TPU (TF 2.x) | Google Cloud,
id,url,body,title,description
110,https://cloud.google.com/tpu/docs/reference/rest/v1/projects.locations.operations/delete,"Send feedback
Method: projects.locations.operations.delete
Stay organized with collections
Save and categorize content based on your preferences.
Deletes a long-running operation. This method indicates that the client is no longer interested in the operation result. It does not cancel the operation. If the server doesn't support this method, it returns
google.rpc.Code.UNIMPLEMENTED.
HTTP request
DELETE https://tpu.googleapis.com/v1/{name=projects/*/locations/*/operations/*}
The URL uses
gRPC Transcoding syntax.
Path parameters
Parameters
name
string
The name of the operation resource to be deleted.
Request body
The request body must be empty.
Response body
If successful, the response body is empty.
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
Authentication Overview.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]
Need to tell us more?",Method: projects.locations.operations.delete | Cloud TPU | Google Cloud,
id,url,body,title,description
153,https://cloud.google.com/tpu/docs/reference/rest/v1alpha1/projects.locations.nodes/delete,"Method: projects.locations.nodes.delete
Stay organized with collections
Save and categorize content based on your preferences.
HTTP request
DELETE https://tpu.googleapis.com/v1alpha1/{name=projects/*/locations/*/nodes/*}
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
string
Required. The resource name.
Query parameters
|Parameters
|
requestId
|
string
Idempotent request UUID.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[Operation](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse#Operation)
Authorization scopes
Requires one of the following OAuth scopes:
-
https://www.googleapis.com/auth/cloud.tpu
-
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Method: projects.locations.nodes.delete | Cloud TPU | Google Cloud,
id,url,body,title,description
160,https://cloud.google.com/tpu/docs/reference/rest/v1/projects.locations.nodes/list,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Query parameters](#body.QUERY_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [Try it!](#try-it)
Lists nodes.
HTTP request
GET https://tpu.googleapis.com/v1/{parent=projects/*/locations/*}/nodes
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
parent
|
Required. The parent resource name.
Query parameters
|Parameters
|
pageSize
|
The maximum number of items to return.
|
pageToken
|
The nextPageToken value returned from a previous List request, if any.
Request body
The request body must be empty.
Response body
Response for
.
[nodes.list](/tpu/docs/reference/rest/v1/projects.locations.nodes/list#google.cloud.tpu.v1.Tpu.ListNodes)
If successful, the response body contains data with the following structure:
|JSON representation
|
{
""nodes"": [
{
object (
|Fields
|
nodes[]
|
The listed nodes.
|
nextPageToken
|
The next page token or empty if none.
|
unreachable[]
|
Locations that could not be reached.
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).",Method: projects.locations.nodes.list | Cloud TPU | Google Cloud,
id,url,body,title,description
114,https://cloud.google.com/tpu/docs/reference/rest/v2alpha1/projects.locations.nodes/list,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Query parameters](#body.QUERY_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [Try it!](#try-it)
Lists nodes.
HTTP request
GET https://tpu.googleapis.com/v2alpha1/{parent=projects/*/locations/*}/nodes
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
parent
|
Required. The parent resource name.
Query parameters
|Parameters
|
pageSize
|
The maximum number of items to return.
|
pageToken
|
The nextPageToken value returned from a previous List request, if any.
Request body
The request body must be empty.
Response body
Response for
.
[nodes.list](/tpu/docs/reference/rest/v2alpha1/projects.locations.nodes/list#google.cloud.tpu.v2alpha1.Tpu.ListNodes)
If successful, the response body contains data with the following structure:
|JSON representation
|
{
""nodes"": [
{
object (
|Fields
|
nodes[]
|
The listed nodes.
|
nextPageToken
|
The next page token or empty if none.
|
unreachable[]
|
Locations that could not be reached.
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).",Method: projects.locations.nodes.list | Cloud TPU | Google Cloud,
id,url,body,title,description
162,https://cloud.google.com/tpu/docs/reference/rest/v1/projects.locations.nodes/stop,"Method: projects.locations.nodes.stop
Stay organized with collections
Save and categorize content based on your preferences.
Stops a node, this operation is only available with single TPU nodes.
HTTP request
POST https://tpu.googleapis.com/v1/{name=projects/*/locations/*/nodes/*}:stop
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
string
The resource name.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[Operation](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse#Operation)
Authorization scopes
Requires one of the following OAuth scopes:
-
https://www.googleapis.com/auth/cloud.tpu
-
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Method: projects.locations.nodes.stop | Cloud TPU | Google Cloud,
id,url,body,title,description
190,https://cloud.google.com/tpu/docs/getting-support,"Getting Support
Get a Google support package
Google Cloud offers different support packages to meet different needs, such as 24/7
coverage, phone support, and access to a technical support manager. For more information,
see
[Google Cloud Support](/support).
Get support from the community
Ask a question on Stack Overflow
We support developers using Cloud TPU on
[Stack Overflow](http://stackoverflow.com/questions/tagged/google-compute-engine).
Google engineers monitor and answer questions tagged with
google-compute-engine and
google-cloud-tpu.
Please use one or both of these tags when asking questions. We aim to
answer all questions in reasonable time.
Discuss Cloud TPU and get updates
Join the
[gce-discussion](https://groups.google.com/forum/#!forum/gce-discussion)
Google group to discuss Cloud TPU and receive announcements and updates.
You can also visit the Google Cloud
[Slack community](https://googlecloud-community.slack.com/) to discuss
Cloud TPU and other Google Cloud products. If you haven't
already joined,
[use this form to sign up](https://join.slack.com/t/googlecloud-community/shared_invite/zt-ywj8ieuc-BrAaHC~qe5IgelXS9vzNRA).
For Cloud TPU, join the
[
channel.
#compute-engine](https://googlecloud-community.slack.com/messages/C0EM1FJKB/)
File bugs or feature requests
From the Cloud TPU documentation, click ""Send feedback"" near the top right of the page. This will open a feedback form. Your comments will be reviewed by the Cloud TPU team.",Getting Support | Cloud TPU | Google Cloud,
id,url,body,title,description
12,https://cloud.google.com/tpu/docs/support,"Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
[{ ""type"": ""thumb-down"", ""id"": ""hardToUnderstand"", ""label"":""Hard to understand"" },{ ""type"": ""thumb-down"", ""id"": ""incorrectInformationOrSampleCode"", ""label"":""Incorrect information or sample code"" },{ ""type"": ""thumb-down"", ""id"": ""missingTheInformationSamplesINeed"", ""label"":""Missing the information/samples I need"" },{ ""type"": ""thumb-down"", ""id"": ""otherDown"", ""label"":""Other"" }] [{ ""type"": ""thumb-up"", ""id"": ""easyToUnderstand"", ""label"":""Easy to understand"" },{ ""type"": ""thumb-up"", ""id"": ""solvedMyProblem"", ""label"":""Solved my problem"" },{ ""type"": ""thumb-up"", ""id"": ""otherUp"", ""label"":""Other"" }]",Support | Cloud TPU | Google Cloud,
id,url,body,title,description
155,https://cloud.google.com/tpu/docs/run-calculation-tensorflow,"Run a calculation on a Cloud TPU VM by using TensorFlow
This quickstart shows you how to create a Cloud TPU, install
TensorFlow and run a simple calculation on a Cloud TPU. For a more
in depth tutorial showing you how to train a model on a Cloud TPU see one of
the
[Cloud TPU Tutorials](/tpu/docs/tutorials).
Before you begin
Before you follow this quickstart, you must create a Google Cloud Platform
account, install the Google Cloud CLI. and configure the
gcloud command.
For more information, see
[Set up an account and a Cloud TPU project](/tpu/docs/setup-gcp-account).
Create a Cloud TPU VM or Node with
gcloud
Launch a Compute Engine Cloud TPU using the
gcloud
command. The command you use depends on whether you are using a TPU VM or a TPU
node. For more information on the two VM architecture, see
[System Architecture](/tpu/docs/system-architecture-tpu-vm). For more
information on the
gcloud command, see the
[. gcloud reference](/sdk/gcloud/reference)
TPU VM
$ gcloud compute tpus tpu-vm create tpu-name \
--zone=europe-west4-a \
--accelerator-type=v3-8 \
--version=tpu-vm-tf-2.15.0-pjrt
Command flag descriptions
zone
- The
[zone](/tpu/docs/types-zones)where you plan to create your Cloud TPU.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
version
- The Cloud TPU
[software version](/tpu/docs/supported-tpu-versions#tpu_software_versions).
TPU Node
$ gcloud compute tpus execution-groups create \
--name=tpu-name \
--zone=europe-west4-a \
--disk-size=300 \
--machine-type=n1-standard-16 \
--tf-version=2.12.0 \
Command flag descriptions
project
- Your Google Cloud project ID
name
- The name of the Cloud TPU to create.
zone
- The
[zone](/tpu/docs/types-zones)where you plan to create your Cloud TPU.
disk-size
- The size of the hard disk in GB of the VM created by the
gcloudcommand.
machine-type
- The
[machine type](https://cloud.google.com/compute/docs/machine-types)of the Compute Engine VM to create.
tf-version
- The version of TensorFlow
gcloudinstalls on the VM. See
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
Connect to your Cloud TPU VM
When using TPU VMs, you must explicitly SSH into your TPU VM. When using TPU Nodes, you should be automatically SSHed into your Compute EngineVM. If you are not automatically connected, use the following command.
TPU VM
$ gcloud compute tpus tpu-vm ssh tpu-name \
--zone europe-west4-a
TPU Node
$ gcloud compute ssh tpu-name \
--zone=europe-west4-a
Run a simple example using TensorFlow
TPU VM
Once you are connected to the TPU VM, set the following environment variable.
(vm)$ export TPU_NAME=local
If you set
--version=tpu-vm-tf-2.15.0-pjrt when creating your TPU, set the
following environment variables to enable the PJRT runtime:
(vm)$ export NEXT_PLUGGABLE_DEVICE_USE_C_API=true (vm)$ export TF_PLUGGABLE_DEVICE_LIBRARY_PATH=/lib/libtpu.so
Create a file named
tpu-test.pyin the current directory and copy and paste
the following script into it.
import tensorflow as tf print(""Tensorflow version "" + tf.__version__) @tf.function def add_fn(x,y): z = x + y return z cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver() tf.config.experimental_connect_to_cluster(cluster_resolver) tf.tpu.experimental.initialize_tpu_system(cluster_resolver) strategy = tf.distribute.TPUStrategy(cluster_resolver) x = tf.constant(1.) y = tf.constant(1.) z = strategy.run(add_fn, args=(x,y)) print(z)
TPU Node
Create a file named
tpu-test.pyin the current directory and copy and paste
the following script into it.
import tensorflow as tf
print(""Tensorflow version "" + tf.__version__)
tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='your-tpu-name') # TPU detection
print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])
tf.config.experimental_connect_to_cluster(tpu)
tf.tpu.experimental.initialize_tpu_system(tpu)
strategy = tf.distribute.experimental.TPUStrategy(tpu)
@tf.function
def add_fn(x,y):
z = x + y
return z
x = tf.constant(1.)
y = tf.constant(1.)
z = strategy.run(add_fn, args=(x,y))
print(z)
Run this script with the following command:
(vm)$ python3 tpu-test.py
This script performs a simple computation on a each TensorCore of a TPU. The output will look similar to the following:
PerReplica:{ 0: tf.Tensor(2.0, shape=(), dtype=float32), 1: tf.Tensor(2.0, shape=(), dtype=float32), 2: tf.Tensor(2.0, shape=(), dtype=float32), 3: tf.Tensor(2.0, shape=(), dtype=float32), 4: tf.Tensor(2.0, shape=(), dtype=float32), 5: tf.Tensor(2.0, shape=(), dtype=float32), 6: tf.Tensor(2.0, shape=(), dtype=float32), 7: tf.Tensor(2.0, shape=(), dtype=float32) }
Clean up
To avoid incurring charges to your Google Cloud account for the resources used on this page, follow these steps.
Disconnect from the Compute Engine instance, if you have not already done so:
(vm)$ exit
Your prompt should now be
username@projectname, showing you are in the Cloud Shell.
Delete your Cloud TPU.
TPU VM
$ gcloud compute tpus tpu-vm delete tpu-name \ --zone=europe-west4-a
TPU Node
$ gcloud compute tpus execution-groups delete tpu-name \ --zone=europe-west4-a
Verify the resources have been deleted by running
gcloud compute tpus tpu-vm list. The deletion might take several minutes.
TPU VM
$ gcloud compute tpus tpu-vm list --zone=europe-west4-a
TPU Node
$ gcloud compute tpus execution-groups list --zone=europe-west4-a
What's next
For more information about Cloud TPU, see:",Quickstart: Run a calculation on a Cloud TPU VM by using TensorFlow | Google Cloud,"Learn how to create a Cloud TPU, install TensorFlow and run a simple calculation on a Cloud TPU."
id,url,body,title,description
167,https://cloud.google.com/tpu/docs/reference/rest/v2alpha1/AcceleratorConfig,"AcceleratorConfig
Stay organized with collections
Save and categorize content based on your preferences.
A TPU accelerator configuration.
|JSON representation
|
{
""type"": enum (
),
""topology"": string
}
[Type](/tpu/docs/reference/rest/v2alpha1/AcceleratorConfig#Type)
|Fields
|
type
|
enum (
)
[Type](/tpu/docs/reference/rest/v2alpha1/AcceleratorConfig#Type)
Required. Type of TPU.
|
topology
|
string
Required. Topology of TPU in chips.
Type
|Enums
|
TYPE_UNSPECIFIED
|Unspecified version.
|
V2
|TPU v2.
|
V3
|TPU v3.
|
V4
|TPU v4.
|
V5P
|TPU v5.
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2024-01-12 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",AcceleratorConfig | Cloud TPU | Google Cloud,
id,url,body,title,description
181,https://cloud.google.com/tpu/docs/reference/rest/v1alpha1/projects.locations.nodes/create,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Query parameters](#body.QUERY_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [Try it!](#try-it)
Creates a node.
HTTP request
POST https://tpu.googleapis.com/v1alpha1/{parent=projects/*/locations/*}/nodes
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
parent
|
Required. The parent resource name.
Query parameters
|Parameters
|
nodeId
|
The unqualified resource name.
|
requestId
|
Idempotent request UUID.
Request body
The request body contains an instance of
.
[Node](/tpu/docs/reference/rest/v1alpha1/projects.locations.nodes#Node)
Response body
If successful, the response body contains a newly created instance of
.
[Operation](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse#Operation)
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).",Method: projects.locations.nodes.create | Cloud TPU | Google Cloud,
id,url,body,title,description
150,https://cloud.google.com/tpu/docs/tutorials/mask-rcnn-2.x,"Overview
This tutorial demonstrates how to run the
[Mask RCNN](https://arxiv.org/abs/1703.06870) model using
Cloud TPU with the [COCO](http://cocodataset.org) dataset.
Mask RCNN is a deep neural network designed to address object detection and image segmentation, one of the more difficult computer vision challenges.
The Mask RCNN model generates bounding boxes
and segmentation masks for each instance of an object in the image. The model is
based on the
[Feature Pyramid Network (FPN)](https://arxiv.org/abs/1612.03144) and a [ResNet50](/tpu/docs/tutorials/resnet) backbone.
This tutorial uses
[ to train the model. The
Keras API is a high-level TensorFlow API that can be used to
build and run a machine learning model on Cloud TPU. The API simplifies the
model development process by hiding most of the low-level implementation,
which makes it easier to switch between TPU and other platforms such as GPU or
CPU. Tensorflow Keras APIs](https://www.tensorflow.org/api_docs/python/tf/keras)
The instructions below assume you are already familiar with training a model on
Cloud TPU. If you are new to Cloud TPU, you can
refer to the
[Quickstart](/tpu/docs/quickstart) for a basic introduction.
Objectives
- Prepare the COCO dataset
- Create a Cloud Storage bucket to hold your dataset and model output
- Set up TPU resources for training and evaluation
- Run training and evaluation on a single Cloud TPU or a Cloud TPU Pod
Costs
In this document, you use the following billable components of Google Cloud:
- Compute Engine
- Cloud TPU
- Cloud Storage
To generate a cost estimate based on your projected usage,
use the
[pricing calculator](/products/calculator).
[free trial](/free-trial).
Before you begin
Before starting this tutorial, check that your Google Cloud project is correctly set up.
-
Sign in to your Google Cloud account. If you're new to
Google Cloud,
[create an account](https://console.cloud.google.com/freetrial)to evaluate how our products perform in real-world scenarios. New customers also get $300 in free credits to run, test, and deploy workloads.
-
In the Google Cloud console, on the project selector page, select or
[create a Google Cloud project](/resource-manager/docs/creating-managing-projects).
-
[Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console).
-
In the Google Cloud console, on the project selector page, select or
[create a Google Cloud project](/resource-manager/docs/creating-managing-projects).
-
[Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console).
This walkthrough uses billable components of Google Cloud. Check the
[Cloud TPU pricing page](/tpu/docs/pricing) to
estimate your costs. Be sure to
[clean up](#clean_up)
resources you create when you've finished with them to avoid unnecessary
charges.
Prepare the COCO dataset
This tutorial uses the COCO dataset. The dataset needs to be in TFRecord format on a Cloud Storage bucket to be used for the training.
If you already have the COCO dataset prepared
on a Cloud Storage bucket that is located in the
[zone](/tpu/docs/types-zones-tpu-vm) you will
be using to train the model, you can go directly to
[single device training.](#single-device-training) Otherwise,
use the following steps to prepare the dataset.
Open a Cloud Shell window.
In your
[Cloud Shell](https://console.cloud.google.com/), configure
gcloudwith your project ID.
export PROJECT_ID=project-id gcloud config set project ${PROJECT_ID}
In your
[Cloud Shell](https://console.cloud.google.com/), create a Cloud Storage bucket using the following command:
gsutil mb -p ${PROJECT_ID} -c standard -l europe-west4 gs://bucket-name
Launch a Compute Engine VM instance.
This VM instance will only be used to download and preprocess the COCO dataset. Fill in the instance-name with a name of your choosing.
$ gcloud compute tpus execution-groups create \ --vm-only \ --name=instance-name \ --zone=europe-west4-a \ --disk-size=300 \ --machine-type=n1-standard-16 \ --tf-version=2.12.0
Command flag descriptions
vm-only
- Create a VM only. By default the
gcloud compute tpus execution-groupscommand creates a VM and a Cloud TPU.
name
- The name of the Cloud TPU to create.
zone
- The
[zone](https://cloud.google.com/tpu/docs/types-zones)where you plan to create your Cloud TPU.
disk-size
- The size of the hard disk in GB of the VM created by the
gcloud compute tpus execution-groupscommand.
machine-type
- The
[machine type](https://cloud.google.com/compute/docs/machine-types)of the Compute Engine VM to create.
tf-version
- The version of Tensorflow
gcloud compute tpus execution-groupsinstalls on the VM.
-
If you are not automatically logged in to the Compute Engine instance, log in by running the following
sshcommand. When you are logged into the VM, your shell prompt changes from
username@projectnameto
username@vm-name:
$ gcloud compute ssh instance-name --zone=europe-west4-a
Set up two variables, one for the storage bucket you created earlier and one for the directory that holds the training data (DATA_DIR) on the storage bucket.
(vm)$ export STORAGE_BUCKET=gs://bucket-name
(vm)$ export DATA_DIR=${STORAGE_BUCKET}/coco
Install the packages needed to pre-process the data.
(vm)$ sudo apt-get install -y python3-tk && \ pip3 install --user Cython matplotlib opencv-python-headless pyyaml Pillow && \ pip3 install --user ""git+https://github.com/cocodataset/cocoapi#egg=pycocotools&subdirectory=PythonAPI""
Run the
download_and_preprocess_coco.shscript to convert the COCO dataset into a set of TFRecords (
*.tfrecord) that the training application expects.
(vm)$ git clone https://github.com/tensorflow/tpu.git (vm)$ sudo bash tpu/tools/datasets/download_and_preprocess_coco.sh ./data/dir/coco
This installs the required libraries and then runs the preprocessing script. It outputs a number of
*.tfrecordfiles in your local data directory. The COCO download and conversion script takes approximately 1 hour to complete.
Copy the data to your Cloud Storage bucket
After you convert the data into TFRecords, copy them from local storage to your Cloud Storage bucket using the
gsutilcommand. You must also copy the annotation files. These files help validate the model's performance.
(vm)$ gsutil -m cp ./data/dir/coco/*.tfrecord ${DATA_DIR} (vm)$ gsutil cp ./data/dir/coco/raw-data/annotations/*.json ${DATA_DIR}
Clean up the VM resources
Once the COCO dataset has been converted to TFRecords and copied to the DATA_DIR on your Cloud Storage bucket, you can delete the Compute Engine instance.
Disconnect from the Compute Engine instance:
(vm)$ exit
Your prompt should now be
username@projectname, showing you are in the Cloud Shell.
Delete your Compute Engine instance.
$ gcloud compute instances delete instance-name --zone=europe-west4-a
Cloud TPU single device training
Open a Cloud Shell window.
Create an environment variable for your project's ID.
export PROJECT_ID=project-id
Configure Google Cloud CLI to use the project where you want to create the Cloud TPU.
gcloud config set project ${PROJECT_ID}
The first time you run this command in a new Cloud Shell VM, an
Authorize Cloud Shellpage is displayed. Click
Authorizeat the bottom of the page to allow
gcloudto make Google Cloud API calls with your credentials.
Create a Service Account for the Cloud TPU project.
gcloud beta services identity create --service tpu.googleapis.com --project $PROJECT_ID
The command returns a Cloud TPU Service Account with following format:
service-PROJECT_NUMBER@cloud-tpu.iam.gserviceaccount.com
Export TPU setup variables
Export your project id, the name you want to use for your TPU resources, and the
[zone](/tpu/docs/types-zones-tpu-vm)where you will train the model and store any training-related data.
$ export TPU_NAME=mask-rcnn-tutorial $ export ZONE=europe-west4-a
Launch a Compute Engine VM and Cloud TPU using the
gcloudcommand. The command you use depends on whether you are using TPU VMs or TPU nodes. For more information on the two VM architectures, see
[System Architecture](/tpu/docs/system-architecture-tpu-vm).
TPU VM
$ gcloud compute tpus tpu-vm create mask-rcnn-tutorial \ --zone=europe-west4-a \ --accelerator-type=v3-8 \ --version=tpu-vm-tf-2.15.0-pjrt
Command flag descriptions
zone
- The
[zone](/tpu/docs/types-zones)where you plan to create your Cloud TPU.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
version
- The Cloud TPU
[software version](/tpu/docs/supported-tpu-versions#tpu_software_versions).
TPU Node
$ gcloud compute tpus execution-groups create \ --zone=europe-west4-a \ --name=mask-rcnn-tutorial \ --accelerator-type=v3-8 \ --machine-type=n1-standard-8 \ --disk-size=300 \ --tf-version=2.12.0
Command flag descriptions
zone
- The
[zone](/tpu/docs/types-zones)where you plan to create your Cloud TPU.
name
- The TPU name. If not specified, defaults to your username.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
machine-type
- The
[machine type](/compute/docs/machine-types)of the Compute Engine VM to create.
disk-size
- The root volume size of your Compute Engine VM (in GB).
tf-version
- The version of Tensorflow
gcloudinstalls on the VM.
For more information on the
gcloudcommand, see the
[gcloud Reference](/sdk/gcloud/reference).
-
If you are not automatically logged in to the Compute Engine instance, log in by running the following
sshcommand. When you are logged into the VM, your shell prompt changes from
username@projectnameto
username@vm-name:
TPU VM
gcloud compute tpus tpu-vm ssh mask-rcnn-tutorial --zone=europe-west4-a
TPU Node
gcloud compute tpus execution-groups ssh mask-rcnn-tutorial --zone=europe-west4-a
Install TensorFlow requirements.
The command you use depends on whether you are using TPU VMs or TPU Nodes.
TPU VM
(vm)$ pip3 install -r /usr/share/tpu/models/official/requirements.txt
TPU Node
(vm)$ pip3 install --user -r /usr/share/models/official/requirements.txt
Set the Cloud TPU name variable.
TPU VM
(vm)$ export TPU_NAME=local
TPU Node
(vm)$ export TPU_NAME=mask-rcnn-tutorial
Set up the following environment variables, replacing bucket-name with the name of the Cloud Storage bucket that stores the COCO dataset:
(vm)$ export STORAGE_BUCKET=gs://bucket-name
Add environment variables for the data and model directories.
(vm)$ export DATA_DIR=${STORAGE_BUCKET}/coco (vm)$ export MODEL_DIR=${STORAGE_BUCKET}/mask-rcnn
Add some additional required environment variables:
(vm)$ export RESNET_CHECKPOINT=gs://cloud-tpu-checkpoints/retinanet/resnet50-checkpoint-2018-02-07 (vm)$ export TRAIN_FILE_PATTERN=${DATA_DIR}/train-* (vm)$ export EVAL_FILE_PATTERN=${DATA_DIR}/val-* (vm)$ export VAL_JSON_FILE=${DATA_DIR}/instances_val2017.json
Set the
PYTHONPATHenvironment variable:
TPU VM
(vm)$ export PYTHONPATH=""${PYTHONPATH}:/usr/share/tpu/models""
TPU Node
(vm)$ export PYTHONPATH=""${PYTHONPATH}:/usr/share/models""
- If you set
--version=tpu-vm-tf-2.15.0-pjrtwhen creating your TPU, set the following environment variables to enable the PJRT runtime:
(vm)$ export NEXT_PLUGGABLE_DEVICE_USE_C_API=true (vm)$ export TF_PLUGGABLE_DEVICE_LIBRARY_PATH=/lib/libtpu.so
Change to directory that stores the model:
TPU VM
(vm)$ cd /usr/share/tpu/models/official/vision
TPU Node
(vm)$ cd /usr/share/models/official/legacy/detection
The following script runs a sample training that trains for 10 training steps and 10 evaluation steps. It takes approximately 6 minutes to complete on a v3-8 TPU. To train to convergence takes about 22,500 steps and approximately 6 hours on a v3-8 TPU.
Run the following command to train Mask-RCNN model:
(vm)$ python3 train.py \ --tpu=${TPU_NAME} \ --experiment=maskrcnn_resnetfpn_coco \ --mode=train_and_eval \ --config_file=configs/experiments/maskrcnn/r50fpn_640_coco_scratch_tpu4x4.yaml \ --model_dir=${MODEL_DIR} \ --params_override=""task.train_data.input_path=${TRAIN_FILE_PATTERN},task.validation_data.input_path=${EVAL_FILE_PATTERN},task.annotation_file=${VAL_JSON_FILE},runtime.distribution_strategy=tpu,trainer.train_steps=10,trainer.validation_steps=10,task.train_data.global_batch_size=8,task.validation_data.global_batch_size=8""
Command flag descriptions
strategy_type
- The distribution strategy.
tpu
- The name of your TPU.
- Specifies the directory where checkpoints and summaries are stored
during model training. If the folder is missing, the program creates
one. When using a Cloud TPU, the
model_dirmust be a Cloud Storage path (`gs://...`). You can reuse an existing folder to load current checkpoint data and to store additional checkpoints as long as the previous checkpoints were created using TPU of the same size and TensorFlow version.
model_dir
When the training completes, a message similar to the following appears:
{'frcnn_box_loss': 0.033865165, 'frcnn_cls_loss': 1.2535654, 'learning_rate': 0.008266499, 'mask_loss': 1.2039567, 'model_loss': 2.821458, 'rpn_box_loss': 0.034982488, 'rpn_score_loss': 0.2950886, 'total_loss': 4.340171, 'training_loss': 4.340171} train | step: 10 | steps/sec: 0.1 | output: {'frcnn_box_loss': 0.033865165, 'frcnn_cls_loss': 1.2535654, 'learning_rate': 0.008266499, 'mask_loss': 1.2039567, 'model_loss': 2.821458, 'rpn_box_loss': 0.034982488, 'rpn_score_loss': 0.2950886, 'total_loss': 4.340171, 'training_loss': 4.340171}
This is followed by output from the evaluation steps.
You have now completed single-device training and evaluation. Use the following steps to delete the current single-device TPU resources.
-
Disconnect from the Compute Engine instance:
(vm)$ exit
Your prompt should now be
username@projectname, showing you are in the Cloud Shell.
Delete the TPU resource.
TPU VM
$ gcloud compute tpus tpu-vm delete mask-rcnn-tutorial \ --zone=europe-west4-a
Command flag descriptions
zone
- The
[zone](https://cloud.google.com/tpu/docs/types-zones)where your Cloud TPU resided.
TPU Node
$ gcloud compute tpus execution-groups delete mask-rcnn-tutorial \ --tpu-only \ --zone=europe-west4-a
Command flag descriptions
tpu-only
- Deletes only the Cloud TPU. The VM remains available.
zone
- The
[zone](https://cloud.google.com/tpu/docs/types-zones)that contains the TPU to delete.
-
At this point, you can either conclude this tutorial and
[clean up](#cleanup),
or you can continue and explore running the model on Cloud TPU Pods.
Scaling your model with Cloud TPU Pods
Training your model on Cloud TPU Pods may require some changes
to your training script. For information, see
[Training on TPU Pods](/tpu/docs/training-on-tpu-pods).
TPU Pod training
Open a Cloud Shell window.
Create a variable for your project's ID.
export PROJECT_ID=project-id
Configure Google Cloud CLI to use the project where you want to create Cloud TPU.
gcloud config set project ${PROJECT_ID}
The first time you run this command in a new Cloud Shell VM, an
Authorize Cloud Shellpage is displayed. Click
Authorizeat the bottom of the page to allow
gcloudto make Google Cloud API calls with your credentials.
Create a Service Account for the Cloud TPU project.
Service accounts allow the Cloud TPU service to access other Google Cloud services.
gcloud beta services identity create --service tpu.googleapis.com --project $PROJECT_ID
The command returns a Cloud TPU Service Account with following format:
service-PROJECT_NUMBER@cloud-tpu.iam.gserviceaccount.com
If you previously prepared the COCO dataset and moved it to your storage bucket, you can use it again for Pod training. If you have not yet prepared the COCO dataset,
[prepare it now](#prepare-coco)and return here to set up the training.
Launch a Cloud TPU Pod
This tutorial specifies a v3-32 Pod. For other Pod options, see the
[available TPU types page](/tpu/docs/supported-tpu-configurations).
TPU VM
$ gcloud compute tpus tpu-vm create mask-rcnn-tutorial \ --zone=europe-west4-a \ --accelerator-type=v3-32 \ --version=tpu-vm-tf-2.15.0-pod-pjrt
Command flag descriptions
zone
- The
[zone](/tpu/docs/types-zones)where you plan to create your Cloud TPU.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
version
- The Cloud TPU
[software version](/tpu/docs/supported-tpu-versions#tpu_software_versions).
TPU Node
(vm)$ gcloud compute tpus execution-groups create \ --zone=europe-west4-a \ --name=mask-rcnn-tutorial \ --accelerator-type=v3-32 \ --tf-version=2.12.0
Command flag descriptions
zone
- The
[zone](/tpu/docs/types-zones)where you plan to create your Cloud TPU.
tpu-only
- Creates the Cloud TPU only. By default the
gcloud compute tpus execution-groupscommand creates both a VM and a Cloud TPU.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
tf-version
- The version of Tensorflow
gcloudinstalls on the VM.
-
If you are not automatically logged in to the Compute Engine instance, log in by running the following
sshcommand. When you are logged into the VM, your shell prompt changes from
username@projectnameto
username@vm-name:
TPU VM
gcloud compute tpus tpu-vm ssh mask-rcnn-tutorial --zone=europe-west4-a
TPU Node
gcloud compute ssh mask-rcnn-tutorial --zone=europe-west4-a
Install TensorFlow requirements.
The command you use depends on whether you are using TPU VMs or TPU Nodes.
TPU VM
(vm)$ pip3 install -r /usr/share/tpu/models/official/requirements.txt
TPU Node
(vm)$ pip3 install --user -r /usr/share/models/official/requirements.txt
The training script requires an extra package. Install it now:
(vm)$ pip3 install --user tensorflow-model-optimization>=0.1.3
Set the Cloud TPU name variable.
(vm)$ export TPU_NAME=mask-rcnn-tutorial
Set up the following environment variables, replacing bucket-name with the name of your Cloud Storage bucket:
(vm)$ export STORAGE_BUCKET=gs://bucket-name
Add some additional required environment variables:
(vm)$ export RESNET_CHECKPOINT=gs://cloud-tpu-checkpoints/retinanet/resnet50-checkpoint-2018-02-07 (vm)$ export DATA_DIR=${STORAGE_BUCKET}/coco (vm)$ export TRAIN_FILE_PATTERN=${DATA_DIR}/train-* (vm)$ export EVAL_FILE_PATTERN=${DATA_DIR}/val-* (vm)$ export VAL_JSON_FILE=${DATA_DIR}/instances_val2017.json (vm)$ export MODEL_DIR=${STORAGE_BUCKET}/mask-rcnn-pod
Set the
PYTHONPATHenvironment variable:
TPU VM
(vm)$ export PYTHONPATH=""/usr/share/tpu/models:${PYTHONPATH}"" (vm)$ export TPU_LOAD_LIBRARY=0
TPU Node
(vm)$ export PYTHONPATH=""${PYTHONPATH}:/usr/share/models""
Change to directory that stores the model:
TPU VM
(vm)$ cd /usr/share/tpu/models/official/vision
TPU Node
(vm)$ cd /usr/share/models/official/legacy/detection
Train the model:
This procedure trains the model on the COCO dataset for 10 training steps. This training takes approximately 10 minutes on a v3-32 Cloud TPU.
TPU VM
(vm)$ python3 train.py \ --tpu=${TPU_NAME} \ --experiment=maskrcnn_resnetfpn_coco \ --mode=train_and_eval \ --config_file=configs/experiments/maskrcnn/r50fpn_640_coco_scratch_tpu4x4.yaml \ --model_dir=${MODEL_DIR} \ --params_override=""task.train_data.input_path=${TRAIN_FILE_PATTERN},task.validation_data.input_path=${EVAL_FILE_PATTERN},task.annotation_file=${VAL_JSON_FILE},runtime.distribution_strategy=tpu,trainer.train_steps=10,trainer.validation_steps=10,task.train_data.global_batch_size=256,task.validation_data.global_batch_size=256""
Command flag descriptions
tpu
- The name of your TPU.
model_dir
- Specifies the directory where checkpoints and summaries are stored
during model training. If the folder is missing, the program creates
one. When using a Cloud TPU, the
model_dirmust be a Cloud Storage path (
gs://...). You can reuse an existing folder to load current checkpoint data and to store additional checkpoints as long as the previous checkpoints were created using Cloud TPU of the same size and TensorFlow version.
params_override
- A JSON string that overrides default script parameters.
TPU Node
(vm)$ python3 main.py \ --strategy_type=tpu \ --tpu=${TPU_NAME} \ --model_dir=${MODEL_DIR} \ --mode=train \ --model=mask_rcnn \ --params_override=""{train: { batch_size: 128, iterations_per_loop: 500, total_steps: 20, learning_rate: {'learning_rate_levels': [0.008, 0.0008], 'learning_rate_steps': [10000, 13000] }, checkpoint: { path: ${RESNET_CHECKPOINT}, prefix: resnet50/ }, train_file_pattern: ${TRAIN_FILE_PATTERN} }, eval: { val_json_file: ${VAL_JSON_FILE}, eval_file_pattern: ${EVAL_FILE_PATTERN}} }""
Command flag descriptions
tpu
- The name of your TPU.
model_dir
- Specifies the directory where checkpoints and summaries are stored
during model training. If the folder is missing, the program creates
one. When using a Cloud TPU, the
model_dirmust be a Cloud Storage path (
gs://...). You can reuse an existing folder to load current checkpoint data and to store additional checkpoints as long as the previous checkpoints were created using Cloud TPU of the same size and TensorFlow version.
params_override
- A JSON string that overrides default script parameters.
-
When the training completes, a message similar to the following appears:
I0706 19:47:16.108213 139955064548416 controller.py:457] train | step: 10 | steps/sec: 0.1 | output: {'frcnn_box_loss': 0.05632668, 'frcnn_cls_loss': 1.3012192, 'learning_rate': 0.008266499, 'mask_loss': 1.2371812, 'model_loss': 2.9746659, 'rpn_box_loss': 0.08227444, 'rpn_score_loss': 0.2976642, 'total_loss': 4.493513, 'training_loss': 4.493513} train | step: 10 | steps/sec: 0.1 | output: {'frcnn_box_loss': 0.05632668, 'frcnn_cls_loss': 1.3012192, 'learning_rate': 0.008266499, 'mask_loss': 1.2371812, 'model_loss': 2.9746659, 'rpn_box_loss': 0.08227444, 'rpn_score_loss': 0.2976642, 'total_loss': 4.493513, 'training_loss': 4.493513}
Clean up
To avoid incurring charges to your Google Cloud account for the resources used in this tutorial, either delete the project that contains the resources, or keep the project and delete the individual resources.
After running the training, delete the TPU VM and remove your storage bucket.
Disconnect from the Compute Engine instance, if you have not already done so:
(vm)$ exit
Your prompt should now be
username@projectname, showing you are in the Cloud Shell.
Delete your Cloud TPU and Compute Engine resources. The command you use to delete your resources depends upon whether you are using TPU VMs or TPU Nodes. For more information, see
[System Architecture](/tpu/docs/system-architecture-tpu-vm).
TPU VM
$ gcloud compute tpus tpu-vm delete mask-rcnn-tutorial \ --zone=europe-west4-a
TPU Node
$ gcloud compute tpus execution-groups delete mask-rcnn-tutorial \ --zone=europe-west4-a
Verify the resources have been deleted by running
gcloud compute tpus execution-groups list. The deletion might take several minutes. The output from the following command should not include any of the TPU resources created in this tutorial:
$ gcloud compute tpus execution-groups list --zone=europe-west4-a
Run
gsutilas shown, replacing bucket-name with the name of the Cloud Storage bucket you created for this tutorial:
$ gsutil rm -r gs://bucket-name
What's next
The TensorFlow Cloud TPU tutorials generally train the model using a
sample dataset. The results of this training are not usable
for inference. To use a model for inference, you can train the data on a
publicly available dataset or your own data set. TensorFlow models
trained on Cloud TPUs generally require datasets to be in
[TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) format.
You can use the
[dataset conversion tool sample](https://cloud.google.com/tpu/docs/classification-data-conversion) to convert an image classification dataset into TFRecord format. If you are not using an image classification model, you will have to convert your dataset to [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) format yourself. For more information, see [TFRecord and tf.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord).
Hyperparameter tuning
To improve the model's performance with your dataset, you can tune
the model's hyperparameters. You can find information about hyperparameters
common to all TPU supported models on
[GitHub](https://github.com/tensorflow/tpu/tree/master/models/hyperparameters).
Information about model-specific hyperparameters can be found in the
[source code](https://github.com/tensorflow/tpu/tree/master/models/official)
for each model. For more information on hyperparameter tuning, see
[Overview of hyperparameter tuning](https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview),
[Using the Hyperparameter tuning service](https://cloud.google.com/ai-platform/training/docs/using-hyperparameter-tuning),
and [Tune hyperparameters](https://developers.google.com/machine-learning/guides/text-classification/step-5).
Inference
Once you have trained your model you can use it for inference
(also called prediction).
[AI Platform](https://cloud.google.com/ai-platform/docs/technical-overview)
is a cloud-based solution for developing,
[training](https://cloud.google.com/ai-platform/training/docs),
and [deploying](https://cloud.google.com/ai-platform/prediction/docs/deploying-models)
machine learning models. Once a model is deployed, you can use the
[AI Platform Prediction service](https://cloud.google.com/ai-platform/prediction/docs).
- Explore the
[TPU tools in TensorBoard](/tpu/docs/cloud-tpu-tools).",Training Mask RCNN on Cloud TPU (TF 2.x) | Google Cloud,
id,url,body,title,description
49,https://cloud.google.com/tpu/docs/reference/rest/Shared.Types/GetOperationRequest,"GetOperationRequest
Stay organized with collections
Save and categorize content based on your preferences.
|JSON representation
|
{
""name"": string
}
|Fields
|
name
|
string
The name of the operation resource.
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2022-11-28 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",GetOperationRequest | Cloud TPU | Google Cloud,
id,url,body,title,description
140,https://cloud.google.com/tpu/docs/troubleshooting/trouble-tf,"Troubleshooting TensorFlow - TPU
This guide, along with the
[FAQ](https://cloud.google.com/tpu/docs/faq),
provides troubleshooting help for users who are training
TensorFlow models on Cloud TPU. If you are troubleshooting Pytorch
or JAX training, you can refer to the troubleshooting documents for those
frameworks:
For more general guides on how to use Cloud TPU, see:
[Use TPUs colab](https://www.tensorflow.org/guide/tpu) [The Cloud TPU quickstart guides](/tpu/docs/quick-starts) [MNIST tutorial](/tpu/docs/tutorials/mnist-2.x) [Train ML Models on Cloud Tensor Processing Units (TPUs)](/tpu/docs/tpus)
Overview
Common issues encountered with Cloud TPUs fall into the following categories:
Trouble connecting to the TPU server
This section describes how to troubleshoot situations where TensorFlow stops responding or prints an error when connecting to the TPU. The TPU graph compilation step can take a long time for large models, so let the script execute for at least 5 minutes before concluding that it has stopped responding.
The first step is to verify whether the issue is with the server itself, or with
your TensorFlow training pipeline. To do this, run the
[MNIST tutorial](/tpu/docs/tutorials/mnist-2.x)
using your TPU server URL and verify that it works correctly. If there are still
connection issues with the MNIST tutorial, this confirms that it is an issue
with the TPU server. In this case:
Run the following command to list the available TPUs. Replacing zone and project-id with your zone and project ID.
(vm)$ gcloud compute tpus list --zone zone --project project-id
This prints output such as:
NAME ZONE ACCELERATOR_TYPE NETWORK_ENDPOINT NETWORK RANGE STATUS demo-tpu us-central1-b v2-8 10.240.1.2:8470 default 10.240.1.0 READY
Verify that you are passing the correct value to
--tpu(
demo-tpuin the above example), and that this TPU is listed as
READY.
If your TPU is not listed as
READYor you are still having trouble connecting, manually restart the server with:
(vm)$ gcloud compute tpus stop $TPU_SERVER_NAME && gcloud compute tpus start $TPU_SERVER_NAME
In the above example
$TPU_SERVER_NAMEis
demo-tpu. This may take several minutes to complete.
Re-run the above
... tpus listcommand and wait for the TPU to be in the
READYstate. This may take several minutes.
Try to run the MNIST tutorial again.
If you are still having trouble running the MNIST tutorial, ask for help using one of the mechanisms described in
[Getting Support](/tpu/docs/getting-support).
If the MNIST example runs correctly but your model still stops responding,
then the issue is likely with your training pipeline.
To debug this, start by replacing the TPUStrategy in your code with the
default strategy. When you use the default strategy, wherever you use
strategy.scope() or
strategy.run(), the model runs
on CPU (or GPU if present) instead
of on the TPU. If the model runs on CPU and not TPU, there must be a
TPU-specific issue. If it still does not run, best practice is to debug
the issue on CPU.
Loss of
ssh connection during training
Your
ssh connection to the Cloud TPU might time out during
a long running training (particularly if you are using the Cloud Shell).
At that point, there is no output to the TPU console and it might
appear as though the TPU has stopped training. To avoid this, run the
training session with a terminal multiplexer or session management tool such
as
tmux or
screen. This will keep the
ssh
connection active regardless of the length of the training.
Debugging common errors
Cannot create a TPU
When creating a Cloud TPU, you may see the following error:
googleapiclient.errors.HttpError: < HttpError 403 when requesting https://content-tpu.googleapis.com/v1/projects/{PROJECT}/locations/{ZONE}/nodes/{TPU_NAME}?alt=json returned ""Request had insufficient authentication scopes.""
This is a permissions issue and can be resolved by running the following command:
gcloud auth login --update-adc
This command updates your Application Default Credentials (ADC) and should solve
the issue. For more information, see
[gcloud auth login](https://cloud.google.com/sdk/gcloud/reference/auth/login).
Cannot use local filesystem
Error Message
InvalidArgumentError: Unimplemented: File system scheme '[local]' not implemented
Frameworks and Configurations Affected
This message can occur when training with TensorFlow using the
[TPU Node
architecture](/tpu/docs/system-architecture-tpu-vm#tpu_nodes).
Details
All input files and the model directory must use a cloud storage bucket path
(
gs://bucket-name/...), and this bucket must be accessible from the TPU
server. Note that all data processing and model checkpointing is performed on
the TPU server, not the local machine. For information on how to properly
configure cloud storage for use with the TPU, see the guide
[Connecting to Cloud
Storage Buckets](/tpu/docs/storage-buckets).
Unsupported data type
Error Message
TypeError: DataType is not a supported TPU infeed type.
Frameworks and Configurations Affected
This message can occur when training with TensorFlow using the
[TPU Node
architecture](/tpu/docs/system-architecture-tpu-vm#tpu_nodes).
Details
Currently, only the
tf.float32,
tf.int32,
tf.bfloat16, and
tf.bool data
types are supported on the TPU. Other common data types, such as
tf.uint8,
tf.string, and
tf.int64, must be converted to one of the supported data
types during data pre-processing (that is, in the tf.data.Dataset pipeline).
See an example of the conversion in the
decode_image function used
in the
[MNIST training.](https://github.com/tensorflow/models/blob/6a55ecdea7afda51f9dc42dc17104bd6444395d9/official/vision/image_classification/mnist_main.py)
Dynamic shapes not supported
Error Message
ValueError: shape [Shape] must have a fixed size for dimension d that is known at graph construction time.
Frameworks and Configurations Affected
This message only occurs during XLA compilation with TensorFlow.
Details
To execute a model on the TPU, TensorFlow compiles the model
using the
[XLA compiler](https://www.tensorflow.org/performance/xla/). While
this compilation step significantly improves training speed and memory usage,
the shapes (dimension sizes) of all tensors in the graph
must be known at graph compilation time.
If any shapes cannot be determined at compile time, TPU compilation fails
with an error like the one above.
One common op that returns a dynamic shape is
dataset.batch(batch_size),
since the number of samples remaining in a stream might be less than
the batch size. Therefore, when training on the TPU, set
drop remainder=True for
dataset.batch.
This potentially drops the last few samples from a file to ensure
that every batch has a static shape of batch_size. For example:
dataset = tf.data.Dataset.range(8)
dataset = dataset.batch(3, drop_remainder=True)
Unavailable TensorFlow op
Error Message
NotFoundError: No registered 'OpName' OpKernel for XLA_TPU_JIT devices compatible with node
Frameworks and Configurations Affected
This message can occur when training with TensorFlow.
Details
The model uses a TensorFlow op which is not currently available on the TPU.
For a list of ops available on the TPU, along with plans for future support and
suggestions for workarounds, please see the guide to
[available TensorFlow Ops](/tpu/docs/tensorflow-ops).
Out-of-memory error message
Error Message
ResourceExhaustedError: Ran out of memory in memory space hbm; used: YYY; limit: 7.48G.
Frameworks and Configurations Affected
This message can occur when training with TensorFlow, PyTorch, or JAX.
Details
Each Cloud TPU is made of eight TPU cores, v2 TPUs have 8GB
and v3 TPUs have 16GB of RAM (or HBM, High-Bandwidth Memory).
This memory is used to store the weight
(variable) tensors, as well as intermediate result tensors needed for gradient
computation. If the model is too large to fit into TPU RAM, the initialization
fails and the above error message is printed. See the section on
[reducing
memory usage](#memory-usage) for more help.
Tips for reducing memory use:
- Check for
[excessive tensor padding](https://cloud.google.com/tpu/docs/troubleshooting/trouble-tf#memory-tensor-padding)
- Use the
[bfloat16](https://cloud.google.com/tpu/docs/bfloat16)format
- If the input sizes or model is too large, you might be able to use
TensorFlow's
[experimental model parallelism](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy)to address model size.
Problems stopping execution
If TensorFlow encounters an error during TPU execution, the script
sometimes seems to stop responding rather than exit to the shell. If this happens, press
CTRL+\ on the keyboard to trigger a
SIGQUIT, which causes
Python to exit immediately.
Similarly, pressing
CTRL+C during TPU execution does not shut down TensorFlow
immediately, but instead waits until the end of the current iteration loop to
exit cleanly.
If you encounter any new errors when re-connecting to the TPU after exiting in this manner, manually reset the TPU server with the commands:
gcloud compute tpus stop tpu-name --zone=zone gcloud compute tpus start tpu-name --zone=zone
where tpu-name is taken from the first column displayed by the
gcloud compute tpus list command and zone is the zone shown in
the second column.
Excessive tensor padding
Possible Cause of Memory Issue
Tensors in TPU memory are padded, that is, the TPU rounds up the sizes of tensors stored in memory to perform computations more efficiently. This padding happens transparently at the hardware level and does not affect results. However, in certain cases the padding can result in significantly increased memory use and execution time.
How to Reduce Memory Usage
The TPU software attempts to lay out tensors in memory to maximize computational efficiency and minimize padding. This memory layout process is complex, however, for the best results the model should obey the following rule of thumb. To minimize memory overhead and maximize computational efficiency, one of the following must be true:
The total batch size should be a multiple of 64 (8 per TPU core), and feature dimensions should be a multiple of 128,
or
The total batch size should be a multiple of 1024 (128 per TPU core), and feature dimensions should be a multiple of 8.
Using a batch size of 1024 and feature dimensions that are a multiple of 128 results in the best efficiency, although this may not be possible for all models. For clarity, ""feature dimension"" refers to the hidden size of a fully-connected layer or the number of output channels in a convolution. Not all layers can conform to this rule, especially the first and last layers of the network. This is fine, and it is expected that most models require some amount of padding.
Reducing memory usage
If you encounter an out-of-memory error when executing your model on the TPU, you must take steps to reduce the model's memory usage.
The most effective ways to reduce memory usage are to:
- Reduce excessive tensor padding
- Reduce the batch size
Batch size or model too large
Possible Cause of Memory Issue
When training a neural network on a CPU, GPU, or TPU, the memory use comes from two places:
- The memory use is proportional to the number of weights in the model.
- Storing intermediate activations from the forward pass necessary to compute the backward pass. The memory use is directly proportional to the batch size, layer sizes, and number of layers.
Therefore, the memory required by a model is largely dependent on the batch size.
TThe memory required by a model is dependent on the number of layers in the network.
The TPU runtime attempts to optimize operators to
fit the model in memory (called
[rematerialization](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/hlo_rematerialization.h),
similar to [gradient
checkpointing](https://medium.com/@yaroslavvb/fitting-larger-networks-into-memory-583e3c758ff9)),
but it is not always able to do this.
How to Reduce Memory Usage
Slowly reduce the batch size until it fits in memory, making sure that the total batch size is a multiple of 64 (the per-core batch size should be a multiple of 8). Keep in mind that larger batch sizes are more efficient on the TPU. A total batch size of 1024 (128 per core) is generally a good starting point.
If the model cannot be run on the TPU even with a small batch size (for example, 64), try reducing the number of layers or the layer sizes.
Improving training speed
If your model is able to run successfully on the TPU, but the training speed is
less than expected, this section outlines several potential ways to improve the
speed. See the
[Performance guide](https://cloud.google.com/tpu/docs/performance-guide#model_processing_performance)
for other suggestions on how to improve training performance.
Too few steps per execution per training loop
Description of Performance Issue
Passing the argument
steps_per_execution to
Model.compile controls
how many training steps are executed between host callbacks.
Each host callback requires significant communication
between the TPU server's host CPU and the TPU device, so if
steps_per_execution
is too small, it can slow down training.
How to Know if Your Model is Affected
If a TPU profile reveals frequent host CPU callbacks between TPU device steps,
then your training can benefit from a larger
steps_per_execution value.
How to Mitigate
Set
steps_per_execution to a larger value. Note that
steps_per_execution can be set to a large value, but keep in mind
logging messages and saving a checkpoint can only occur after the
specified number of steps have run.
Input processing bottleneck
Description of Performance Issue
While the TPU is training on a particular chunk of data, the input processing function prepares the next chunk of data on the CPU. If your input function takes longer than the model function, the TPU is left idle while your input function is retrieving data.
How to Know if Your Model is Affected
Follow the instructions in the
[Cloud TPU Tools: Input Pipeline
Analyzer](/tpu/docs/cloud-tpu-tools#input_pipeline_analyzer) for viewing the
input pipeline analysis in TensorBoard:
The input pipeline analysis page displays a clear summary which shows if your model is bottlenecked by input processing. The same page also shows per-op execution time, which allows you to pinpoint problematic ops.
How to Mitigate
There are several possible mitigations when loading data with the
Dataset API:
- Store your data as a collection of
tf.train.Examplestructures in
TFRecordfiles, and load them with
TFRecordDataset. See the
[Dataset API tutorial](https://www.tensorflow.org/programmers_guide/datasets)or the [ResNet tutorial](/tpu/docs/tutorials/resnet)for examples.
- Use
dataset.cache()and/or
dataset.prefetch()to buffer the input data. This prevents sporadic slowdowns in file access from creating a bottleneck.
- Specify the
num_parallel_callsparameter of the
dataset.map()function to enable multi-threaded
map()ops. A simple heuristic for the value of
num_parallel_callsis to use the number of available CPU cores.
- Perform expensive data pre-processing offline as a one time cost, rather than incurring the cost through every epoch of every training.
All input processing is performed on CPUs located on the TPU server, not on the local machine, so the speed of the local machine is not a factor.
Slow step times and low MXU utilization
Description of Performance Issue
The Cloud TPU can perform matrix multiplications and convolutions at incredibly high speeds. The majority of other TensorFlow ops do have efficient implementations on the TPU, but these are not the TPU's primary strength relative to other hardware. Therefore, a model should be dominated by matrix multiplications or convolutions to fully take advantage of the TPU.
How to Know if Your Model is Affected
The symptoms you will see in this case are slow step times coupled with
low MXU utilization shown when you
[profile the performance.](/tpu/docs/cloud-tpu-tools#op_profile)
How to Mitigate
Try to reduce the number of ops that are not matrix multiplications. After reducing the number of matrix multiplications, re-benchmark to see if performance is acceptable on TPUs.
Excessive tensor padding
Description of Performance Issue
The TPU pads tensors in memory so that the TPU can use its computational units
efficiently. The padding can increase usage of both memory and memory
bandwidth. See the section on
[tensor padding](#memory-tensor-padding) for help
understanding and fixing tensor padding issues.
Slow throughput and low memory usage
Description of Performance Issue
As a general rule, using larger batch sizes results in greater training speed on the TPU, in terms of samples/second.
How to Know if Your Model is Affected
The batch size of any model should always be at least 64 (8 per TPU core), since the TPU always pads the tensors to this size. The ideal batch size when training on the TPU is 1024 (128 per TPU core), since this eliminates inefficiencies related to memory transfer and padding.
How to Mitigate
Best practice is to use the largest batch size which fits in to memory and is a multiple of 64. The easiest way to achieve this is to start with 1024, and if this causes an out-of-memory error then try reducing the batch size until the model runs successfully. Changing the batch size of a model may require adjusting other hyperparameters to achieve the same model accuracy, such as the the learning rate, but this must be evaluated on a case-by-case basis.
Layer sizes too small
Description of Performance Issue
Even when a model is dominated by matrix multiplications or convolutions, the TPU may not run at full efficiency if the input tensors are small. When compared to other hardware, the TPU runs most efficiently when both the batch size and layer sizes are large (for example, dimension >= 512).
How to Know if Your Model is Affected
As a general rule, layer sizes smaller than 128 achieve poor efficiency on the TPU, since 128 is the native dimension of the TPU matrix multiplication unit. For fully-connected layers, a minimum hidden size of 512 is recommended in order to achieve high efficiency. Note that convolutional layers typically do not need to be as large as fully connected layers to achieve an equal efficiency level.
How to Mitigate
If the primary motivation for small layer sizes in your model is training speed, re-benchmark your models with larger layers on the TPU. For example, increasing the output size of a layer from 256 to 512 may only increase the training time by 20% even though the model is performing 2x the computations.
Op-level model profiling
It is often useful to measure op-level execution time and memory usage in order
to identify performance bottlenecks. For instructions on how to do this,
see the guide
[Cloud TPU Tools: Trace Viewer](/tpu/docs/cloud-tpu-tools#trace_viewer).
Debugging drops in model accuracy
One of the goals of the Cloud TPU ecosystem is that any model that is currently being trained on a CPU or GPU achieves a very similar accuracy when it is trained on the TPU, with perhaps minor adjustments to hyperparameters like the batch size and learning rate. Occasionally, however, users can observe a degradation in accuracy when training models on the TPU. Debugging such issues can be extremely frustrating due to the random nature of neural network training. This section provides guidance on how to pinpoint the root cause of any drops in model accuracy when porting a model to the TPU.
Understanding data sharding (data parallelism)
One of TensorFlow's primary goals is that each op should produce nearly
identical results whether it is executed on the CPU, GPU, or TPU. There are
certain exceptions to this, such as random ops. In general, if you find any
significant difference between the output of non-random ops on the TPU and CPU,
[report it as a bug](/tpu/docs/getting-support#file_bugs_or_feature_requests).
However, for the training pipeline as a whole, there is a significant
difference between training on the CPU/GPU and TPU. When training on a TPU,
TensorFlow performs
[data sharding,](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy)
Each Cloud TPU contains 8 TPU cores which operate as independent
processing units. For each step in the training, each TPU core receives a batch
of data, computes the weight gradients, exchanges the gradients with the
other TPU cores,
and then computes the weight update. By default, the loss is averaged across the
cores, but it can be summed by changing the parameter of
CrossShardOptimizer.
If the total loss of the model can be computed as the average (or sum) of independent per-sample losses, then this procedure is mathematically equivalent to training on a single large batch.
The most common op which is not independent per-sample is
[batch
normalization](https://arxiv.org/abs/1502.03167), which runs over
each per-core batch separately. For example, if the total batch size is 128,
then the per-core batch size is 16, and each of the 8 cores performs
batch normalization over its own 16 samples. In some cases, performing batch
normalization over small batches (for example, less than 32) has been found to
degradate accuracy. In the ideal scenario, total batch size should
be large (for example, 256 to 1024). If a batch size is too large to fit into
memory, the effect of sharding must be evaluated on a case-by-case basis.
Deterministic training
One reason why it is difficult to debug differences in model accuracy is that across different frameworks (TensorFlow, PyTorch, JAX), the training software uses different weight initialization and data shuffling each time a model is trained. It is beneficial to modify the training procedure to be deterministic, so that multiple runs produce nearly identical models. This section demonstrates how to run the MNIST tutorial deterministically:
- Generate an initial checkpoint file by running for a single step on the CPU. The step is used to achieve deterministic weight initialization. Also, make sure you use a fixed random seed for any random function in the model.
# Run training for 1 step to create an initial checkpoint. python mnist_tpu.py \ --use_tpu=False \ --data_dir=${STORAGE_BUCKET}/data/ \ --model_dir=${STORAGE_BUCKET}/init_output \ --random_seed=12345 \ --iterations=1 --train_steps=1
- Modify any data shuffling functions in your input function to use a random seed. This has already been done in the MNIST tutorial. This works for the input data processing ops because those always run on the CPU. Random ops in the model function may not be deterministic between the TPU and CPU. Whenever you call a random op, pass a fixed seed to ensure the same results between runs. For example:
# In the flag definitions
tf.flags.DEFINE_integer(""batch_size"", None, ""Random seed for training"")
# In the input_fn
if FLAGS.random_seed is not None:
dataset = dataset.shuffle(seed=FLAGS.random_seed)
-
Run the same model twice on the CPU to verify that the training is
deterministic. Note that the training must be run for a reasonable number of
steps (for example, 1000) but it does not need to be run to convergence.
Since the CPU training is compared to a single-core TPU training, use a batch size that can fit on a single TPU core (typically, the full batch size divided by 8). TensorFlow does not guarantee bit-for-bit determinism between runs, but the loss should be very close:
Copy the initial weights
gsutil mkdir ${STORAGE_BUCKET}/cpu_output_1 gsutil cp -f ${STORAGE_BUCKET}/init_output/* ${STORAGE_BUCKET}/cpu_output_1 gsutil mkdir ${STORAGE_BUCKET}/cpu_output_2 gsutil cp -f ${STORAGE_BUCKET}/init_output/* ${STORAGE_BUCKET}/cpu_output_2
Run 1
python mnist_tpu.py \ --use_tpu=False \ --data_dir=${STORAGE_BUCKET}/data/ \ --model_dir=${STORAGE_BUCKET}/cpu_output_1 \ --batch_size=128 \ --random_seed=12345 \ --train_steps=2000 \ --eval_steps=10
Output 1
accuracy = 0.9910644, global_step = 1000, loss = 0.025323588
Run 2
python mnist_tpu.py \ --use_tpu=False \ --data_dir=${STORAGE_BUCKET}/data/ \ --model_dir=${STORAGE_BUCKET}/cpu_output_1 \ --batch_size=128 \ --random_seed=12345 \ --train_steps=2000 \ --eval_steps=10
Output 2
accuracy = 0.9910644, global_step = 1000, loss = 0.025323414
Single-core TPU training
Once you can run the MNIST tutorial deterministically, the next step is to replicate the CPU-trained results on the TPU, using a single TPU core to pinpoint whether the issue is related to data sharding or to the TPU execution engine itself.
Here's how to execute single-core training and evaluation on the MNIST tutorial:
Use the same weight initialization as the CPU
gsutil cp -f ${STORAGE_BUCKET}/init_output/* ${STORAGE_BUCKET}/tpu_output
Run training for 1000 steps
python mnist.py \ --use_tpu=True \ --master=$GRPC_SERVER \ --train_file=${STORAGE_BUCKET}/data/train.tfrecords \ --model_dir=${STORAGE_BUCKET}/tpu_output \ --random_seed=12345 \ --num_shards=1 \ --batch_size=128 \ --train_steps=1000 \ --eval_steps=10
Output
accuracy = 0.9910644, global_step = 1000, loss = 0.02514153
The loss will not exactly match the CPU-trained model, but it should be close.
If it isn't close for your model, this might indicate that you have found
a bug in the TPU execution engine. Before
[submitting a bug report](/tpu/docs/getting-support#file_bugs_or_feature_requests),
double check the following:
You are passing
num_shards=1to
TPUConfig.
You do not have any random ops in your model function, and any random ops in your input function are being seeded correctly.
You are using the same initial checkpoint file for the CPU and TPU training.
Debugging multi-core TPU training
If your model does achieve the same loss on the CPU and single-core TPU, then the issue is likely one of the following:
(a) The degradation is due to the natural random variance when training neural models with different initializations.
(b) The degradation is due to an issue related to data sharding on the TPU.
To determine whether (a) is the issue, re-train the full model on the CPU/GPU and multi-core TPU using the same weight initialization, as above.
If you are confident that the drop in accuracy is statistically significant, then the most likely issues related to data sharding are:
- If your model uses batch normalization, a total batch size less than 256 (for example, less than 32 per core) might reduce accuracy.
- Batch-wise loss functions are affected by sharding. Such loss functions
are typically quite specialized. For example,
[Karras et al. 2017](http://research.nvidia.com/publication/2017-10_Progressive-Growing-of)uses a batch discriminator when training a generative adversarial network.
TPU VM troubleshooting
The following problems and solutions are only applicable to TPU VM configurations.
gcloud setup troubleshooting
- Problem
gcloud components updatedisplays the following error message:
ERROR: (gcloud.components.update) You cannot perform this action because the Cloud SDK component manager is disabled for this installation.
- Solution
- To use
gcloudwith TPU VM, you will need to use a
gcloudinstallation that is not managed through a package manager. Follow these steps to install
gcloudfrom source code:
sudo apt-get remove google-cloud-sdk curl -O https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-sdk-311.0.0-linux-x86_64.tar.gz tar -xzf google-cloud-sdk-311.0.0-linux-x86_64.tar.gz ./google-cloud-sdk/install.sh source ~/.bashrc
- Problem
gcloud compute tpus tpu-vm ssh ${TPU_NAME} --zone ${ZONE}command displays the following error message:
Waiting for SSH key to propagate. ssh: connect to host 34.91.136.59 port 22: Connection timed out ssh: connect to host 34.91.136.59 port 22: Connection timed out ssh: connect to host 34.91.136.59 port 22: Connection timed out ERROR: (gcloud.compute.tpus.tpu-vm.ssh) Could not SSH into the instance. It is possible that your SSH key has not propagated to the instance yet. Try running this command again. If you still cannot connect, verify that the firewall and instance are set to accept ssh traffic.
- Solution
Something may be wrong with the SSH key propagation. Try moving the automatically-generated keys to a backup location to force
gcloudto recreate them:
mv ~/.ssh/google_compute_engine ~/.ssh/old-google_compute_engine mv ~/.ssh/google_compute_engine.pub ~/.ssh/old-google_compute_engine.pub
Debug logs
The supported Cloud TPU frameworks, JAX, PyTorch, and TensorFlow
access TPUs via a shared library called
libtpu that is present on
every TPU VM. This library includes the XLA compiler used to compile
TPU programs, the TPU runtime used to run compiled programs,
and the TPU driver used by the runtime for low-level access to the TPU.
The
libtpu library logs information that can be useful for debugging.
By default, these logs are written to
/tmp/tpu_logs on each Cloud TPU VM.
The following environment variables can be set before you begin training
to modify the logging behavior:
- TPU_LOG_DIR: the directory to which logs are written
- The directory location defaults to
/tmp/tpu_logs. The directory is created if it does not already exist, but no parent directories are created. If there is an error finding or creating the specified directory, a message is printed to stderr, but it will not halt the program and logging is disabled. Set the directory name to ""disabled"" to disable logging to disk altogether.
- TPU_MIN_LOG_LEVEL: the minimum severity that will be logged to disk
- The choices are 0 (INFO), 1 (WARNING), 2 (ERROR), and 3 (FATAL). The default is 0.
- TPU_STDERR_LOG_LEVEL: the minimum severity that will be logged to stderr, in addition to disk, if applicable
- The choices are the same as TPU_MIN_LOG_LEVEL. The default is 3.
- TPU_MAX_LOG_SIZE_MB: the maximum size in megabytes of each log file
- A new log file will automatically be started when the previous one reaches roughly this size. Defaults to 1024.",Troubleshooting TensorFlow - TPU | Google Cloud,
id,url,body,title,description
151,https://cloud.google.com/tpu/docs/reference/rest/v1/projects.locations.acceleratorTypes,"REST Resource: projects.locations.acceleratorTypes
Stay organized with collections
Save and categorize content based on your preferences.
Resource: AcceleratorType
A accelerator type that a Node can be configured with.
|JSON representation
|
{
""name"": string,
""type"": string
}
|Fields
|
name
|
string
The resource name.
|
type
|
string
the accelerator type.
|
Methods
|
|
Gets AcceleratorType.
|
|
Lists accelerator types supported by this API.
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2022-11-28 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",REST Resource: projects.locations.acceleratorTypes | Cloud TPU | Google Cloud,
id,url,body,title,description
188,https://cloud.google.com/tpu/docs/tutorials/diffusion-pytorch,"This tutorial shows how to train diffusion models on TPUs using PyTorch Lightning and Pytorch XLA.
Objectives
- Create a Cloud TPU
- Install PyTorch Lightning
- Clone the diffusion repo
- Prepare the Imagenette dataset
- Run the training script
Costs
In this document, you use the following billable components of Google Cloud:
- Compute Engine
- Cloud TPU
To generate a cost estimate based on your projected usage,
use the
[pricing calculator](/products/calculator).
[free trial](/free-trial).
Before you begin
Before starting this tutorial, check that your Google Cloud project is correctly set up.
-
Sign in to your Google Cloud account. If you're new to
Google Cloud,
[create an account](https://console.cloud.google.com/freetrial)to evaluate how our products perform in real-world scenarios. New customers also get $300 in free credits to run, test, and deploy workloads.
-
In the Google Cloud console, on the project selector page, select or
[create a Google Cloud project](/resource-manager/docs/creating-managing-projects).
-
[Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console).
-
In the Google Cloud console, on the project selector page, select or
[create a Google Cloud project](/resource-manager/docs/creating-managing-projects).
-
[Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console).
This walkthrough uses billable components of Google Cloud. Check the
[Cloud TPU pricing page](/tpu/docs/pricing) to
estimate your costs. Be sure to
[clean up](#clean_up)
resources you create when you've finished with them to avoid unnecessary
charges.
Create a Cloud TPU
These instructions work on both single host and multi-host TPUs. This tutorial uses a v4-128, but it works similarly on all accelerator sizes.
Set up some environment variables to make the commands easier to use.
export ZONE=us-central2-b export PROJECT_ID=your-project-id export ACCELERATOR_TYPE=v4-128 export RUNTIME_VERSION=tpu-ubuntu2204-base export TPU_NAME=your_tpu_name
Create a Cloud TPU.
gcloud compute tpus tpu-vm create ${TPU_NAME} \ --zone=${ZONE} \ --accelerator-type=${ACCELERATOR_TYPE} \ --version=${RUNTIME_VERSION} \ --subnetwork=tpusubnet
Install required software
Install required packages along with PyTorch/XLA latest release v2.2.0.
gcloud compute tpus tpu-vm ssh ${TPU_NAME} \ --zone=us-central2-b \ --worker=all \ --command=""sudo apt-get update -y && sudo apt-get install libgl1 -y git clone https://github.com/pytorch-tpu/stable-diffusion.git cd stable-diffusion pip install -e . pip install https://github.com/Lightning-AI/lightning/archive/refs/heads/master.zip -U pip install clip pip install torch~=2.2.0 torch_xla[tpu]~=2.2.0 torchvision -f https://storage.googleapis.com/libtpu-releases/index.html""
Fix source files to be compatible with torch 2.2 and newer.
gcloud compute tpus tpu-vm ssh ${TPU_NAME} \ --zone=us-central2-b \ --worker=all \ --command=""cd ~/stable-diffusion/ sed -i \'s/from torch._six import string_classes/string_classes = (str, bytes)/g\' src/taming-transformers/taming/data/utils.py sed -i \'s/trainer_kwargs\\[\""callbacks\""\\]/# trainer_kwargs\\[\""callbacks\""\\]/g\' main_tpu.py""
Download Imagenette (a smaller version of
[Imagenet dataset](https://www.image-net.org/)) and move it to the appropriate directory.
gcloud compute tpus tpu-vm ssh ${TPU_NAME} \ --zone us-central2-b \ --worker=all \ --command=""wget -nv https://s3.amazonaws.com/fast-ai-imageclas/imagenette2.tgz tar -xf imagenette2.tgz mkdir -p ~/.cache/autoencoders/data/ILSVRC2012_train/data mkdir -p ~/.cache/autoencoders/data/ILSVRC2012_validation/data mv imagenette2/train/* ~/.cache/autoencoders/data/ILSVRC2012_train/data mv imagenette2/val/* ~/.cache/autoencoders/data/ILSVRC2012_validation/data""
Download the first stage pretrained model.
gcloud compute tpus tpu-vm ssh ${TPU_NAME} \ --zone us-central2-b \ --worker=all \ --command=""cd ~/stable-diffusion/ wget -nv -O models/first_stage_models/vq-f8/model.zip https://ommer-lab.com/files/latent-diffusion/vq-f8.zip cd models/first_stage_models/vq-f8/ unzip -o model.zip""
Train the model
Run the training with following command:
gcloud compute tpus tpu-vm ssh ${TPU_NAME} \ --zone us-central2-b \ --worker=all \ --command=""python3 stable-diffusion/main_tpu.py --train --no-test --base=stable-diffusion/configs/latent-diffusion/cin-ldm-vq-f8-ss.yaml -- data.params.batch_size=32 lightning.trainer.max_epochs=5 model.params.first_stage_config.params.ckpt_path=stable-diffusion/models/first_stage_models/vq-f8/model.ckpt lightning.trainer.enable_checkpointing=False lightning.strategy.sync_module_states=False""
Clean up
Perform a cleanup to avoid incurring unnecessary charges to your account after using the resources you created:
Use Google Cloud CLI to delete the Cloud TPU resource.
$ gcloud compute tpus delete diffusion-tutorial --zone=us-central2-b
What's next
Try the PyTorch colabs:
[Getting Started with PyTorch on Cloud TPUs](https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/getting-started.ipynb) [Training MNIST on TPUs](https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/mnist-training.ipynb) [Training ResNet18 on TPUs with Cifar10 dataset](https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/resnet18-training.ipynb) [Inference with Pretrained ResNet50 Model](https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/resnet50-inference.ipynb) [Fast Neural Style Transfer](https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/style_transfer_inference.ipynb) [MultiCore Training AlexNet on Fashion MNIST](https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/multi-core-alexnet-fashion-mnist.ipynb) [Single Core Training AlexNet on Fashion MNIST](https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/single-core-alexnet-fashion-mnist.ipynb)",Training diffusion models with PyTorch | Cloud TPU | Google Cloud,
id,url,body,title,description
94,https://cloud.google.com/tpu/docs/reference/rest/v1/projects.locations.nodes/reimage,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [Try it!](#try-it)
Reimages a node's OS.
HTTP request
POST https://tpu.googleapis.com/v1/{name=projects/*/locations/*/nodes/*}:reimage
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
The resource name.
Request body
The request body contains data with the following structure:
|JSON representation
|
{ ""tensorflowVersion"": string }
|Fields
|
tensorflowVersion
|
The version for reimage to create.
Response body
If successful, the response body contains an instance of
.
[Operation](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse#Operation)
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).",Method: projects.locations.nodes.reimage | Cloud TPU | Google Cloud,
id,url,body,title,description
185,https://cloud.google.com/tpu/docs/reference/rest/v2alpha1/projects.locations.queuedResources/delete,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Query parameters](#body.QUERY_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [Try it!](#try-it)
Deletes a QueuedResource TPU instance.
HTTP request
DELETE https://tpu.googleapis.com/v2alpha1/{name=projects/*/locations/*/queuedResources/*}
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
Required. The resource name.
Query parameters
|Parameters
|
requestId
|
Idempotent request UUID.
|
force
|
If set to true, all running nodes belonging to this queued resource will be deleted first and then the queued resource will be deleted. Otherwise (i.e. force=false), the queued resource will only be deleted if its nodes have already been deleted or the queued resource is in the ACCEPTED, FAILED, or SUSPENDED state.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[Operation](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse#Operation)
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).",Method: projects.locations.queuedResources.delete | Cloud TPU | Google Cloud,
id,url,body,title,description
105,https://cloud.google.com/tpu/docs/reference/rest/v1alpha1/projects.locations/get,"Method: projects.locations.get
Stay organized with collections
Save and categorize content based on your preferences.
Gets information about a location.
HTTP request
GET https://tpu.googleapis.com/v1alpha1/{name=projects/*/locations/*}
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
string
Resource name for the location.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[Location](/tpu/docs/reference/rest/Shared.Types/ListLocationsResponse#Location)
Authorization scopes
Requires one of the following OAuth scopes:
-
https://www.googleapis.com/auth/cloud.tpu
-
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Method: projects.locations.get | Cloud TPU | Google Cloud,
id,url,body,title,description
179,https://cloud.google.com/tpu/docs/resources,"Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
[{ ""type"": ""thumb-down"", ""id"": ""hardToUnderstand"", ""label"":""Hard to understand"" },{ ""type"": ""thumb-down"", ""id"": ""incorrectInformationOrSampleCode"", ""label"":""Incorrect information or sample code"" },{ ""type"": ""thumb-down"", ""id"": ""missingTheInformationSamplesINeed"", ""label"":""Missing the information/samples I need"" },{ ""type"": ""thumb-down"", ""id"": ""otherDown"", ""label"":""Other"" }] [{ ""type"": ""thumb-up"", ""id"": ""easyToUnderstand"", ""label"":""Easy to understand"" },{ ""type"": ""thumb-up"", ""id"": ""solvedMyProblem"", ""label"":""Solved my problem"" },{ ""type"": ""thumb-up"", ""id"": ""otherUp"", ""label"":""Other"" }]",Resources | Cloud TPU | Google Cloud,
id,url,body,title,description
134,https://cloud.google.com/tpu/docs/multislice-introduction,"Cloud TPU Multislice Overview
Cloud TPU Multislice is a full stack performance-scaling technology that enables a training job to use multiple TPU slices within a single Pod or on slices in multiple Pods with simple data parallelism. With TPU v4 chips this means training jobs can use more than 4096 chips in a single run. For training jobs that require less than 4096 chips, a single slice can offer the best performance. However, multiple smaller slices are more readily available, allowing for a faster startup time when Multislice is used with smaller slices.
When deployed in Multislice configurations, TPU chips in each slice communicate through inter-chip-interconnect (ICI). TPU chips in different slices communicate by transferring data to CPUs (hosts) which in turn transmit the data over the data-center network (DCN).
Developers don't have to write code to implement inter-slice DCN communication. The XLA compiler generates that code for you and overlaps communication with computation for maximum performance.
Concepts
- Accelerator type
- The shape of each TPU slice that comprises a Multislice. Each
slice in a multislice request is of the same accelerator type. An accelerator
type consists of a TPU type (v4 or v5e) followed by the number of
TensorCores. For example,
v4-128specifies a TPU v4 with 128 TensorCores.
- Auto-repair
- When a slice encounters a maintenance event, preemption or hardware failure, Cloud TPU will create a new slice. In the rare case when there is insufficient resources to create a new slice, the creation won't complete until hardware becomes available. After the new slice is created all other slices in the Multislice environment will be restarted so training can continue.With a properly configured startup script, the training script can automatically relaunch without user intervention, loading and resuming from the latest checkpoint.
- Dataset
- The data that is used by a model for training or inference.
- Data Center Networking (DCN)
- A higher latency, lower-throughput network (when compared with ICI) that connects TPU slices in a Multislice configuration.
- Gang scheduling
- When all TPU slices are provisioned together, at the same time, guaranteeing either all or none of the slices are provisioned successfully.
- Host
- A host is a physical computer that runs VMs. A host can run at most four VMs at one time. Each VM has a dedicated TPU.
- Inference
- Load a pre-trained machine learning model onto a host and make predictions on data.
- Interchip interconnect (ICI)
- High speed, low latency internal links that connect TPUs within a TPU Pod.
- Multislice
- Two or more TPU chip slices that can communicate over DCN.
- Node
- In the Multislice context, node refers to a single TPU slice. Each TPU slice in a Multislice is given a node ID.
- Pod
- A collection of TPU chips connected by dedicated ICI network interfaces. A Pod lets you distribute the processing load across multiple TPUs.
- Queued resource (QR)
- A representation of TPU resources, used to enqueue and manage a request for a single-slice or Multislice TPU environment.
- Startup script
- A standard
[Compute Engine startup script](/compute/docs/instances/startup-scripts/linux)that is run every time a VM is booted or rebooted. For Multislice, it is specified in the QR creation request. For more information about Cloud TPU startup scripts, see [Manage TPU resources](/tpu/docs/users-guide-tpu-vm#expandable-2).
- TPU slice
- A logical subsection of a TPU Pod consisting of TPU chips. All chips in a slice communicate with each other using the ICI network.
- TPU VM
- A virtual machine running Linux that has access to the underlying TPUs. For v4 TPUs, each TPU VM has direct access to four chips. Sometimes we call a TPU VM a worker.
- Tensor
- A data structure that is used to represent multidimensional data in a machine learning model.
- Tensor processing unit (TPU)
- Google's internally developed ML acceleration chip. They are designed to offer fast and power-efficient compute for key machine learning tasks like matrix multiplication.
- Types of Cloud TPU capacity
TPUs can be created from three types of capacity (see Usage Options in
[How TPU pricing works](/tpu/pricing)) :
- Reservation: Targets reserved quota. To use reeserved quota you must have a
reservation agreement with Google. Use the
--reservedflag when creating your resources.
- Preemptible: Targets
[preemptible](/tpu/docs/preemptible)quota. Your resources may be preempted to make room for requests for a higher priority job. Use the
--best-effortflag when creating your resources.
- On-demand: Targets on-demand quota, which doesn't need a reservation and won't be preempted. The TPU request will be enqueued to an on-demand quota queue offered by Cloud TPU, the availability of resources is not guaranteed. Selected by default, no flags needed.
- Reservation: Targets reserved quota. To use reeserved quota you must have a reservation agreement with Google. Use the
Get started
If you have not used TPUs before, start by
[installing the Google Cloud CLI](/sdk/docs/install),
and [set up your Cloud TPU environment](/tpu/docs/setup-gcp-account). To use
Multislice, your TPU resources must be managed as [Queued Resources](/tpu/docs/queued-resources).
If you are an existing TPU v4 user and have a reservation you may need to migrate your reservation to a new reservation system. For more information, contact your Google Cloud account representative.
Introductory example
This tutorial uses code from the
[MaxText GitHub repo](https://github.com/google/maxtext).
MaxText is a high performance, arbitrarily scalable, open source, and well-tested
basic LLM written in Python and Jax. MaxText was designed to train efficiently on
Cloud TPU.
The code in
[
is designed to help you get started experimenting with different parallelism
options. For example, data parallelism, fully sharded data parallelism (FSDP),
and tensor parallelism. The code scales from single slice to Multislice
environments. shardings.py](https://github.com/google/maxtext/blob/main/pedagogical_examples/shardings.py)
ICI parallelism
ICI refers to the high speed interconnect that connects the TPUs in a single
slice. ICI sharding corresponds to sharding within a slice.
shardings.py
provides three ICI parallelism parameters:
ici_data_parallelism
ici_fsdp_parallelism
ici_tensor_parallelism
The values you specify for these parameters determine the number of shards for each parallelism method.
These inputs must be constrained so that
ici_data_parallelism * ici_fsdp_parallelism * ici_tensor_parallelism is equal
to the number of chips in the slice.
The following table shows example user inputs for ICI parallelism for the four chips available in v4-8:
|ici_data_parallelism
|ici_fsdp_parallelism
|ici_tensor_parallelism
|4-way FSDP
|1
|4
|1
|4-way Tensor parallelism
|1
|1
|4
|2-way FSDP + 2-way Tensor parallelism
|1
|2
|2
Note that
ici_data_parallelism should be left as 1 in most cases because the
ICI network is fast enough to almost always prefer FSDP to data parallelism.
This example assumes you are familiar with running code on a single TPU slice
such as in
[Run a calculation on a Cloud TPU VM using JAX](/tpu/docs/run-calculation-jax).
This example show how to run
shardings.py on a single slice.
Set up the environment:
$ gcloud auth login $ gcloud config set project your-project-id $ gcloud config set compute/zone your-zone
Create SSH keys for
gcloud. We recommend leaving a blank password (press enter twice after running the following command). If you are prompted that the
google_compute_enginefile already exists, replace the existing version.
$ ssh-keygen -f ~/.ssh/google_compute_engine
Provision your TPUs with the following command:
$ gcloud alpha compute tpus queued-resources \ create your-qr-id \ --accelerator-type your-accelerator-type \ --runtime-version tpu-ubuntu2204-base \ --node-id qr-id \ [--reserved |--best-effort]
Command flag descriptions
your-qr-id
- A user-defined string that identifies the QR request.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
runtime-version
- The [Cloud TPU software version](/tpu/docs/supported-tpu-configurations#tpu_software_versions).
node-id
- The ID of the TPU resources that will be created in response to the QR request.
reserved
- Use reserved quota when creating the slices.
best-effort
- Use best-effort quota when creating the slices [Default].
The Google Cloud CLI does not support all create QR options, such as tags. For more information, see
[Create QRs](#create-queued-resources).
-
Wait until the QR is in the
ACTIVEstate which means the worker nodes are in the
READYstate. Once the QR provisioning starts, it may take one to five minutes to complete depending on the size of the QR. You can check the status of a QR request using the following command:
$ gcloud alpha compute tpus queued-resources \ list --filter=your-qr-id
A v4-8 slice has a single TPU VM. Connect to the TPU VM using SSH:
$ gcloud compute tpus tpu-vm ssh your-qr-id
Clone MaxText (which includes
shardings.py) to your TPU VM.
Within the MaxText repository directory, run the setup script to install JAX and other dependencies on your TPU slice. Setup script takes a few minutes to run.
$ bash setup.sh
Run the following command to run
shardings.pyon your TPU slice.
$ python3 pedagogical_examples/shardings.py \ --ici_fsdp_parallelism 4 \ --batch_size 131072 \ --embedding_dimension 2048
You can see the results in the logs. Your TPUs should achieve about 260 TFLOP per second or an impressive 90%+ FLOP utilization! In this case, we've selected approximately the maximum batch that fits into the TPU's High Bandwidth Memory (HBM).
Feel free to explore
[other sharding strategies](#single-slice-sharding-using-ici-parallelism)over ICI, for example you could try the following combination:
$ python3 pedagogical_examples/shardings.py \ --ici_tensor_parallelism 4 \ --batch_size 131072 \ --embedding_dimension 2048
Delete the QR and TPU slice when finished. You should run these cleanup steps from the environment where you set up the slice (first run
exitto exit the SSH session). The deletion will take two to five minutes to complete, and can be run in the background with the optional
--asyncflag.
$ gcloud alpha compute tpus queued-resources delete your-qr-id --force (--async)
Multislice sharding using DCN parallelism
The
shardings.py script takes three parameters that specify DCN parallelism,
corresponding to the number of shards of each type of data parallelism:
- dcn_data_parallelism
- dcn_fsdp_parallelism
- dcn_tensor_parallelism
The values of these parameters must be constrained so that
dcn_data_parallelism * dcn_fsdp_parallelism * dcn_tensor_parallelism equals
the number of slices.
As an example for two slices, use
--dcn_data_parallelism = 2.
|dcn_data_parallelism
|dcn_fsdp_parallelism
|dcn_tensor_parallelism
|# of slices
|2-way data parallelism
|2
|1
|1
|2
dcn_tensor_parallelism should always be set to
1 because the DCN is a poor
fit for such sharding. For typical LLM workloads on v4 chips,
dcn_fsdp_parallelism should also be set to
1 and therefore
dcn_data_parallelism should be set to the number of slices, but this is
application dependent.
As you increase the number of slices (assuming you keep the slice size and batch per slice constant), you increase the amount of data parallelism.
Running
shardings.py in a Multislice environment
You can run
shardings.py in a Multislice environment using
multihost_runner.py or by running
shardings.py on each TPU VM. Here we use
multihost_runner.py. The following steps are very similar to those
[Getting Started: Quick Experiments on Multiple slices](https://github.com/google/maxtext/blob/main/README.md#getting-started)
from the MaxText repository, except here we run
shardings.py instead of the
more complex LLM in
train.py.
The
multihost_runner.py tool is optimized for quick experiments, repeatedly
re-using the same TPUs. Because the
multihost_runner.py script depends on
long-lived SSH connections, we don't recommend it for any long-running jobs.
If you want to run a longer job (for example, hours or days), we recommend you
use
multihost_job.py.
In this tutorial, we use the term runner to indicate the machine on which you
run the
multihost_runner.py script. We use the term workers to indicate the
TPU VMs that make up your slices. You can run
multihost_runner.py on a local
machine or any Compute Engine VM in the same project as your slices. Running
multihost_runner.py on a worker is not supported.
multihost_runner.py automatically connects to TPU workers using SSH.
In this example, we run
shardings.py over two v4-16 slices, a total of four
VMs and 16 TPU chips. You can modify the example to run on more TPUs.
Set up your environment
Clone
[MaxText](https://github.com/google/maxtext/tree/main)on your runner machine.
Go to the repository directory.
Create SSH keys for
gcloud, we recommend leaving a blank password (press enter twice after running the following command). If you are prompted that the
google_compute_enginefile already exists, select not to keep your existing version.
$ ssh-keygen -f ~/.ssh/google_compute_engine
Add an environment variable to set the TPU slice count to
2.
$ export SLICE_COUNT=2
Create a Multislice environment using
queued-resources create.
The following command shows how to create a v4 Multislice TPU. To use v5e, specify a v5e
accelerator-type(for example
v5litepod-16) and the v5e
runtime-version(
v2-alpha-tpuv5-lite).
$ gcloud alpha compute tpus queued-resources
create your-qr-id
--accelerator-type=your-accelerator-type
--runtime-version=tpu-vm-runtime-version
--node-count=node-count
--node-prefix=your-qr-id
[--reserved|--best-effort]
Command flag descriptions
your-qr-id
- A user-defined string that identifies the QR request.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
runtime-version
- The
[Cloud TPU software version](/tpu/docs/supported-tpu-configurations#tpu_software_versions).
node-count
- The number of slices to create.
node-prefix
- The prefix used to generate names for each slice. A number is appended
to the prefix for each slice. For example if you set
node-prefixto
mySlice, the slices are named:
mySlice-0,
mySlice-1, and so on.
reserved
- Use reserved quota when creating the slices.
best-effort
- Use best-effort quota when creating the slices [Default].
-
When the QR provisioning starts it may take up to five minutes to complete depending on the size of the QR. Wait until the Queued Resource (QR) is in the
ACTIVEstate. You can check the status of a QR request using the following command:
$ gcloud alpha compute tpus queued-resources list \ --filter=your-qr-id
This should generate output that looks like:
NAME ZONE NODE_COUNT ACCELERATOR_TYPE STATE ... que-res-id us-central2-b 4 v4-16 ACTIVE ...
Contact your Google Cloud account representative if the QR status is in the
WAITING_FOR_RESOURCESor
PROVISIONINGstate for more than 15 minutes.
Install dependencies.
$ python3 multihost_runner.py \ --TPU_PREFIX=your-qr-id \ --COMMAND=""bash setup.sh""
Run
shardings.pyon each worker using
multihost_runner.py.
$ python3 multihost_runner.py \ --TPU_PREFIX=your-qr-id \ --COMMAND=""python3 pedagogical_examples/shardings.py \ --dcn_data_parallelism $SLICE_COUNT \ --ici_fsdp_parallelism 8 \ --batch_size 131072 \ --embedding_dimension 2048""
You'll see approximately 230 TFLOPs per second of performance in the log files.
Clean up the TPUs and QR when finished. The deletion will take two to five minutes to complete, and can be run in the background with the optional
--asyncflag.
Scaling a workload to Multislice
Before running your model in a Multislice environment, make the following code changes:
- Use
[jax.experimental.mesh_utils.create_hybrid_device_mesh](https://github.com/google/jax/blob/0affb3bb180048e7d989861cb81a0398117fbe7a/jax/experimental/mesh_utils.py#L275-L297)instead of [jax.experimental.mesh_utils.create_device_mesh](https://github.com/google/jax/blob/0affb3bb180048e7d989861cb81a0398117fbe7a/jax/experimental/mesh_utils.py#L218-L238)when creating your mesh.
These should be the only necessary code changes when moving to Multislice.
To achieve high performance, DCN needs to be mapped onto data parallel, fully
sharded data parallel or pipeline parallel axes. Performance considerations and
sharding strategies are discussed in more detail in
[Sharding With Multislice for Maximum Performance](#sharding-with-multislice-for-maximum-performance).
To validate that your code can access all the devices, you can assert that
len(jax.devices()) is equal to the number of chips in your Multislice
environment. For example, if you are using four slices of
v4-16, you have
eight chips per slice * 4 slices, so
len(jax.devices()) should return 32.
Choosing slice sizes for Multislice environments
To get a linear speed up, add new slices of the same size as your existing
slice. For example, if you use a
v4-512 slice, Multislice will
achieve approximately twice the performance by adding a second
v4-512 slice
and doubling your global batch size. For more information, see
[Sharding With Multislice for Maximum Performance](#sharding-with-multislice-for-maximum-performance).
Running your Job on multiple slices
There are three different approaches to running your custom workload in a Multislice environment:
- Using the experimentation runner script,
multihost_runner.py
- Using the production runner script,
multihost_job.py
- Using a manual approach
Experimentation runner script
The
[
script distributes code to an existing Multislice environment, and runs
your command on each host, copies your logs back, and tracks each command's error
status. The multihost_runner.py](https://github.com/google/maxtext/blob/main/multihost_runner.py)
multihost_runner.py script is documented in
[MaxText README](https://github.com/google/maxtext/blob/main/README.md).
Because
multihost_runner.py maintains persistent SSH connections, it is only
suitable for modestly sized, relatively short-running experimentation. You can
adapt the steps in the
[multihost_runner.py tutorial](#multislice-sharding-using-dcn-parallelism)
to your workload and hardware configuration.
Production runner script
For production jobs that need resiliency against hardware failures and other
preemptions, it is best to integrate directly with the Create Queued Resource
API. As a working example, we provide
[,
which triggers the Created Queued Resource API call with the appropriate startup
script to run your training and resume on preemption. The multihost_job.py](https://github.com/google/maxtext/blob/main/multihost_job.py)
multihost_job.py
script is documented in the
[MaxText README](https://github.com/google/maxtext/blob/main/README.md).
Because
multihost_job.py must provision resources for each run, it doesn't
provide as fast an iteration cycle as
multihost_runner.py.
Manual approach
We recommend you use or adapt
[multihost_runner.py](#experimentation-runner-script)
or [multihost_job.py](#production-job-script) to run your custom workload in
your Multislice configuration. However, if you prefer to provision and
manage your environment using QR commands directly, see
[Manage a Multislice Environment](#manage-a-multislice-environment).
Manage a Multislice environment
To manually provision and manage QRs without using the tools
provided in the
[MaxText repo](https://github.com/google/maxtext), read the
following sections.
Create QRs
Set the following environment variables before provisioning capacity:
$ export your-qr-id=your-queued-resource-id $ export PROJECT=your-project-name $ export ZONE=us-central2-b $ export NETWORK_NAME=your-network-name $ export SUBNETWORK_NAME=your-subnetwork-name $ export RUNTIME_VERSION=tpu-ubuntu2204-base $ export ACCELERATOR_TYPE=v4-16 $ export SLICE_COUNT=4 $ export STARTUP_SCRIPT=""#!/bin/bash\n ..."" $ gcloud config set project project-name $ gcloud config set compute/zone zone
|Input
|Description
|your-qr-id
|The user-assigned ID of the QR.
|PROJECT
|Google Cloud Project Name
|ZONE
|us-central2-b
|NETWORK_NAME
|Name of the VPC networks.
|SUBNETWORK_NAME
|Name of the subnet in VPC networks
|RUNTIME_VERSION
|tpu-ubuntu2204-base
|ACCELERATOR_TYPE
|v4-16
|EXAMPLE_TAG_1, EXAMPLE_TAG_2 
|Tags used to identify valid sources or targets for network firewalls
|SLICE_COUNT
|Number of slices. Limited to a maximum of 256 slices.
|STARTUP_SCRIPT
|If added to the create request,
Create a QR request using
gcloud
$ gcloud alpha compute tpus queued-resources \
create ${your-qr-id} \
--project your-project-id \
--zone your-zone \
--node-count ${SLICE_COUNT} \
--accelerator-type ${ACCELERATOR_TYPE} \
--runtime-version ${RUNTIME_VERSION} \
--network ${NETWORK_NAME} \
--subnetwork ${SUBNETWORK_NAME} \
--tags ${EXAMPLE_TAG_1},${EXAMPLE_TAG_2} \ --metadata=startup-script='${STARTUP_SCRIPT}'
[--reserved|--best-effort]
Command flag descriptions
your-qr-id
- A user-defined string that identifies the QR request.
project
- A user-defined string that identifies the QR request.
zone
- The Google Cloud zone in which to create the QR.
node-count
- The number of slices to create.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
runtime-version
- The
[Cloud TPU software version](/tpu/docs/supported-tpu-configurations#tpu_software_versions).
network
- The name of a VPC network to which to attach the TPU resource.
subnetwork
- The name of a VPC subnetwork to which to attach the TPU resource.
reserved
- Use reserved quota when creating the slices.
best-effort
- Use best-effort quota when creating the slices [Default].
Ensure you have the respective quota before selecting
--reserved, or
--best_effort, or the default on-demand quota. For information on quota types,
see
[Quota Policy](/tpu/docs/quota).
Create a QR request using
curl
Create a file named
queued-resource-req.json and copy the following JSON into it.
{ ""guaranteed"": { ""reserved"": true }, ""tpu"": { ""node_spec"": [ { ""parent"": ""projects/your-project-number/locations/your-zone"", ""node"": { ""accelerator_type"": ""accelerator-type"", ""runtime_version"": ""tpu-vm-runtime-version"", ""network_config"": { ""network"": ""your-network-name"", ""subnetwork"": ""your-subnetwork-name"", ""enable_external_ips"": true }, ""tags"" : [""example-tag-1""] ""metadata"": { ""startup-script"": ""your-startup-script"" } }, ""multi_node_params"": { ""node_count"": slice-count, ""node_id_prefix"": ""your-queued-resource-id"" } } ] } }
- your-project-number - Your Google Cloud project number
- your-zone - The zone in which you want to create your QR
- accelerator-type - The version and size of a single slice
- tpu-vm-runtime-version - The TPU VM runtime versions
- your-network-name - Optional, a network to which the QR will be attached
- your-subnetwork-name - Optional, a subnetwork to which the QR will be attached
- example-tag-1 - Optional, an arbitrary tag string
- your-startup-script - A startup script that will be run when the QR is allocated
- slice-count - The number of TPU slices in your Multislice environment
- your-qr-id - The user supplied ID for the QR
For more information, see the
[REST Queued Resource API](/tpu/docs/reference/rest/v2alpha1/projects.locations.queuedResources)
documentation for all available options.
To use preemptible capacity, replace:
""guaranteed"": { ""reserved"": true } with
""best_effort"": {}
Or remove the line to use the default on-demand capacity.
Submit the QR create request with the JSON payload:
$ curl -X POST -H ""Authorization: Bearer $(gcloud auth print-access-token)"" -H ""Content-Type: application/json"" -d @queuedresourcereq.json https://tpu.googleapis.com/v2alpha1/projects/your-project-id/locations/your-zone/queuedResources\?queued_resource_id\=your-qr-id
- your-project-id - Your Google Cloud project ID
- your-zone - The zone in which you want to create your QR
- your-qr-id - The user supplied ID for the QR
The response should look like the following:
{ ""name"": ""projects/<your-project-id>/locations/<your-zone>/operations/operation-<your-qr-guid>"", ""metadata"": { ""@type"": ""type.googleapis.com/google.cloud.common.OperationMetadata"", ""createTime"": ""2023-11-01T00:17:05.742546311Z"", ""target"": ""projects/<your-project-id>/locations/<your-zone>/queuedResources/<your-qa-id>"", ""verb"": ""create"", ""cancelRequested"": false, ""apiVersion"": ""v2alpha1"" }, ""done"": false }
Use GUID value at the end of the string value for the
name attribute to get
information about the QR request.
Retrieve status of a QR
To get the status of the QR request, use the following command:
$ curl -X GET -H ""Authorization: Bearer $(gcloud auth print-access-token)"" -H ""Content-Type: application/json"" https://tpu.googleapis.com/v2alpha1/projects/your-project-id/locations/your-zone/operations/operation-your-qr-guid
- your-project-id - Your Google Cloud project ID
- your-zone - The zone in which to create the QR
- your-qr-guid - The GUID following
namein the output from the QR creation request.
The response of this command contains the status of the operation:
{ ""name"": ""projects/<your-project-id>/locations/<your-zone>/operations/operation-<your-qa-guid>, ""metadata"": {...}, ""done"": true, ""response"": { ""@type"": ""type.googleapis.com/google.cloud.tpu.v2alpha1.QueuedResource"", ... ""state"": { ""state"": ""WAITING_FOR_RESOURCES"" } } }
If the QR is created successfully
(""done = true""), the state within the
response field will be either
WAITING_FOR_RESOURCES or
FAILED.
If the QR is in the
WAITING_FOR_RESOURCES state, the QR has been
enqueued and will start provisioning when there are enough resources. If the QR
is in the
FAILED state, the failure reason will be in the output. For more
information about other possible states, see the
[Queued resources user guide](/tpu/docs/queued-resources).
Once the operation is done, use the
[describe QRs](#describe-queued-resources)
to monitor the stages of the QR.
In a rare scenario, you might find your QR in the
FAILED state while some
slices are
ACTIVE. If this happens, delete the resources that were created,
and try again in a few minutes or
[reach out](https://forms.gle/pLFRKSdWZ97o2o867)
to the Cloud TPU team to resolve the issue.
SSH and install dependencies
[Run JAX code on TPU Pod slices](/tpu/docs/jax-pods)
describes how to connect to your TPU VMs using SSH in a single slice. To
connect to all TPU VMs in your Multislice environment over SSH and
install dependencies, use the following
gcloud command:
$ gcloud compute tpus queued-resources ssh ${your-qr-id} \
--zone your-zone \
--node=all \
--worker=all \
--command=""command-to-run""
--batch-size=4
This
gcloud command sends the specified command to all workers and nodes in
QR using SSH. The command is batched into groups of four and is sent
simultaneously. The next batch of commands are sent when the current batch
completes execution. If there is a failure with one of the commands, processing
stops, and no further batches are sent. For more information, see the
[Queued resource API reference](/sdk/gcloud/reference/alpha/compute/tpus/queued-resources/ssh).
If the number of slices you are using exceeds your local computer's threading
limit (also called batching limit) you will run into a deadlock. As an example,
assume the batching limit on your local machine is 64. If you try to run a
training script on more than 64 slices, say 100, the SSH command will break up
the slices into batches. It will run the training script on the first batch of 64
slices and wait for the scripts to complete before running the script on the
remaining batch of 36 slices. However, the first batch of 64 slices cannot
complete until the remaining 36 slices start running the script, causing a
deadlock.
To prevent this scenario, you can run the training script in the background on
each VM by appending an ampersand (
&) to the script command you specify
with the
--command flag. When you do this, after starting the training script
on the first batch of slices, control will immediately return to
the SSH command. The SSH command can then start running the training script on
the remaining batch of 36 slices. You'll need to pipe your
stdout and
stderr
streams appropriately when running the commands in the background. To increase
parallelism within the same QR, you can select specific slices using the
--node
parameter.
Network setup
Ensure TPU slices can communicate with each other by running the following steps.
Install JAX on each of the slices. For more information, see
[Run JAX code on TPU Pod slices](/tpu/docs/jax-pods). Assert that
len(jax.devices()) is equal to the number of chips in your Multislice
environment. To do this, on each slice, run:
$ python3 -c 'import jax; print(jax.devices())'
If you run this code on four slices of v4-16's, there are eight chips per
slice and four slices, a total of 32 chips (devices) should be returned
by
jax.devices().
List QRs
You can view the state of your QRs using the
queued-resources list command:
$ gcloud alpha compute tpus queued-resources list
NAME ZONE NODE_COUNT ACCELERATOR_TYPE STATE
...
que-res-id us-central2-b 4 v4-16 ACTIVE
...
Describe QRs
To view the detailed configuration and state of a QR, use the
describe QR API. You can call this API using
gcloud or
curl.
Using
gcloud:
$ gcloud alpha compute tpus queued-resources describe ${your-qr-id}
...state:
state: ACTIVE
...
Using
curl:
$ curl -X GET -H ""Authorization: Bearer $(gcloud auth print-access-token)"" -H ""Content-Type: application/json"" https://tpu.googleapis.com/v2alpha1/projects/your-project-id/locations/your-zone/queuedResources/${your-qr-id}
{
""name"": your-queued-res,
""tpu"": {
""nodeSpec"": [
{
... // node 1
},
{
... // node 2
},
...
]
},
...
""state"": ""ACTIVE""
}
state represents the status of a QR. For more information on the possible
states of QRs, see
[Queued resources](/tpu/docs/queued-resources).
Start your job on a provisioned environment
You can manually run workloads by connecting to all hosts in each slice over SSH and running the following command on all hosts.
$ gcloud compute tpus tpu-vm ssh your-qr-id \
--zone=your-zone \
--worker=all \
--node=all \
--command=""command-to-run""
Resetting QRs
The
ResetQueuedResource API can be used to
[reset](/compute/docs/samples/compute-reset-instance)
all the VMs in an
ACTIVE QR. Resetting the VMs forcibly erases the memory of
the machine and resets the VM to its initial state. Any data stored locally will
remain intact and the startup script will be invoked after a reset. The
ResetQueuedResource API can be useful when you want to restart all TPUs. For
example, when training is stuck and resetting all VMs is easier than debugging.
The resets of all VMs are performed in parallel and a
ResetQueuedResource
operation takes one to two minutes to complete. To invoke the API, use the following
command:
$ gcloud alpha compute tpus queued-resources reset your-qr-id
Deleting QRs
To release resources at the end of your training session, delete the queued
resource with the
--force flag. The deletion will take two to five minutes to
complete, and can be run in the background with the optional
--async flag.
$ gcloud alpha compute tpus queued-resources \
delete your-qr-id --force (--async)
Automatic failure recovery
In the event of a disruption, Multislice offers intervention-free
repair of the impacted slice and reset of all slices afterward. The impacted
slice is replaced with a new slice and the remaining otherwise healthy slices
are
[reset](/compute/docs/samples/compute-reset-instance). If no capacity is
available to allocate a replacement slice, training stops.
To resume training automatically after an interruption, you must specify a
[startup script](/compute/docs/instances/startup-scripts/linux) that checks for
and loads the last saved checkpoints. Your startup script is automatically run
every time a slice is reallocated or a VM is reset. You specify a startup
script in the JSON payload you send to the create QR request API.
The following startup script (used in
[Create QRs](#create-queued-resources))
lets you automatically recover from failures and resume training from
checkpoints stored in a Cloud Storage bucket during MaxText training:
{ ""tpu"": { ""node_spec"": [ { ... ""metadata"": { ""startup-script"": ""#! /bin/bash \n pwd \n runuser -l user1 -c 'cd /home/user1/MaxText && python3 MaxText/train.py MaxText/configs/base.yml run_name=run_test_failure_recovery dcn_data_parallelism=4 ici_fsdp_parallelism=8 steps=10000 save_period=10 base_output_directory='gs://user1-us-central2'' EOF"" } ... } ] } }
Clone the
[MaxText repo](https://github.com/google/maxtext) before trying this
out.
Profiling and debugging
Profiling is the same in single slice and Multislice environments. For
more information, see
[Profiling JAX programs](https://jax.readthedocs.io/en/latest/profiling.html).
Optimizing training
Sharding with Multislice for maximum performance
Achieving maximum performance in Multislice environments requires considering how to shard across the multiple slices. There are typically three choices (data parallelism, fully-sharded data parallelism and pipeline parallelism). We don't recommend sharding activations across the model dimensions (sometimes called tensor parallelism) because it requires too much inter-slice bandwidth. For all these strategies, you can keep the same sharding strategy within a slice that has worked for you in the past.
We recommend starting with pure data parallelism. Using fully-sharded data parallelism is useful for freeing up memory usage. The drawback is that communication between slices uses the DCN network and will slow down your workload. Use pipeline parallelism only when necessary based on batch size (as analyzed below).
When to use data parallelism
Pure data parallelism will work well in cases where you have a workload that is running well, but you'd like to improve its performance by scaling across multiple slices.
To achieve strong scaling across multiple slices, the amount of time required to perform all-reduce over DCN needs to be less than the amount of time required to perform a backwards pass. DCN is used for communication between slices and is a limiting factor in workload throughput.
Each v4 TPU chip performs at a peak of 275 * 1012 FLOPS per second.
There are four chips per TPU host and each host has a maximum network bandwidth of 50 Gbps.
That means the
[arithmetic intensity](https://crd.lbl.gov/divisions/amcr/computer-science-amcr/par/research/roofline/introduction/#:%7E:text=Arithmetic%20Intensity%20is%20the%20ratio,ndependent%20of%20the%20vector%20size./)
is 4 * 275 * 1012 FLOPS / 50 Gbps = 22000 FLOPS / bit.
Your model will use 32 to 64 bits of DCN bandwidth for each parameter per step. If you use two slices, your model will use 32 bits of DCN bandwidth. If you use more than two slices the compiler will perform a full shuffle all-reduce operation and you'll use up to 64 bits of DCN bandwidth for each parameter per step. The amount of FLOPS needed for each parameter will vary depending on your model. Specifically, for Transformer based language models, the number of FLOPS required for a forward and a backward pass are approximately 6 * B * P where:
- B is the batch size in tokens
- P is the number of parameters
The number of FLOPS per parameter is
6 * B and the number of FLOPS per parameter
during the backwards pass is
4 * B.
To ensure strong scaling across multiple slices, ensure that the operational
intensity exceeds the arithmetic intensity of the TPU hardware. To calculate the
operational intensity, divide the number of FLOPS per parameter during the
backwards pass by the network bandwidth (in bits) per parameter per step:
Operational Intensity = FLOPSbackwards_pass / DCN bandwidth
Therefore, for a Transformer based language model, if you are using two slices:
Operational intensity = 4 * B / 32
If you are using more than two slices:
Operational intensity = 4 * B/64
This suggests a minimum batch size of between 176k and 352k for Transformer based language models. Because the DCN network can briefly drop packets, it's best to maintain a significant margin for error, deploying data parallelism only if the batch size per Pod is at least 350k (two Pods) to 700k (many Pods).
For other model architectures, you'll need to estimate the runtime of your backwards pass per slice (either by timing it using a profiler or by counting FLOPS). Then you can compare that to the expected run time to all reduce over DCN and get a good estimate of if data parallelism will make sense for you.
When to use fully sharded data parallelism (FSDP)
Fully sharded data parallelism (FSDP) combines data parallelism (sharding the data across nodes) with sharding the weights across nodes. For each operation in the forward and backward passes, the weights are all-gathered so that each slice has the weights it needs. Instead of synchronizing the gradients using all-reduce, the gradients are reduce-scattered as they are produced. In this way, each slice only gets the gradients for the weights it's responsible for.
Similar to data parallelism, FSDP will require scaling the global batch size linearly with the number of slices. FSDP will decrease the memory pressure as you increase the number of slices. This is because the number of weights and optimizer state per slice decreases but it does so at the price of increased network traffic and the greater possibility for blocking due to a delayed collective.
In practice, FSDP across slices is best if you are increasing the batch per slice, storing more activations to minimize re-materialization during the backwards pass or increasing the number of parameters in your neural network.
The all-gather and all-reduce operations in FSDP work similarly to those in DP, so you can determine if your FSDP workload is limited by DCN performance in the same way as described in the previous section.
When to use pipeline parallelism
Pipeline parallelism becomes relevant when achieving high performance with other parallelism strategies that require a global batch size greater than your preferred maximum batch size. Pipeline parallelism allows the slices comprising a pipeline to ""share"" a batch. However, pipeline parallelism has two significant downsides:
- It incurs the ""pipeline bubble"" where chips are idle because they are waiting for data.
- It requires micro-batching which decreases the effective batch size, the arithmetic intensity and ultimately model FLOP utilization.
Pipeline parallelism should be used only if the other parallelism strategies require too large a global batch size. Before trying pipeline parallelism, it is worth experimenting to see empirically if convergence per sample slows down at the batch size necessary to achieve high performance FSDP. FSDP tends to achieve higher model FLOP utilization but if the convergence per sample slows as the batch size grows, pipeline parallelism may still be the better choice. Most workloads can tolerate sufficiently large batch sizes to not benefit from pipeline parallelism, but your workload may be different.
If pipeline parallelism is necessary, we recommend combining it with data parallelism or FSDP. This will enable you to minimize the pipeline depth while increasing the per pipeline batch size until DCN latency becomes less of a factor in throughput. Concretely, if you have N slices, consider pipelines of depth 2 and N/2 replicas of data parallelism, then pipelines of depth 4 and N/4 replicas of data parallelism and so on, until the batch per pipeline gets large enough that the DCN collectives can be hidden behind the arithmetic in the backwards pass. This will minimize the slowdown introduced by pipeline parallelism while allowing you to scale past the global batch size limit.
Multislice best practices
Data loading
During training we repeatedly load batches from a dataset to feed into the model. Having an efficient, async data loader which shards the batch across hosts is important to avoid starving the TPUs of work. The current data loader in MaxText has each host load an equal subset of the examples. This solution is adequate for text but requires a reshard within the model. Additionally, MaxText doesn't yet offer deterministic snapshotting which would allow the data iterator to load the same data before and after preemption.
Checkpointing
The
[Orbax](https://github.com/google/orbax) checkpointing library provides
primitives for checkpointing JAX PyTrees to local storage or Google Cloud storage.
We provide a reference integration with synchronous checkpointing into MaxText
in
checkpointing.py.
Supported configurations
Shapes
All slices must be of the same shape (for example, the same
AcceleratorType).
Heterogeneous slice shapes are not supported.
Orchestration
Orchestration is supported with GKE. For more information,
see
[TPUs in GKE](/tpu/docs/tpus-in-gke).
Frameworks
Multislice only supports JAX and PyTorch workloads.
Parallelism
We recommend users test Multislice with data parallelism. To learn more about implementing pipeline parallelism with Multislice, contact your Google Cloud account representative.
Support and Feedback
We welcome all feedback! To share feedback or request support, reach out to us
using the
[Cloud TPU Support or Feedback form](https://forms.gle/pLFRKSdWZ97o2o867).",Cloud TPU Multislice Overview | Google Cloud,
id,url,body,title,description
91,https://cloud.google.com/tpu/docs/v5p-training,"Cloud TPU v5p training
Cloud TPU v5p is Google Cloud's fifth generation Cloud TPU and the successor to the v4 TPU. v5p is optimized for large scale training and to be a leading platform for the development of foundational LLMs, diffusion models, and generative AI. At a high level, v5p provides up to 2x the performance of v4, while also packing 2x more TPUs into a Pod (6k largest slice versus 3k in v4), resulting in up to 4x performance at a Pod-level. It also runs at a higher clock frequency (1.75Ghz vs. 1.05Ghz), adds SparseCore for large scale embeddings, and triples High Bandwidth Memory (HBM) capacity.
Cloud TPU v5p concepts
If you are new to Cloud TPUs, check out the
[TPU documentation home](/tpu/docs).
General Cloud TPU concepts (for example, slices, hosts, chips,
and TensorCores), and Cloud TPU system architecture are described in
the
[Cloud TPU System Architecture](/tpu/docs/system-architecture-tpu-vm)
page.
Each Cloud TPU version requires specific accelerator types for
training. These accelerator types are described in
[accelerator type](#accelerator-types).
Chips
There are 8960 chips in a single v5p Pod. For more on TPU architecture,
see the TPU
[System Architecture](#system-architecture). However,
the largest job that can
be scheduled is a 96 cube (6144 chip) job.
Also, v5p includes four SparseCores per chip which are dataflow processors that accelerate models relying on embeddings found in recommendation models.
Multislice versus single slice
Multislice is a group of slices, extending TPU connectivity beyond the ICI connections and leveraging the data-center network (DCN) for transmitting data beyond a slice. Data within each slice is still transmitted by inter chip interconnects (ICI). Using this hybrid connectivity, Multislice enables parallelism across slices and lets you use a greater number of TPU cores for a single job than what a single slice can accommodate.
TPUs can be used to run a job either on a single slice or multiple slices.
Refer to the
[Multislice introduction](/tpu/docs/multislice-introduction)
for more details.
Queued resource
A representation of TPU resources, used to enqueue and manage a request
for a single-slice or multi-slice TPU environment. See the
[Queued Resources](/tpu/docs/queued-resources) user guide for details.
Single host versus multi host
Slices using fewer than four chips use at most one host. Slices greater than four chips use multiple hosts and can run distributed training using multiple hosts.
Slices
A slice represents a collection of chips all located inside the same
Pod connected by high-speed inter chip interconnects (ICI).
v5p has 3D slice shapes. See the table in
[Accelerator Types](/tpu/docs/supported-tpu-configurations#tpu-v5p-config) section for a list
of supported slice shapes.
Chip shape and chip topology also refer to slice shapes.
TPU VM or worker
A virtual machine running Linux that has access to the underlying TPUs. For v5p TPUs, each TPU VM has direct access to four chips. A TPU VM is also known as a worker.
System architecture
Each v5p chip contains two TensorCores and four
[SparseCores](#chips).
Each TensorCore has four Matrix
Multiply Units (MXU), a vector unit, and a scalar unit. The following
table shows the key specifications and their values for a v5p.
The following table shows the key specifications for a v5p.
|Key specifications
|v5p values
|Peak compute per chip (bf16)
|459 TFLOPs
|HBM2e capacity and bandwidth
|95GB, 2765 GBps
|TPU Pod size
|8960 chips
|Interconnect topology
|3D Torus
|Interchip Interconnect BW
|4800 Gbps
v5p TPU configurations
A TPU v5p Pod is composed of 8960 chips interconnected with reconfigurable
high-speed links. TPU v5p's flexible networking lets you connect the
chips in a same-sized slice in multiple ways. When you create a TPU
slice, you specify the TPU version and the number of TPU resources you require.
You specify its type and size in
one of two ways:
AcceleratorType and
AccleratorConfig.
The following table shows the most common single-slice shapes supported with v5p, plus most (but not all) full cube shapes greater than 1 cube. The maximum v5p shape is 16x16x24 (6144 chips, 96 cubes).
|Slice Shape
|VM Size
|# Cores
|# Chips
|# of Machines
|# of Cubes
|Supports Twisted?
|2x2x1
|Full host
|8
|4
|1
|N/A
|N/A
|2x2x2
|Full host
|16
|8
|2
|N/A
|N/A
|2x4x4
|Full host
|64
|32
|8
|N/A
|N/A
|4x4x4
|Full host
|128
|64
|16
|1
|N/A
|4x4x8
|Full host
|256
|128
|32
|2
|Yes
|4x8x8
|Full host
|512
|256
|64
|4
|Yes
|8x8x8
|Full host
|1024
|512
|128
|8
|N/A
|8x8x16
|Full host
|2048
|1024
|256
|16
|Yes
|8x16x16
|Full host
|4096
|2048
|512
|32
|Yes
|16x16x16
|Full host
|8192
|4096
|1024
|64
|N/A
|16x16x24
|Full host
|12288
|6144
|1536
|96
|N/A
Single slice training is supported for up to 6144 chips. It is extensible to 18432 chips using Multislice.
Using
AcceleratorType
When you allocate TPU resources, you use the
AcceleratorType argument to
specify the number of TensorCores in a slice.
AcceleratorType is
a formatted string
""v
$VERSION_NUMBERp
-$CORES_COUNT"".
For example,
v5p-32 specifies a v5p TPU slice with 32 TensorCores (16 chips).
To provision TPUs for a v5p training job, use one of the following accelerator types in your CLI or TPU API creation request:
- v5p-8
- v5p-16
- v5p-32
- v5p-64
- v5p-128 (one full cube/rack)
- v5p-256 (2 cubes)
- v5p-512
- v5p-1024 ... v5p-12288
Using
AcceleratorConfig
For v5p and later Cloud TPU versions,
[AcceleratorConfig](/tpu/docs/supported-tpu-configurations#using-accelerator-config)
is used in much the same way it is with Cloud TPU v4. The difference is
that instead of specifying the TPU type as
--type=v4, you specify it as
the TPU version you are using (for example,
--type=v5p for the v5p release).
Cloud TPU ICI resiliency
ICI resiliency helps improve fault tolerance of optical links and optical circuit switches (OCS) that connect TPUs between cubes. (ICI connections within a cube use copper links that are not impacted). ICI resiliency allows ICI connections to be routed around OCS and optical ICI faults. As a result, it improves the scheduling availability of TPU slices, with the trade-off of temporary degradation in ICI performance.
Similar to Cloud TPU v4 , ICI resiliency is enabled by default for v5p slices that are one cube or larger:
- v5p-128 when specifying acclerator type
- 4x4x4 when specifying accelerator config
VM, host and slice properties
|Property
|Value in a TPU
|# of v5p chips
|4
|# of vCPUs
|208 (only half is usable if using NUMA binding to avoid cross-NUMA performance penalty)
|RAM (GB)
|448 (only half is usable if using NUMA binding to avoid cross-NUMA performance penalty)
|# of NUMA Nodes
|2
|NIC Throughput (Gbps)
|200
|Cores
|Chips
|Hosts/VMs
|Cubes
|Host
|8
|4
|1
|Cube (aka rack)
|128
|64
|16
|1
|Largest supported slice
|12288
|6144
|1536
|96
|v5p full Pod
|17920
|8960
|2240
|140
Get started
Prepare your Google Cloud project
[Sign in](https://accounts.google.com/Login)to your Google Account. If you haven't already, [sign up for a new account](https://accounts.google.com/SignUp).
- In the
[Google Cloud console](https://console.cloud.google.com/), [select](/resource-manager/docs/creating-managing-projects#get_an_existing_project)or [create](/resource-manager/docs/creating-managing-projects#creating_a_project)a Cloud project from the project selector page. [Billing setup](/billing/docs)is required for all Google Cloud usage so make sure billing is enabled for your project.
- Install
[gcloud alpha components](/sdk/gcloud/reference/components/install).
Update your
gcloud alphacomponents, to ensure that relevant commands and flags are supported:
gcloud components update
Enable the TPU API through the following
gcloudcommand in the Cloud Shell. You may also enable it from the
[Google Cloud console](https://console.cloud.google.com/apis/library/tpu.googleapis.com).
gcloud services enable tpu.googleapis.com
Enable the TPU service account.
Service accounts allow the Cloud TPU service to access other Google Cloud services. A user-managed
[service account](/run/docs/securing/service-identity)is a recommended Google Cloud practice. Another option is to use the default service account generated when you create a TPU. This service account has a set of default permissions that you can view and modify at: [Google Cloud console](https://console.cloud.google.com/iam-admin/serviceaccounts).
You can either:
- grant proper permissions to the default service account, and use
it by not specifying the
--service-accountflag when creating the TPU.
- create a new user-managed service account with proper permissions,
and specify it using the service account flag (
--service-account) when creating the TPU. This option is recommended as it separates permissions from other projects and tasks.
Follow these guides to
[create](/iam/docs/service-accounts-create)and [grant roles](/iam/docs/granting-changing-revoking-access). Update the following roles as needed:
- TPU Admin (necessary)
- Storage Admin: Needed for accessing Cloud Storage (optional)
- Logs Writer: Needed for writing logs with the Logging API (optional)
- Monitoring Metric Writer: Needed for writing metrics to Cloud Monitoring (optional)
- grant proper permissions to the default service account, and use it by not specifying the
Configure the project and zone.
Your project ID is the name of your project
[shown on the Cloud console](/resource-manager/docs/creating-managing-projects#identifying_projects).
export PROJECT_ID=your_project_ID export ZONE=us-east5-a gcloud auth login gcloud config set project ${PROJECT_ID} gcloud config set compute/zone ${ZONE}
Secure capacity
The v5p Public Preview is invite-only. Contact
[Cloud Sales](https://cloud.google.com/contact) to share your project ID
and secure capacity or, if you have
questions around your capacity limits. You can also contact your account
team to handle the capacity request for you.
Provision the Cloud TPU environment
The best practice is to provision Cloud TPU v5ps as
[Queued Resources](/tpu/docs/queued-resources) using the
queued-resource create command. However, you can
also use the Create Node API (
gcloud alpha compute tpus tpu-vm create) to
provision Cloud TPU v5ps.
Create environment variables
Set the necessary environment variables for TPU creation.
Replace the variables (in red) in the following list with values you will use for your training job.
export PROJECT_ID=your_project_ID export ACCELERATOR_TYPE=v5p-32 export ZONE=us-east5-a export RUNTIME_VERSION=v2-alpha-tpuv5 export SERVICE_ACCOUNT=your_service_account export TPU_NAME=your_tpu_name export QUEUED_RESOURCE_ID=your_queued_resource_id export QUOTA_TYPE=quota_type export VALID_UNTIL_DURATION=1d
If you will be using Multislice, see the
[Multislice Introduction](/tpu/docs/multislice-introduction#set_up_your_environment) for more information.
When using Multislice, specify the following additional variables:
export NODE_COUNT=node_count
export NODE_PREFIX=your_tpu_prefix # Optional
|Variable
|Description
|PROJECT_ID
|Google Cloud Project Name
|ACCELERATOR_TYPE
|See the
[Queued Resources](/tpu/docs/queued-resources)document for information about queued resources.
reserved
or
best-effort. Use
best-effort with queued resource requests for preemptible
TPUs.
See
[TPU Quota types](/tpu/docs/quota#quota_types)for information on the different types of quotas supported by Cloud TPU. on-demand quota is the default, so you don't need to set that value. [Queued resources](/tpu/docs/queued-resources)for information about the different valid durations.
[Create a TPU resource](/tpu/docs/queued-resources) (single slice)
gcloud alpha compute tpus queued-resources create ${QUEUED_RESOURCE_ID} \ --node-id ${TPU_NAME} \ --project ${PROJECT_ID} \ --zone ${ZONE} \ --accelerator-type ${ACCELERATOR_TYPE} \ --runtime-version ${RUNTIME_VERSION} \ --valid-until-duration ${VALID_UNTIL_DURATION} \ --service-account ${SERVICE_ACCOUNT} \ --${QUOTA_TYPE}
When using Multislice, specify the following additional variables:
--node-count${NODE_COUNT}
--node-prefix${NODE_PREFIX} # optional, the default is
QUEUED_RESOURCE_ID
If the queued resource is created successfully, the state within
the
response field will be
WAITING_FOR_RESOURCES.
If the queued resource is in the
WAITING_FOR_RESOURCES state, that means that the
queued resource has passed preliminary validation and is awaiting capacity.
Once capacity is available the request will transition to
PROVISIONING.
Being in
WAITING_FOR_RESOURCES state does not guarantee the resources you
request will be allocated to you. In addition, it may take some time for
your request's status to transition to the
ACTIVE status.
If the queued resource is in the
FAILED
state, the failure reason will be displayed in the queued-resources
create response.
If your request is not filled within the duration specified within the
--valid-until-duration flag, the request will transition to the
FAILED state.
You will be able to access your TPU VM using SSH once your
queued resource is in the
ACTIVE state.
Use the
command to query the status of your queued resource.
[describe](/tpu/docs/queued-resources#retrieve_state_and_diagnostic_information_about_a_queued_resource_request)
gcloud alpha compute tpus queued-resources describe ${QUEUED_RESOURCE_ID} \ --project ${PROJECT_ID} \ --zone ${ZONE} \
The state represents the status of a queued resource. The states are defined as:
|State
|Description
|
WAITING_FOR_RESOURCES
|The queued resource
create command has been received and will
begin provisioning, as soon as capacity is available.
|
PROVISIONING
|The TPU slices are being provisioned.
|
ACTIVE
|All TPUs are provisioned, and are ready to use. If a startup
script is given, it will start executing on all TPUs, when the
queued resource state transitions to
ACTIVE.
|
FAILED
|The slices couldn't be provisioned.
|
SUSPENDING
|One or more slices are being deleted.
|
SUSPENDED
|All underlying slices are deleted but the queued resource request stays intact, until explicitly deleted. A suspended queued resource cannot be resumed and should be deleted.
|
DELETING
|The queued resource is being deleted.
Connect to the TPU VM using SSH
The following section describes how you can install binaries on each TPU VM in your TPU slice and run code. In this context, a TPU VM is also known as a worker.
See the
[Host and slice properties](#host-slice-properties) section
to determine how many VMs your slice will have.
To install the binaries or run code, you connect to the TPU VM using
the
.
[tpu-vm ssh command](/sdk/gcloud/reference/alpha/compute/tpus/tpu-vm/ssh)
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME}
To access a specific TPU VM with SSH, use the
--worker flag which follows
a 0-based index:
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} --worker=1
If you have slice shapes greater than 4 chips, you will have
multiple VMs in one slice. In this case use the
--worker=all
and
--command flags to run commands on all TPU VMs with one command without
having to use SSH to connect to each one separately. For example:
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} --project ${PROJECT_ID} \ --zone ${ZONE} \ --worker=all \ --command='pip install ""jax[tpu]==0.4.20"" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html'
For Multislice, you can either run a command on a single VM
(by using the enumerated tpu-name) or use the
--node=all,
--worker=all,
and
--command flags
to run the command on all TPU VMs of all slices of the Multislice,
with an optional
[ field. --batch-size](https://cloud.google.com/sdk/gcloud/reference/alpha/compute/tpus/queued-resources/ssh#--batch-size)
gcloud compute tpus queued-resources ssh ${QUEUED_RESOURCE_ID} \ --project ${PROJECT_ID} --zone ${ZONE} --node=all --worker=all \ --command='pip install ""jax[tpu]==0.4.20"" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html' --batch-size=4
Manage
All commands you can use to manage your TPU VMs are described in
[Managing TPUs](/tpu/docs/managing-tpus-tpu-vm) or
[Queued resources user guide](/tpu/docs/queued-resources) for managing
queued resources.
Framework Setup
This section describes the general setup process for model training using JAX or PyTorch with TPU v5p.
Setup for JAX
If you have slice shapes greater than 4 chips, you will have multiple VMs
in one slice. In this case, you need to use the
--worker=all flag
to run the installation on all TPU VMs using a single command:
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} --project ${PROJECT_ID} \ --zone ${ZONE} \ --worker=all \ --command='pip install ""jax[tpu]==0.4.20"" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html'
You can run the following command to check number of devices (the outputs shown here were produced with a v5p-32 slice). This code tests that everything is installed correctly by checking that JAX sees the Cloud TPU TensorCores and can run basic operations:
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \ --project ${PROJECT_ID} \ --zone ${ZONE} \ --worker=all \ --command='python3 -c ""import jax; print(jax.device_count()); print(jax.local_device_count())""'
The output will be similar to the following:
SSH: Attempting to connect to worker 0... SSH: Attempting to connect to worker 1... SSH: Attempting to connect to worker 2... SSH: Attempting to connect to worker 3... 16 4 16 4 16 4 16 4
jax.device_count() shows the total number of chips in
the given slice.
jax.local_device_count() indicates the
count of chips accessible by a single VM in this slice.
# Check the number of chips in the given slice by summing the count of chips # from all VMs through the # jax.local_device_count() API call. gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \ --project ${PROJECT_ID} \ --zone ${ZONE} \ --worker=all \ --command='python3 -c ""import jax; xs=jax.numpy.ones(jax.local_device_count()); print(jax.pmap(lambda x: jax.lax.psum(x, \""i\""), axis_name=\""i\"")(xs))""'
The output will be similar to the following:
SSH: Attempting to connect to worker 0... SSH: Attempting to connect to worker 1... SSH: Attempting to connect to worker 2... SSH: Attempting to connect to worker 3... [16. 16. 16. 16.] [16. 16. 16. 16.] [16. 16. 16. 16.] [16. 16. 16. 16.]
Use
--node=all to run the command on all Multislice workers.
$ gcloud compute tpus queued-resources ssh ${QUEUED_RESOURCE_ID} \ --project ${PROJECT_ID} --zone ${ZONE} --node=all --worker=all \ --command='python3 -c ""import jax; print(jax.device_count()); print(jax.local_device_count())""'
Try the
[JAX tutorials](#jax-tutorials) in this document to get
started with v5p training using JAX.
Setup for PyTorch
The
[PJRT runtime](https://github.com/pytorch/xla/blob/master/docs/pjrt.md)
is the only supported runtime for v5p, and PyTorch 2.1+ uses
PJRT as the default runtime for all TPU versions. This section describes
how to start using PJRT on v5p Pods with PyTorch/XLA nightlies for all workers.
Install dependencies
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \ --project ${PROJECT_ID} \ --zone ${ZONE} \ --worker=all \ --command=' sudo apt-get update sudo apt-get install libopenblas-dev -y pip3 install numpy pip3 install --pre torch==2.2.0.dev20231026+cpu --index-url https://download.pytorch.org/whl/nightly/cpu pip3 install ""torch_xla[tpuvm] @https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch_xla-nightly+20231026-cp310-cp310-linux_x86_64.whl"" '
Use a Python script with PJRT to do a validation of the installation to show available TPU devices (the outputs shown here were produced with a v5p-32 slice).
$ gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \ --project ${PROJECT_ID} --zone ${ZONE} --worker=all \ --command=' PJRT_DEVICE=TPU python3 -c ""import torch_xla.core.xla_model as xm; print(xm.get_xla_supported_devices(\""TPU\""))"" '
SSH: Attempting to connect to worker 0... SSH: Attempting to connect to worker 1... SSH: Attempting to connect to worker 2... SSH: Attempting to connect to worker 3... ['xla:0', 'xla:1', 'xla:2', 'xla:3'] ['xla:0', 'xla:1', 'xla:2', 'xla:3'] ['xla:0', 'xla:1', 'xla:2', 'xla:3'] ['xla:0', 'xla:1', 'xla:2', 'xla:3']
Try the
[PyTorch tutorials](#pytorch-xla) in this document to get
started with v5p training using PyTorch.
Clean up
Delete your TPU and queued resource request at the end of your session or to remove queued resource requests that are in the ""FAILED"" state. To delete a queued resource, delete the slice(s) and then the queued resource request in 2 steps:
gcloud alpha compute tpus tpu-vm delete ${TPU_NAME} --project=${PROJECT_ID} --zone=${ZONE} --quiet
gcloud alpha compute tpus queued-resources delete ${QUEUED_RESOURCE_ID} --project ${PROJECT_ID} --zone ${ZONE} --quiet
Or, use
--force to delete the slice(s) and the queued resource request
in a single step:
# With --force $ gcloud alpha compute tpus queued-resources delete ${QUEUED_RESOURCE_ID} --project ${PROJECT_ID} --zone ${ZONE} --quiet --force
Monitor and profile
Cloud TPU v5p supports monitoring and profiling using the
same methods as previous generations of Cloud TPU. You can
read
[Profile your model with Cloud TPU tools](/tpu/docs/profile-tpu-vm)
to learn more about profiling and [Monitoring Cloud TPU VMs](/tpu/docs/troubleshooting/tpu-vm-monitoring)
to learn more about monitoring.
Training tutorials
This section focuses on training tutorials for a single slice.
Adapting these tutorials to Multislice training can be
achieved by adding the
--node=all flag to SSH commands.
For details and best practices, refer to the
[Multislice introduction.](/tpu/docs/multislice-introduction#set_up_your_environment)
JAX tutorials
Train Diffusion 2.1
This tutorial shows you how to train the Stable Diffusion model from
HuggingFace using the
[Pokmon](https://huggingface.co/datasets/lambdalabs/pokemon-blip-captions)
dataset on Cloud TPU v5p.
The Stable Diffusion model is a latent text-to-image model that generates photo-realistic images from any text input. For more information, see the following resources:
Set up
Create environment variables:
export PROJECT_ID=your_project_ID export ACCELERATOR_TYPE=v5p-32 export ZONE=us-east5-a export RUNTIME_VERSION=v2-alpha-tpuv5 export SERVICE_ACCOUNT=your_service_account export TPU_NAME=your_tpu_name export QUEUED_RESOURCE_ID=queued_resource_id export QUOTA_TYPE=quota_type export VALID_UNTIL_DURATION=1d
gcloud alpha compute tpus queued-resources create ${QUEUED_RESOURCE_ID} \ --node-id ${TPU_NAME} \ --project ${PROJECT_ID} \ --zone ${ZONE} \ --accelerator-type ${ACCELERATOR_TYPE} \ --runtime-version ${RUNTIME_VERSION} \ --valid-until-duration ${VALID_UNTIL_DURATION} \ --service-account ${SERVICE_ACCOUNT} \ --${QUOTA_TYPE}
gcloud alpha compute tpus queued-resources describe ${QUEUED_RESOURCE_ID} \ --project ${PROJECT_ID} --zone ${ZONE}
When the queued resource is in the
ACTIVEstate, the output will be similar to the following:
state: ACTIVE
Install JAX and its dependencies.
# compatible with v5p: only jax version 0.4.19 and later \ # jax 0.4.19 requires py 3.10 \ gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \ --project=${PROJECT_ID} --zone=${ZONE} --worker=all \ --command='pip install ""jax[tpu]==0.4.20"" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html'
Download HuggingFace
[repo](https://github.com/huggingface/diffusers)and install requirements.
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \ --project=${PROJECT_ID} \ --zone=${ZONE} \ --worker=all \ --command='git clone https://github.com/huggingface/diffusers.git && cd diffusers && pip install . && pip install tensorflow clu && pip install -U -r examples/text_to_image/requirements_flax.txt'
Train the model
Train the model with a pre-mapped buffer at 4GB.
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} --project=${PROJECT_ID} \ --zone=${ZONE} \ --worker=all \ --command='export PATH=$PATH:$HOME/.local/bin && cd diffusers/examples/text_to_image && JAX_PLATFORMS=tpu,cpu python3 train_text_to_image_flax.py --pretrained_model_name_or_path=stabilityai/stable-diffusion-2-1 --dataset_name=lambdalabs/pokemon-blip-captions --resolution=256 --center_crop --random_flip --train_batch_size=1 --mixed_precision=bf16 --max_train_steps=150 --learning_rate=1e-05 --max_grad_norm=1 --output_dir=sd-pokemon-model --from_pt'
Clean up
For instructions on how to delete your TPU and queued resource, see
[Clean up.](#clean-up)
Benchmark results
The Stable Diffusion training script ran on v5p-8, v5p-32, and v5p-128. The following table shows the throughput.
|
v5p-8
|
v5p-32
|
v5p-128
|
Train Step
|
150
|
150
|
150
|
Global batch size
|
32
|
64
|
64
|
Throughput (examples/sec)
|
12.10
|
18.08
|
19.10
MaxText
This tutorial shows you how to train the
[MaxText](https://github.com/google/maxtext) model using a synthetic
dataset on Cloud TPU.
MaxText is a high performance, arbitrarily scalable, open source, well-tested LLM written in pure Python/JAX targeting Cloud TPUs. MaxText empowers researchers and developers with an accessible and adaptable tool for advancing the frontiers of Natural Language Processing (NLP) research and development.
Set up environment variables
export PROJECT_ID=your_project_ID export ZONE=us-east5-a export ACCELERATOR_TYPE=v5p-256 export RUNTIME_VERSION=v2-alpha-tpuv5 export RUN_NAME=your_experiment_run_name export GCS_BUCKET_NAME=your_bucket_name # Should start with gs:// export MAXTEXT_OUTPUT_PATH=${GCS_BUCKET_NAME}/your_experiment_output_path export NUM_SLICES=1 # Update the value to a number >1 for Multislice.
Optional variables recommended for Multislice:
export NETWORK_NAME=your_network_name export FIREWALL_RULE_NAME=your_firewall_rule_name
Clone the MaxText repository
git clone https://github.com/google/maxtext.git
Optional If you're running Multislice workloads and want optimal network performance, consider creating a dedicated network with a Maximum Transmission Unit (MTU) of 8896 bytes and configuring appropriate firewall rules. While optional, this step can significantly improve performance, especially when scaling up the number of slices over the data-center network (DCN). Note creating a network requires
compute.networks.createpermission in the project. The following examples show how to create a dedicated network and firewall rule.
gcloud compute networks create ${NETWORK_NAME} \ --mtu=8896 \ --project=${PROJECT_ID} \ --subnet-mode=auto \ --bgp-routing-mode=regional gcloud compute firewall-rules create ${FIREWALL_RULE_NAME} --network ${NETWORK_NAME} --allow tcp,icmp,udp --project=${PROJECT_ID}
Train the model
The following sections describe two options for training MaxText.
Option 1
If you want a script to manage the entire workflow, from provisioning Cloud TPUs and installing dependencies to running your model and tearing down resources, you can use
multihost_job.py.
cd maxtext && python3 multihost_job.py --PROJECT=${PROJECT_ID} --ZONE=${ZONE} \ --NUM_SLICES=${NUM_SLICES} --TPU_TYPE=${ACCELERATOR_TYPE} \ --VERSION=${RUNTIME_VERSION} --RUN_NAME=${RUN_NAME} \ --BUCKET_NAME=${GCS_BUCKET_NAME} \ #used to store logs and configs --COMMAND=""bash setup.sh && bash MaxText/configs/experimental/64b.sh RUN_NAME=${RUN_NAME} OUTPUT_PATH=${MAXTEXT_OUTPUT_PATH} PLATFORM=gce""
After initiating the script, you should see a message similar to the following in the log. The log location is referenced in the output message. Click the first link to access the logs of all workers once TPU provisioning is complete.
------------------------------------ multihost_job finished running, TPUs are starting up to run your job remotely. Logs for your job are displayed here: https://console.cloud.google.com/logs/query;query=resource.type%3D%22gce_instance%22%20AND%0Alog_id%2528%22
_log%22%2529;?project=PROJECT_ID To see the output of a single host, you may edit the slice and worker number in the `log_file_path` property here: https://console.cloud.google.com/logs/query;query=resource.type%3D%22gce_instance%22%20AND%0Alog_id%2528%22RUN_NAME_log%22%2529%20AND%0Alabels.%22agent.googleapis.com%2Flog_file_path%22%3D%20%22%2FRUN_NAME%2Fmain_command_log_slice_0_worker_0%22;?project=PROJECT_ID When your job is finished, the main command log is in your Cloud Storage bucket: https://console.cloud.google.com/storage/browser/YOUR_BUCKET_NAME/RUN_NAME?project=PROJECT_ID View the status of the created TPUs using: gcloud alpha compute tpus queued-resources list --filter=RUN_NAME --zone=ZONE --project=PROJECT_ID
Option 2
To run the training script multiple times on a provisioned
Cloud TPU, use
the
multihost_runner.py script to use the resource.
Set up variables to create a TPU.
export SERVICE_ACCOUNT=your_service_account export TPU_NAME=your_tpu_name export QUEUED_RESOURCE_ID=your_queued_resource_id export VALID_DURATION=1d export QUOTA_TYPE=quota_type
Add the following variables for Multislice:
export NODE_COUNT=node_count export NODE_PREFIX=your_tpu_prefix # Optional
Create a TPU resource.
gcloud alpha compute tpus queued-resources create ${QUEUED_RESOURCE_ID} \ --node-id ${TPU_NAME} \ --project ${PROJECT_ID} \ --zone ${ZONE} \ --accelerator-type ${ACCELERATOR_TYPE} \ --runtime-version ${RUNTIME_VERSION} \ --valid-until-duration ${VALID_DURATION} \ --service-account ${SERVICE_ACCOUNT} \ --${QUOTA_TYPE}Note: The following flags are only needed if you are using Multislice:
--node-count ${NODE_COUNT} \ --node-prefix ${NODE_PREFIX} # optional, the default is QUEUED_RESOURCE_ID
You will be able to connect to your TPU VMs using SSH once your
QueuedResourceis in state
ACTIVE:
Use the
command to query the status of your queued resource.
[describe](/tpu/docs/queued-resources#retrieve_state_and_diagnostic_information_about_a_queued_resource_request)
gcloud alpha compute tpus queued-resources describe ${QUEUED_RESOURCE_ID} --project ${PROJECT_ID} --zone ${ZONE}
state: ACTIVE
Set up your project, zone, and SSH key.
gcloud config set project ${PROJECT_ID} && gcloud config set compute/zone ${ZONE} && ssh-keygen -f ~/.ssh/google_compute_engine
Install dependencies.
If you are running the script from a TPU VM, you will need to add flag
--INTERNAL_IP=true.
&& cd maxtext && python3 multihost_runner.py --TPU_PREFIX=${TPU_NAME} --COMMAND='bash setup.sh'
Run the model with various
[configuration scripts](https://github.com/google/maxtext/tree/main/MaxText/configs/experimental), such as 32b.sh, 64b.sh. If you are running the script from a TPU VM, you need to add the flag
--INTERNAL_IP=true.
python3 multihost_runner.py --TPU_PREFIX=${TPU_NAME} --COMMAND=""bash MaxText/configs/experimental/64b.sh RUN_NAME=${RUN_NAME} OUTPUT_PATH=${MAXTEXT_OUTPUT_PATH} PLATFORM=gce""
Clean up
For instructions on how to delete your TPU and queued resource, see
[Clean up.](#clean-up)
Benchmark results
The MaxText training script was run from 32B to 1024B with bf16 precision. The results of these runs are shown in the following table.
|
No. of params
|
Accelerator Type
|
TFLOP/chip/sec
|
Model flops utilization
(MFU)
|
32B
|
v5p-128
|
3.28E+02
|
71.47%
|
64B
|
v5p-128
|
3.23E+02
|
70.31%
|
128B
|
v5p-256
|
3.15E+02
|
68.68%
|
128B
|
v5p-512
|
3.15E+02
|
68.53%
|
256B
|
v5p-1024
|
3.16E+02
|
68.82%
|
512B
|
v5p-1024
|
2.94E+02
|
63.99%
|
1024B
|
v5p-2048
|
2.66E+02
|
57.95%
|
1024B
|
v5p-4096
|
2.97E+02
|
64.80%
The 256B parameter model has been tested on v5p-512 and v5p-1024 using both bf16 and int8 weights. The following table displays the results of these tests.
|
v5p-512
|
v5p-512
|
v5p-1024
|
v5p-1024
|
Global batch size
(tokens)
|
5.24E+05
|
5.24E+05
|
1.05E+06
|
1.05E+06
|
Precision
|
bf16
|
int8
|
bf16
|
int8
|
TFLOP/chip/sec
|
307
|
408
|
308
|
414
|
Model flops utilization
(MFU)
|
66.98%
|
88.85%
|
67.09%
|
90.23%
TensorFlow
Train ResNet on a single host v5p
This tutorial describes how to train ImageNet on
v5p-4 or
v5p-8
using a fake dataset. If you want to use a different dataset, refer to
[Preparing the dataset](https://github.com/google/flax/blob/main/examples/imagenet/README.md#preparing-the-dataset).
Set up
Create environment variables:
export PROJECT_ID=your-project-ID export ACCELERATOR_TYPE=v5p-4 export ZONE=us-east1-c export RUNTIME_VERSION=tpu-vm-tf-2.15.0-pjrt export TPU_NAME=your-tpu-name export QUEUED_RESOURCE_ID=your-queued-resource-id export QUOTA_TYPE=quota-type
ACCELERATOR_TYPEcan be either
v5p-4or
v5p-8.
gcloud alpha compute tpus queued-resources create ${QUEUED_RESOURCE_ID} \ --node-id ${TPU_NAME} \ --project ${PROJECT_ID} \ --zone ${ZONE} \ --accelerator-type ${ACCELERATOR_TYPE} \ --runtime-version ${RUNTIME_VERSION} \ --${QUOTA_TYPE}
gcloud alpha compute tpus queued-resources describe ${QUEUED_RESOURCE_ID} \ --project ${PROJECT_ID} \ --zone ${ZONE}
Connect to your TPU using SSH
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \ --project ${PROJECT_ID} \ --zone ${ZONE}
Set some environment variables
export MODELS_REPO=/usr/share/tpu/models export PYTHONPATH=""${MODELS_REPO}:${PYTHONPATH}"" export MODEL_DIR=gcp-directory-to-store-model export DATA_DIR=gs://cloud-tpu-test-datasets/fake_imagenet export NEXT_PLUGGABLE_DEVICE_USE_C_API=true export TF_PLUGGABLE_DEVICE_LIBRARY_PATH=/lib/libtpu.so
Change to the models repository directory and install requirements.
cd ${MODELS_REPO} && git checkout r2.15.0 pip install -r official/requirements.txt
Train the model
Run the training script.
python3 official/vision/train.py \ --tpu=local \ --experiment=resnet_imagenet \ --mode=train_and_eval \ --config_file=official/vision/configs/experiments/image_classification/imagenet_resnet50_tpu.yaml \ --model_dir=${MODEL_DIR} \ --params_override=""runtime.distribution_strategy=tpu,task.train_data.input_path=${DATA_DIR}/train*,task.validation_data.input_path=${DATA_DIR}/validation*,task.train_data.global_batch_size=2048,task.validation_data.global_batch_size=2048,trainer.train_steps=100""
Clean up
For instructions on how to delete your TPU and queued resource, see
[Clean up.](#clean-up)
Train ResNet on a multi-host v5p
This tutorial describes how to train ImageNet on
v5p-16 or larger using
a fake dataset. If you want to use a different dataset, see
[Preparing the dataset](https://github.com/google/flax/blob/main/examples/imagenet/README.md#preparing-the-dataset).
Create environment variables:
export PROJECT_ID=your_project_ID export ACCELERATOR_TYPE=v5p-16 export ZONE=us-east1-c export RUNTIME_VERSION=tpu-vm-tf-2.15.0-pod-pjrt export TPU_NAME=your_tpu_name export QUEUED_RESOURCE_ID=your-queued-resource-id export QUOTA_TYPE=quota-type
ACCELERATOR_TYPEcan be either
v5p-16or larger.
gcloud alpha compute tpus queued-resources create ${QUEUED_RESOURCE_ID} \ --node-id ${TPU_NAME} \ --project ${PROJECT_ID} \ --zone ${ZONE} \ --accelerator-type ${ACCELERATOR_TYPE} \ --runtime-version ${RUNTIME_VERSION} \ --${QUOTA_TYPE}
Use the
command to query the status of your queued resource:
[describe](/tpu/docs/queued-resources#retrieve_state_and_diagnostic_information_about_a_queued_resource_request)
gcloud alpha compute tpus queued-resources describe ${QUEUED_RESOURCE_ID} \ --project ${PROJECT_ID} \ --zone ${ZONE}
Connect to your TPU (worker zero) using SSH
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \ --project ${PROJECT_ID} \ --zone ${ZONE}
Set some environment variables
export MODELS_REPO=/usr/share/tpu/models export PYTHONPATH=""${MODELS_REPO}:${PYTHONPATH}"" export MODEL_DIR=gcp-directory-to-store-model export DATA_DIR=gs://cloud-tpu-test-datasets/fake_imagenet export TPU_LOAD_LIBRARY=0
Change to the models repository directory and install requirements.
cd $MODELS_REPO && git checkout r2.15.0 pip install -r official/requirements.txt
Train the model
Run the training script.
python3 official/vision/train.py \ --tpu=${TPU_NAME} \ --experiment=resnet_imagenet \ --mode=train_and_eval \ --config_file=official/vision/configs/experiments/image_classification/imagenet_resnet50_tpu.yaml \ --model_dir=${MODEL_DIR} \ --params_override=""runtime.distribution_strategy=tpu,task.train_data.input_path=${DATA_DIR}/train*,task.validation_data.input_path=${DATA_DIR}/validation*,task.train_data.global_batch_size=2048,task.validation_data.global_batch_size=2048,trainer.train_steps=100""
Clean up
For instructions on how to delete your TPU and queued resource, see
[Clean up.](#clean-up)
PyTorch/XLA
Llama 2
This tutorial will cover how to train the Llama 2 7B model on v5p using a fork of the HuggingFace repository on PyTorch/XLA with General and Scalable Parallelization for ML Computation Graphs (GSPMD).
Setup
Create variables for project ID, accelerator type, zone, runtime version, and TPU name.
export PROJECT_ID=your_project_ID export ACCELERATOR_TYPE=v5p-8 export ZONE=us-east5-a export RUNTIME_VERSION=v2-alpha-tpuv5 export SERVICE_ACCOUNT=your_service_account export TPU_NAME=your_tpu_name export QUEUED_RESOURCE_ID=your_queued_resource_id export QUOTA_TYPE=quota_type export VALID_DURATION=1d
Create a TPU resource.
gcloud alpha compute tpus queued-resources create ${QUEUED_RESOURCE_ID} \ --node-id ${TPU_NAME} \ --project ${PROJECT_ID} \ --zone ${ZONE} \ --accelerator-type ${ACCELERATOR_TYPE} \ --runtime-version ${RUNTIME_VERSION} \ --valid-until-duration ${VALID_DURATION} \ --service-account ${SERVICE_ACCOUNT} \ --${QUOTA_TYPE}
You will be able to connect to your TPU VM using SSH once your
QueuedResourceis in the
ACTIVEstate:
Use the
command to query the status of your queued resource.
[describe](/tpu/docs/queued-resources#retrieve_state_and_diagnostic_information_about_a_queued_resource_request)
gcloud alpha compute tpus queued-resources describe ${QUEUED_RESOURCE_ID} \ --project ${PROJECT_ID} \ --zone ${ZONE}
state: ACTIVE
Install Pytorch/XLA and corresponding dependencies.
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \ --project ${PROJECT_ID} \ --zone ${ZONE} \ --worker=all \ --command=' sudo apt-get update sudo apt-get install libopenblas-dev -y pip3 install numpy pip3 install typing-extensions pip install torch~=2.2.0 torch_xla[tpu]~=2.2.0 -f https://storage.googleapis.com/libtpu-releases/index.html '
Download the HuggingFace repository and install requirements.
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} --project=${PROJECT_ID} \ --zone=${ZONE} \ --worker=all \ --command=' git clone -b llama2-google-next-training https://github.com/pytorch-tpu/transformers.git cd transformers pip3 install git+file://$PWD pip3 install datasets accelerate evaluate scikit-learn'
Download the 7B model config.
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \ --project=${PROJECT_ID} \ --zone=${ZONE} \ --worker=all \ --command=""curl https://huggingface.co/TheBloke/Llama-2-7B-fp16/raw/main/config.json --output ~/config.json""
Train the model
gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \ --project=${PROJECT_ID} \ --zone=${ZONE} \ --worker=all \ --command=' export PJRT_DEVICE=TPU export XLA_USE_BF16=1 export XLA_IR_DEBUG=1 export XLA_HLO_DEBUG=1 export LIBTPU_INIT_ARGS=""--xla_enable_async_collective_permute=true --xla_tpu_enable_async_collective_fusion_multiple_steps=true --xla_tpu_enable_async_collective_fusion=true --xla_tpu_overlap_compute_collective_tc=true --xla_enable_async_all_gather=true --xla_jf_spmd_threshold_for_windowed_einsum_mib=0"" export PROFILE_EPOCH=0 export PROFILE_STEP=3 export PROFILE_DURATION_MS=20000 export PROFILE_LOGDIR=/tmp/home/ cd transformers python examples/pytorch/language-modeling/run_clm.py \ --tokenizer_name hf-internal-testing/llama-tokenizer \ --dataset_name wikitext \ --dataset_config_name wikitext-2-raw-v1 \ --per_device_train_batch_size 96 \ --per_device_eval_batch_size 8 \ --num_train_epochs 1 \ --do_train \ --output_dir /tmp/output \ --overwrite_output_dir \ --config_name ~/config.json \ --save_strategy no \ --logging_strategy no \ --remove_unused_columns no \ --optim adafactor \ --torch_dtype bfloat16 \ --dataloader_drop_last yes \ --block_size 2048 \ --spmd_2d_sharding 1 \ --spmd_grad_chkpt '
If you're running in a multislice environment, set the flag
--spmd_dcn_parallelism to the number of slices.
The
[SPMD_USER_GUIDE](https://github.com/pytorch-tpu/transformers/blob/llama2-google-next-training/SPMD_USER_GUIDE.md#steps-to-run-hf-llama-2) provides
a more in-depth user guide that explains all the different environment
variables and toggles of the HF script. To be noted, the
LIBTPU_INIT_ARGS will be incorporated into PyTorch/XLA and
on by default in future releases.
Clean up
For instructions on how to delete your TPU and queued resource, see
[Clean up.](#clean-up)
Benchmark results
Throughput for all three Llama 2 model sizes are included in the following table.
|
v5p-8
|
v5p-128
|
v5p-128
|
Model size
|
7B
|
13B
|
70B
|
Global batch size
|
96
|
1024
|
128
|
Sharding mesh shape
|
(4, 1)
|
(64, 1)
|
(16, 4)
|
Model flops utilization
(MFU)
|
56.67%
|
55.80%
|
51.85%
Support and Feedback
We welcome all feedback! To share feedback or request support,
fill out the
[Cloud TPU Support or Feedback form](https://forms.gle/pLFRKSdWZ97o2o867)
or by email at:
[cloudtpu-preview-support@google.com](mailto:cloudtpu-preview-support@google.com)",Cloud TPU v5p training | Google Cloud,
id,url,body,title,description
138,https://cloud.google.com/tpu/docs/storage-buckets,"Connecting to Cloud Storage Buckets
This page introduces
[Cloud Storage](/storage/docs) as an option for storing your
machine learning data and training output, and describes how to give your
Cloud TPU access to the data objects on Cloud Storage.
Before you begin
You need a Cloud TPU
[Service Account](https://cloud.google.com/iam/docs/service-accounts)
in order to access a Cloud Storage
bucket.
Create a Cloud TPU Service Account for your project.
gcloud beta services identity create --service tpu.googleapis.com --project $PROJECT_ID
The command returns a Cloud TPU Service Account with following format:
service-PROJECT_NUMBER@cloud-tpu.iam.gserviceaccount.com
Follow the instructions found in either the
[Cloud TPU quickstart guide](/tpu/docs/quickstart)or the [Creating and deleting TPUs document](/tpu/docs/creating-deleting-tpus)to configure your Google Cloud project and create your Cloud TPU VM and TPU resources.
Writing data to Cloud Storage
Console
Go to the Cloud Storage page on the Google Cloud console.
Create a new bucket, specifying the following options:
- A unique name of your choosing.
- Default storage class:
Standard
- Location:
us-central1
gsutil
Use the
gsutil mbcommand to create a Cloud Storage bucket:
gsutil mb -l region gs://bucket-name/
where:
region is the region where you created the Cloud TPU. For example:
us-central1. Cloud TPU is available in the following regions:
US
Cloud TPU v2 and Preemptible v2
us-central1
Cloud TPU v3 and Preemptible v3
us-central1
Cloud TPU v4 and Preemptible v4
us-central2
Cloud TPU v2 Pod
us-central1
Cloud TPU v4 Pod
us-central2
Europe
Cloud TPU v2 and Preemptible v2
europe-west4
Cloud TPU v3 and Preemptible v3
europe-west4
Cloud TPU v2 Pod
europe-west4
Cloud TPU v3 Pod
europe-west4
Asia Pacific
Cloud TPU v2 and Preemptible v2
asia-east1
bucket-name is the name of the bucket you want to create.
-
Use the
gsutil cpcommand to write files to the Cloud Storage bucket:
gsutil cp -r local-data-dir gs://bucket-name
where local-data-dir is a local path to your data. For example:
$HOME/your-data
Giving your Cloud TPU access to Cloud Storage
You need to give your Cloud TPU read/write access to your Cloud Storage objects. To do that, you must grant the required access to the Cloud TPU Service Account used by the Cloud TPU. Follow these steps to find the Cloud TPU Service Account and grant the necessary access:
Authorize the Cloud TPU Service Account
Using fine-grained ACLs for Cloud TPU (Recommended)
If you store training data on Cloud Storage, the Cloud TPU Service Account needs read and write permission on the bucket.
Console
Go to the Cloud Storage browser page to view the buckets you own.
Select the bucket whose ACL you want to modify.
Select the
Permissionstab.
Select
Addto add a new permission and type the complete Service Account name in the
New Principalsedit box.
If you are reading from this bucket, you must authorize the TPU Service Account to read from the resource. Do this by granting the Service Account the
Storage Legacy > Storage Legacy Bucket Readerrole.
If you are writing to this bucket, you must authorize the TPU Service Account to write to the resource. Do this by granting the Service Account the
Storage Legacy > Storage Legacy Bucket Writerrole.
gsutil
If you are reading from this bucket, grant read permission for the Cloud TPU Service Account:
gsutil acl ch -u tpu-service-account:READER gs://bucket-name
If you are writing to this bucket, grant write permission for the Cloud TPU Service Account:
gsutil acl ch -u tpu-service-account:WRITER gs://bucket-name
Using IAM permissions for Cloud TPU (Alternative)
If you want to grant broader permissions instead of whitelisting access to each bucket explicitly, you can grant the Identity Access Management (IAM) Storage Admin role to the Cloud TPU Service Account.
Go to the IAM page for your project.
Click the +Add button to add principals to the project.
Enter the names of the Cloud TPU Service Account in the Principals text box.
Click the Roles dropdown list.
Enable the following roles:
Project > Viewer
Storage > Storage Admin
-
Cloud Storage FUSE
Cloud Storage FUSE allows you to mount and access Cloud Storage buckets as local file systems. This allows applications to read and write objects in your bucket using standard file system semantics.
See the
[Cloud Storage FUSE documentation](/storage/docs/gcs-fuse)
for details about how Cloud Storage FUSE works and a description of how
Cloud Storage FUSE operations map to Cloud Storage operations. You can find
additional information about how to use Cloud Storage FUSE, such as how to
install the gcsfuse CLI and mounting buckets on
[GitHub.](https://github.com/GoogleCloudPlatform/gcsfuse/tree/master/docs)
What's next
- For more information about creating Cloud Storage buckets and writing data to
those buckets, see the
[Cloud Storage documentation](/storage/docs/getting-started-gsutil#create).
- For more information about Service Accounts, see the
[authentication overview](/docs/authentication).",Connecting to Cloud Storage Buckets | Cloud TPU | Google Cloud,
id,url,body,title,description
26,https://cloud.google.com/tpu/docs/tutorials/resnet-pytorch,"This tutorial shows you how to train the ResNet-50 model on a Cloud TPU device with PyTorch. You can apply the same pattern to other TPU-optimised image classification models that use PyTorch and the ImageNet dataset.
The model in this tutorial is based on
[Deep Residual Learning for Image
Recognition](https://arxiv.org/pdf/1512.03385.pdf), which first introduces
the residual network (ResNet) architecture. The tutorial uses the 50-layer
variant, ResNet-50, and demonstrates training the model using
[PyTorch/XLA](https://github.com/pytorch/xla).
Objectives
- Prepare the dataset.
- Run the training job.
- Verify the output results.
Costs
In this document, you use the following billable components of Google Cloud:
- Compute Engine
- Cloud TPU
To generate a cost estimate based on your projected usage,
use the
[pricing calculator](/products/calculator).
[free trial](/free-trial).
Before you begin
Before starting this tutorial, check that your Google Cloud project is correctly set up.
-
Sign in to your Google Cloud account. If you're new to
Google Cloud,
[create an account](https://console.cloud.google.com/freetrial)to evaluate how our products perform in real-world scenarios. New customers also get $300 in free credits to run, test, and deploy workloads.
-
In the Google Cloud console, on the project selector page, select or
[create a Google Cloud project](/resource-manager/docs/creating-managing-projects).
-
[Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console).
-
In the Google Cloud console, on the project selector page, select or
[create a Google Cloud project](/resource-manager/docs/creating-managing-projects).
-
[Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console).
This walkthrough uses billable components of Google Cloud. Check the
[Cloud TPU pricing page](/tpu/docs/pricing) to
estimate your costs. Be sure to
[clean up](#clean_up)
resources you create when you've finished with them to avoid unnecessary
charges.
Create a TPU VM
Open a Cloud Shell window.
Create a TPU VM
gcloud compute tpus tpu-vm create your-tpu-name \ --accelerator-type=v4-8 \ --version=tpu-ubuntu2204-base \ --zone=us-central2-b \ --project=your-project
Connect to your TPU VM using SSH:
gcloud compute tpus tpu-vm ssh your-tpu-name --zone=us-central2-b
Install PyTorch/XLA on your TPU VM:
(vm)$ pip install torch~=2.2.0 torch_xla[tpu]~=2.2.0 torchvision -f https://storage.googleapis.com/libtpu-releases/index.html
Clone the
[PyTorch/XLA github repo](https://github.com/pytorch/xla)
(vm)$ git clone --depth=1 --branch r2.2 https://github.com/pytorch/xla.git
Run the training script with fake data
(vm) $ PJRT_DEVICE=TPU python3 xla/test/test_train_mp_imagenet.py --fake_data --batch_size=256 --num_epochs=1
If you are able to train the model using fake data, you can try training on
real data, such as ImageNet. For instructions on downloading
[ImageNet](https://image-net.org/index.php), see
[Downloading ImageNet](/tpu/docs/imagenet-setup). In the training script command,
the
--datadir flag specifies the location of the dataset on which to train.
The following command assumes the ImageNet dataset is located in
~/imagenet.
(vm) $ PJRT_DEVICE=TPU python3 xla/test/test_train_mp_imagenet.py --datadir=~/imagenet --batch_size=256 --num_epochs=1
Clean up
To avoid incurring charges to your Google Cloud account for the resources used in this tutorial, either delete the project that contains the resources, or keep the project and delete the individual resources.
Disconnect from the TPU VM:
(vm) $ exit
Your prompt should now be
username@projectname, showing you are in the Cloud Shell.
Delete your TPU VM.
$ gcloud compute tpus tpu-vm delete resnet50-tutorial \ --zone=us-central2-b
What's next
Try the PyTorch colabs:
[Getting Started with PyTorch on Cloud TPUs](https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/getting-started.ipynb) [Training MNIST on TPUs](https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/mnist-training.ipynb) [Training ResNet18 on TPUs with Cifar10 dataset](https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/resnet18-training.ipynb) [Inference with Pretrained ResNet50 Model](https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/resnet50-inference.ipynb) [Fast Neural Style Transfer](https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/style_transfer_inference.ipynb) [MultiCore Training AlexNet on Fashion MNIST](https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/multi-core-alexnet-fashion-mnist.ipynb) [Single Core Training AlexNet on Fashion MNIST](https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/single-core-alexnet-fashion-mnist.ipynb)",Training Resnet50 on Cloud TPU with PyTorch | Google Cloud,
id,url,body,title,description
51,https://cloud.google.com/tpu/docs/reference/rest/v1/projects.locations.nodes,"[Resource: Node](#Node) [State](#State) [SchedulingConfig](#SchedulingConfig) [NetworkEndpoint](#NetworkEndpoint) [Health](#Health) [ApiVersion](#ApiVersion) [Symptom](#Symptom) [SymptomType](#SymptomType) [Methods](#METHODS_SUMMARY)
Resource: Node
A TPU instance.
|JSON representation
|
{ ""name"": string, ""description"": string, ""acceleratorType"": string, ""ipAddress"": string, ""port"": string, ""state"": enum (
|Fields
|
name
|
Output only. Immutable. The name of the TPU
|
description
|
The user-supplied description of the TPU. Maximum of 512 characters.
|
acceleratorType
|
Required. The type of hardware accelerators associated with this node.
|
ipAddress
|
Output only. DEPRECATED! Use networkEndpoints instead. The network address for the TPU Node as visible to Compute Engine instances.
|
port
|
Output only. DEPRECATED! Use networkEndpoints instead. The network port for the TPU Node as visible to Compute Engine instances.
|
state
|
Output only. The current state for the TPU Node.
|
healthDescription
|
Output only. If this field is populated, it contains a description of why the TPU Node is unhealthy.
|
tensorflowVersion
|
Required. The version of Tensorflow running in the Node.
|
network
|
The name of a network they wish to peer the TPU node to. It must be a preexisting Compute Engine network inside of the project on which this API has been activated. If none is provided, ""default"" will be used.
|
cidrBlock
|
The CIDR block that the TPU node will use when selecting an IP address. This CIDR block must be a /29 block; the Compute Engine networks API forbids a smaller block, and using a larger block would be wasteful (a node can only consume one IP address). Errors will occur if the CIDR block has already been used for a currently existing TPU node, the CIDR block conflicts with any subnetworks in the user's provided network, or the provided network is peered with another network that is using that CIDR block.
|
serviceAccount
|
Output only. The service account used to run the tensor flow services within the node. To share resources, including Google Cloud Storage data, with the Tensorflow job running in the Node, this account must have permissions to that data.
|
createTime
|
Output only. The time when the node was created.
A timestamp in RFC3339 UTC ""Zulu"" format, with nanosecond resolution and up to nine fractional digits. Examples:
|
schedulingConfig
|
The scheduling options for this node.
|
networkEndpoints[]
|
Output only. The network endpoints where TPU workers can be accessed and sent work. It is recommended that Tensorflow clients of the node reach out to the 0th entry in this map first.
|
health
|
The health status of the TPU node.
|
labels
|
Resource labels to represent user-provided metadata.
An object containing a list of
|
useServiceNetworking
|
Whether the VPC peering for the node is set up through Service Networking API. The VPC Peering should be set up before provisioning the node. If this field is set, cidrBlock field should not be specified. If the network, that you want to peer the TPU Node to, is Shared VPC networks, the node must be created with this this field enabled.
|
apiVersion
|
Output only. The API version that created this Node.
|
symptoms[]
|
Output only. The Symptoms that have occurred to the TPU Node.
State
Represents the different states of a TPU node during its lifecycle.
|Enums
|
STATE_UNSPECIFIED
|TPU node state is not known/set.
|
CREATING
|TPU node is being created.
|
READY
|TPU node has been created.
|
RESTARTING
|TPU node is restarting.
|
REIMAGING
|TPU node is undergoing reimaging.
|
DELETING
|TPU node is being deleted.
|
REPAIRING
|TPU node is being repaired and may be unusable. Details can be found in the
help_description field.
|
STOPPED
|TPU node is stopped.
|
STOPPING
|TPU node is currently stopping.
|
STARTING
|TPU node is currently starting.
|
PREEMPTED
|TPU node has been preempted. Only applies to Preemptible TPU Nodes.
|
TERMINATED
|TPU node has been terminated due to maintenance or has reached the end of its life cycle (for preemptible nodes).
|
HIDING
|TPU node is currently hiding.
|
HIDDEN
|TPU node has been hidden.
|
UNHIDING
|TPU node is currently unhiding.
SchedulingConfig
Sets the scheduling options for this node.
|JSON representation
|
{ ""preemptible"": boolean, ""reserved"": boolean }
|Fields
|
preemptible
|
Defines whether the node is preemptible.
|
reserved
|
Whether the node is created under a reservation.
NetworkEndpoint
A network endpoint over which a TPU worker can be reached.
|JSON representation
|
{ ""ipAddress"": string, ""port"": integer }
|Fields
|
ipAddress
|
The IP address of this network endpoint.
|
port
|
The port of this network endpoint.
Health
Health defines the status of a TPU node as reported by Health Monitor.
|Enums
|
HEALTH_UNSPECIFIED
|Health status is unknown: not initialized or failed to retrieve.
|
HEALTHY
|The resource is healthy.
|
DEPRECATED_UNHEALTHY
|The resource is unhealthy.
|
TIMEOUT
|The resource is unresponsive.
|
UNHEALTHY_TENSORFLOW
|The in-guest ML stack is unhealthy.
|
UNHEALTHY_MAINTENANCE
|The node is under maintenance/priority boost caused rescheduling and will resume running once rescheduled.
ApiVersion
TPU API Version.
|Enums
|
API_VERSION_UNSPECIFIED
|API version is unknown.
|
V1_ALPHA1
|TPU API V1Alpha1 version.
|
V1
|TPU API V1 version.
|
V2_ALPHA1
|TPU API V2Alpha1 version.
Symptom
A Symptom instance.
|JSON representation
|
{
""createTime"": string,
""symptomType"": enum (
|Fields
|
createTime
|
Timestamp when the Symptom is created.
A timestamp in RFC3339 UTC ""Zulu"" format, with nanosecond resolution and up to nine fractional digits. Examples:
|
symptomType
|
Type of the Symptom.
|
details
|
Detailed information of the current Symptom.
|
workerId
|
A string used to uniquely distinguish a worker within a TPU node.
SymptomType
SymptomType represents the different types of Symptoms that a TPU can be at.
|Enums
|
SYMPTOM_TYPE_UNSPECIFIED
|Unspecified symptom.
|
LOW_MEMORY
|TPU VM memory is low.
|
OUT_OF_MEMORY
|TPU runtime is out of memory.
|
EXECUTE_TIMED_OUT
|TPU runtime execution has timed out.
|
MESH_BUILD_FAIL
|TPU runtime fails to construct a mesh that recognizes each TPU device's neighbors.
|
HBM_OUT_OF_MEMORY
|TPU HBM is out of memory.
|
PROJECT_ABUSE
|Abusive behaviors have been identified on the current project.
|
Methods
|
|Creates a node.
|
|Deletes a node.
|
|Gets the details of a node.
|
|Lists nodes.
|
|Reimages a node's OS.
|
|Starts a node.
|
|Stops a node, this operation is only available with single TPU nodes.",REST Resource: projects.locations.nodes | Cloud TPU | Google Cloud,
id,url,body,title,description
131,https://cloud.google.com/tpu/docs/tutorials,"Image Classification
-
Running MNIST on Cloud TPU (TF 2.x)
An MNIST image classification model using TensorFlow, optimized to run on Cloud TPU.
-
Training ResNet on Cloud TPU (TF 2.x)
A ResNet image classification model using TensorFlow, optimized to run on Cloud TPU.
-
Training Keras ResNet-RS on Cloud TPU (TF 2.x)
A Keras ResNet-RS model using TensorFlow, optimized to run on Cloud TPU.
-
Training ResNet on Cloud TPU (PyTorch)
A ResNet image classification model using PyTorch, optimized to run on Cloud TPU.
-
Training EfficientNet on Cloud TPU (TF 2.x)
An EfficientNet image classification model using TensorFlow, optimized to run on Cloud TPU.
Object Detection
-
Training RetinaNet on Cloud TPU (TF 2.x)
A RetinaNet object detection model using TensorFlow, optimized to run on Cloud TPU.
-
Training ShapeMask on Cloud TPU (TF 2.x)
A ShapeMask object detection model using TensorFlow, optimized to run on Cloud TPU.
-
Training Mask RCNN on Cloud TPU (TF 2.x)
A Mask RCNN model using TensorFlow, optimized to run on Cloud TPU.
Recommendation Systems
-
Training DLRM and DCN models on Cloud TPU (TF 2.x)
A guide to training DLRM and DCN v2 ranking models for tasks such as click-through rate (CTR) prediction.
-
Training a Neural Collaboration Filtering model on Cloud TPU (TF 2.x)
An implementation of the Neural Collaborative Filtering (NCF) framework with the Neural Matrix Factorization (NeuMF) model
Distributed Processing on a Pod
Natural Language Processing
-
BERT FineTuning with Cloud TPU: Sentence and Sentence-Pair Classification Tasks (TF 2.x)
Discover how to use Bidirectional Encoder Representations from Transformers (BERT) with Cloud TPU.
-
Pre-training FairSeq RoBERTa on Cloud TPU (PyTorch)
A guide to pre-training the FairSeq version of the RoBERTa model on Cloud TPU using the public wikitext-103 dataset.",Tutorials | Cloud TPU | Google Cloud,
id,url,body,title,description
172,https://cloud.google.com/tpu/docs/reference/rest/v2alpha1/projects.locations.nodes/delete,"Method: projects.locations.nodes.delete
Stay organized with collections
Save and categorize content based on your preferences.
HTTP request
DELETE https://tpu.googleapis.com/v2alpha1/{name=projects/*/locations/*/nodes/*}
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
string
Required. The resource name.
Query parameters
|Parameters
|
requestId
|
string
Idempotent request UUID.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[Operation](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse#Operation)
Authorization scopes
Requires one of the following OAuth scopes:
-
https://www.googleapis.com/auth/cloud.tpu
-
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Method: projects.locations.nodes.delete | Cloud TPU | Google Cloud,
id,url,body,title,description
101,https://cloud.google.com/tpu/docs/reference/rest/v1/projects.locations.tensorflowVersions/get,"Method: projects.locations.tensorflowVersions.get
Stay organized with collections
Save and categorize content based on your preferences.
HTTP request
GET https://tpu.googleapis.com/v1/{name=projects/*/locations/*/tensorflowVersions/*}
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
string
Required. The resource name.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[TensorFlowVersion](/tpu/docs/reference/rest/v1/projects.locations.tensorflowVersions#TensorFlowVersion)
Authorization scopes
Requires one of the following OAuth scopes:
-
https://www.googleapis.com/auth/cloud.tpu
-
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Method: projects.locations.tensorflowVersions.get | Cloud TPU | Google Cloud,
id,url,body,title,description
122,https://cloud.google.com/tpu/docs/reference/rest/v2alpha1/projects.locations.nodes/start,"Method: projects.locations.nodes.start
Stay organized with collections
Save and categorize content based on your preferences.
HTTP request
POST https://tpu.googleapis.com/v2alpha1/{name=projects/*/locations/*/nodes/*}:start
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
string
Required. The resource name.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[Operation](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse#Operation)
Authorization scopes
Requires one of the following OAuth scopes:
-
https://www.googleapis.com/auth/cloud.tpu
-
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Method: projects.locations.nodes.start | Cloud TPU | Google Cloud,
id,url,body,title,description
77,https://cloud.google.com/tpu/docs/reference/rest/v1alpha1/projects.locations.operations/cancel,"Starts asynchronous cancellation on a long-running operation. The server makes a best effort to cancel the operation, but success is not guaranteed. If the server doesn't support this method, it returns
google.rpc.Code.UNIMPLEMENTED. Clients can use
or other methods to check whether the cancellation succeeded or whether the operation completed despite cancellation. On successful cancellation, the operation is not deleted; instead, it becomes an operation with an
[Operations.GetOperation](/tpu/docs/reference/rest/v1alpha1/projects.locations.operations/get#google.longrunning.Operations.GetOperation)
value with a
[Operation.error](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse#Operation.FIELDS.error)
of 1, corresponding to
[google.rpc.Status.code](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse#Status.FIELDS.code)
Code.CANCELLED.
HTTP request
POST https://tpu.googleapis.com/v1alpha1/{name=projects/*/locations/*/operations/*}:cancel
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
The name of the operation resource to be cancelled.
Request body
The request body must be empty.
Response body
If successful, the response body is empty.
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).",Method: projects.locations.operations.cancel | Cloud TPU | Google Cloud,
id,url,body,title,description
111,https://cloud.google.com/tpu/docs/reference/rest/v2/projects.locations.acceleratorTypes/list,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Query parameters](#body.QUERY_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [Try it!](#try-it)
Lists accelerator types supported by this API.
HTTP request
GET https://tpu.googleapis.com/v2/{parent=projects/*/locations/*}/acceleratorTypes
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
parent
|
Required. The parent resource name.
Query parameters
|Parameters
|
pageSize
|
The maximum number of items to return.
|
pageToken
|
The nextPageToken value returned from a previous List request, if any.
|
filter
|
List filter.
|
orderBy
|
Sort results.
Request body
The request body must be empty.
Response body
Response for
.
[acceleratorTypes.list](/tpu/docs/reference/rest/v2/projects.locations.acceleratorTypes/list#google.cloud.tpu.v2.Tpu.ListAcceleratorTypes)
If successful, the response body contains data with the following structure:
|JSON representation
|
{
""acceleratorTypes"": [
{
object (
|Fields
|
acceleratorTypes[]
|
The listed nodes.
|
nextPageToken
|
The next page token or empty if none.
|
unreachable[]
|
Locations that could not be reached.
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).",Method: projects.locations.acceleratorTypes.list | Cloud TPU | Google Cloud,
id,url,body,title,description
17,https://cloud.google.com/tpu/docs/reference/rest/v2/projects.locations.nodes/patch,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Query parameters](#body.QUERY_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [Try it!](#try-it)
Updates the configurations of a node.
HTTP request
PATCH https://tpu.googleapis.com/v2/{node.name=projects/*/locations/*/nodes/*}
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
node.name
|
Output only. Immutable. The name of the TPU.
Query parameters
|Parameters
|
updateMask
|
Required. Mask of fields from [Node][Tpu.Node] to update. Supported fields: [description, tags, labels, metadata, networkConfig.enable_external_ips].
This is a comma-separated list of fully qualified names of fields. Example:
Request body
The request body contains an instance of
.
[Node](/tpu/docs/reference/rest/v2/projects.locations.nodes#Node)
Response body
If successful, the response body contains an instance of
.
[Operation](/tpu/docs/reference/rest/Shared.Types/ListOperationsResponse#Operation)
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).",Method: projects.locations.nodes.patch | Cloud TPU | Google Cloud,
id,url,body,title,description
191,https://cloud.google.com/tpu/docs/colabs,"TensorFlow Colab notebooks
-
Fashion MNIST with Keras and TPU
Train, export, and deploy the fashion MNIST model.
-
MNIST with Keras and TPU
Train, export, and deploy the MNIST model.
-
Classification of flowers using TPUEstimator
TPUEstimator is only supported by TensorFlow 1.x. If you are writing a model with TensorFlow 2.x, use [Keras](https://keras.io/about/) instead. Train, evaluate, and generate predictions using TPUEstimator and Cloud TPUs. Use the iris dataset to predict the species of flowers.
-
Shakespeare with Keras and TPU
Use Keras to build and train a language model on Cloud TPU.
-
Profiling TPUs in Colab
Profile an image classification model on Cloud TPUs.
-
Custom training with TPUs
Create a model with Keras with a custom training loop.",Colab notebooks | Cloud TPU | Google Cloud,
id,url,body,title,description
163,https://cloud.google.com/tpu/docs/tutorials/retinanet-2.x,"This document describes an implementation of the RetinaNet object detection
model. The code is available on
[GitHub](https://github.com/tensorflow/models/tree/master/official/vision/detection).
The instructions below assume you are already familiar with running a model on
Cloud TPU. If you are new to Cloud TPU, you can
refer to the
[Quickstart](/tpu/docs/quickstart) for a basic introduction.
If you plan to train on a TPU Pod slice,
review
[Training on TPU Pods](/tpu/docs/training-on-tpu-pods)
to understand parameter changes required for Pod slices.
Objectives
- Prepare the COCO dataset
- Create a Cloud Storage bucket to hold your dataset and model output
- Set up TPU resources for training and evaluation
- Run training and evaluation on a single Cloud TPU or a Cloud TPU Pod
Costs
In this document, you use the following billable components of Google Cloud:
- Compute Engine
- Cloud TPU
- Cloud Storage
To generate a cost estimate based on your projected usage,
use the
[pricing calculator](/products/calculator).
[free trial](/free-trial).
Before you begin
Before starting this tutorial, check that your Google Cloud project is correctly set up.
-
Sign in to your Google Cloud account. If you're new to
Google Cloud,
[create an account](https://console.cloud.google.com/freetrial)to evaluate how our products perform in real-world scenarios. New customers also get $300 in free credits to run, test, and deploy workloads.
-
In the Google Cloud console, on the project selector page, select or
[create a Google Cloud project](/resource-manager/docs/creating-managing-projects).
-
[Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console).
-
In the Google Cloud console, on the project selector page, select or
[create a Google Cloud project](/resource-manager/docs/creating-managing-projects).
-
[Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console).
This walkthrough uses billable components of Google Cloud. Check the
[Cloud TPU pricing page](/tpu/docs/pricing) to
estimate your costs. Be sure to
[clean up](#clean_up)
resources you create when you've finished with them to avoid unnecessary
charges.
Prepare the COCO dataset
This tutorial uses the COCO dataset. The dataset needs to be in TFRecord format on a Cloud Storage bucket to be used for the training.
If you already have the COCO dataset prepared
on a Cloud Storage bucket that is located in the
[zone](/tpu/docs/types-zones-tpu-vm) you will
be using to train the model, you can go directly to
[single device training.](#single-device-training) Otherwise,
use the following steps to prepare the dataset.
Open a Cloud Shell window.
In your
[Cloud Shell](https://console.cloud.google.com/), configure
gcloudwith your project ID.
export PROJECT_ID=project-id gcloud config set project ${PROJECT_ID}
In your
[Cloud Shell](https://console.cloud.google.com/), create a Cloud Storage bucket using the following command:
gsutil mb -p ${PROJECT_ID} -c standard -l europe-west4 gs://bucket-name
Launch a Compute Engine VM instance.
This VM instance will only be used to download and preprocess the COCO dataset. Fill in the instance-name with a name of your choosing.
$ gcloud compute tpus execution-groups create \ --vm-only \ --name=instance-name \ --zone=europe-west4-a \ --disk-size=300 \ --machine-type=n1-standard-16 \ --tf-version=2.12.0
Command flag descriptions
vm-only
- Create a VM only. By default the
gcloud compute tpus execution-groupscommand creates a VM and a Cloud TPU.
name
- The name of the Cloud TPU to create.
zone
- The
[zone](https://cloud.google.com/tpu/docs/types-zones)where you plan to create your Cloud TPU.
disk-size
- The size of the hard disk in GB of the VM created by the
gcloud compute tpus execution-groupscommand.
machine-type
- The
[machine type](https://cloud.google.com/compute/docs/machine-types)of the Compute Engine VM to create.
tf-version
- The version of Tensorflow
gcloud compute tpus execution-groupsinstalls on the VM.
-
If you are not automatically logged in to the Compute Engine instance, log in by running the following
sshcommand. When you are logged into the VM, your shell prompt changes from
username@projectnameto
username@vm-name:
$ gcloud compute ssh instance-name --zone=europe-west4-a
Set up two variables, one for the storage bucket you created earlier and one for the directory that holds the training data (DATA_DIR) on the storage bucket.
(vm)$ export STORAGE_BUCKET=gs://bucket-name
(vm)$ export DATA_DIR=${STORAGE_BUCKET}/coco
Install the packages needed to pre-process the data.
(vm)$ sudo apt-get install -y python3-tk && \ pip3 install --user Cython matplotlib opencv-python-headless pyyaml Pillow && \ pip3 install --user ""git+https://github.com/cocodataset/cocoapi#egg=pycocotools&subdirectory=PythonAPI""
Run the
download_and_preprocess_coco.shscript to convert the COCO dataset into a set of TFRecords (
*.tfrecord) that the training application expects.
(vm)$ git clone https://github.com/tensorflow/tpu.git (vm)$ sudo bash tpu/tools/datasets/download_and_preprocess_coco.sh ./data/dir/coco
This installs the required libraries and then runs the preprocessing script. It outputs a number of
*.tfrecordfiles in your local data directory. The COCO download and conversion script takes approximately 1 hour to complete.
Copy the data to your Cloud Storage bucket
After you convert the data into TFRecords, copy them from local storage to your Cloud Storage bucket using the
gsutilcommand. You must also copy the annotation files. These files help validate the model's performance.
(vm)$ gsutil -m cp ./data/dir/coco/*.tfrecord ${DATA_DIR} (vm)$ gsutil cp ./data/dir/coco/raw-data/annotations/*.json ${DATA_DIR}
Clean up the VM resources
Once the COCO dataset has been converted to TFRecords and copied to the DATA_DIR on your Cloud Storage bucket, you can delete the Compute Engine instance.
Disconnect from the Compute Engine instance:
(vm)$ exit
Your prompt should now be
username@projectname, showing you are in the Cloud Shell.
Delete your Compute Engine instance.
$ gcloud compute instances delete instance-name --zone=europe-west4-a
Cloud TPU single device training
Open a Cloud Shell window.
Create a variable for your project's ID.
export PROJECT_ID=project-id
Configure Google Cloud CLI to use the project where you want to create Cloud TPU.
gcloud config set project ${PROJECT_ID}
The first time you run this command in a new Cloud Shell VM, an
Authorize Cloud Shellpage is displayed. Click
Authorizeat the bottom of the page to allow
gcloudto make Google Cloud API calls with your credentials.
Create a Service Account for the Cloud TPU project.
gcloud beta services identity create --service tpu.googleapis.com --project $PROJECT_ID
The command returns a Cloud TPU Service Account with following format:
service-PROJECT_NUMBER@cloud-tpu.iam.gserviceaccount.com
Create a Cloud Storage bucket using the following command:
gsutil mb -p ${PROJECT_ID} -c standard -l europe-west4 gs://bucket-name
This Cloud Storage bucket stores the data you use to train your model and the training results. The
gcloudcommand used in this tutorial to set up the TPU also sets up default permissions for the Cloud TPU Service Account you set up in the previous step. If you want finer-grain permissions, review the
[access level permissions](/tpu/docs/storage-buckets).
Set up and start the Cloud TPU
Launch a Compute Engine VM and Cloud TPU using the
gcloudcommand. The command you use depends on whether you are using TPU VMs or TPU nodes. For more information on the two VM architecture, see
[System Architecture](/tpu/docs/system-architecture-tpu-vm).
TPU VM
$ gcloud compute tpus tpu-vm create retinanet-tutorial \ --zone=europe-west4-a \ --accelerator-type=v3-8 \ --version=tpu-vm-tf-2.15.0-pjrt
Command flag descriptions
zone
- The
[zone](/tpu/docs/types-zones)where you plan to create your Cloud TPU.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
version
- The Cloud TPU
[software version](/tpu/docs/supported-tpu-versions#tpu_software_versions).
TPU Node
$ gcloud compute tpus execution-groups create \ --zone=europe-west4-a \ --name=retinanet-tutorial \ --accelerator-type=v3-8 \ --machine-type=n1-standard-8 \ --disk-size=300 \ --tf-version=2.12.0
Command flag descriptions
zone
- The
[zone](/tpu/docs/types-zones)where you plan to create your Cloud TPU.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
machine-type
- The
[machine type](/compute/docs/machine-types)of the Compute Engine VM to create.
disk-size
- The root volume size of your Compute Engine VM (in GB).
tf-version
- The version of TensorFlow
gcloudinstalls on the VM.
For more information on the
gcloudcommand, see the
[gcloud Reference](/sdk/gcloud/reference).
-
If you are not automatically logged in to the Compute Engine instance, log in by running the following
sshcommand. When you are logged into the VM, your shell prompt changes from
username@projectnameto
username@vm-name:
TPU VM
gcloud compute tpus tpu-vm ssh retinanet-tutorial --zone=europe-west4-a
TPU Node
gcloud compute ssh retinanet-tutorial --zone=europe-west4-a
As you continue these instructions, run each command that begins with
(vm)$in your VM session window.
Install extra packages
The RetinaNet training application requires several extra packages. Install them now:
(vm)$ sudo apt-get install -y python3-tk
(vm)$ pip3 install --user Cython matplotlib opencv-python-headless pyyaml Pillow
(vm)$ pip3 install --user 'git+https://github.com/cocodataset/cocoapi#egg=pycocotools&subdirectory=PythonAPI'
Install TensorFlow requirements.
TPU VM
(vm)$ pip3 install -r /usr/share/tpu/models/official/requirements.txt
TPU Node
(vm)$ pip3 install --user -r /usr/share/models/official/requirements.txt
Set the Cloud TPU name variable.
TPU VM
(vm)$ export TPU_NAME=local
TPU Node
(vm)$ export TPU_NAME=retinanet-tutorial
Add environment variables for the data and model directories.
(vm)$ export STORAGE_BUCKET=gs://bucket-name
(vm)$ export DATA_DIR=${STORAGE_BUCKET}/coco (vm)$ export MODEL_DIR=${STORAGE_BUCKET}/retinanet-train
Set the
PYTHONPATHenvironment variable:
TPU VM
(vm)$ export PYTHONPATH=""${PWD}/models:${PYTHONPATH}""
TPU Node
(vm)$ export PYTHONPATH=""${PYTHONPATH}:/usr/share/models""
- If you set
--version=tpu-vm-tf-2.15.0-pjrtwhen creating your TPU, set the following environment variables to enable the PJRT runtime:
(vm)$ export NEXT_PLUGGABLE_DEVICE_USE_C_API=true (vm)$ export TF_PLUGGABLE_DEVICE_LIBRARY_PATH=/lib/libtpu.so
Change to directory that stores the model:
TPU VM
(vm)$ cd /usr/share/tpu/models/official/legacy/detection
TPU Node
(vm)$ cd /usr/share/models/official/legacy/detection
Single Cloud TPU device training
The following training scripts were run on a Cloud TPU v3-8. It will take more time, but you can also run them on a Cloud TPU v2-8.
This sample script below trains for only 10 steps and takes less than 5 minutes to run on a v3-8 TPU Node. To train to convergence takes about 22,500 steps and approximately 1 1/2 hours on a Cloud TPU v3-8 TPU.
Set up the following environment variables:
(vm)$ export RESNET_CHECKPOINT=gs://cloud-tpu-checkpoints/retinanet/resnet50-checkpoint-2018-02-07 (vm)$ export TRAIN_FILE_PATTERN=${DATA_DIR}/train-* (vm)$ export EVAL_FILE_PATTERN=${DATA_DIR}/val-* (vm)$ export VAL_JSON_FILE=${DATA_DIR}/instances_val2017.json
Run the training script:
(vm)$ python3 main.py \ --strategy_type=tpu \ --tpu=${TPU_NAME} \ --model_dir=${MODEL_DIR} \ --mode=""train"" \ --params_override=""{ type: retinanet, train: { total_steps: 10, checkpoint: { path: ${RESNET_CHECKPOINT}, prefix: resnet50/ }, train_file_pattern: ${TRAIN_FILE_PATTERN} }, eval: { val_json_file: ${VAL_JSON_FILE}, eval_file_pattern: ${EVAL_FILE_PATTERN}, eval_samples: 5000 } }""
Command flag descriptions
strategy_type
- To train the RetinaNet model on a TPU, you must set the
distribution_strategyto
tpu.
tpu
- The name of the Cloud TPU. This is set using the
TPU_NAMEenvironment variable.
model_dir
- The Cloud Storage bucket where checkpoints and summaries are stored during training. You can use an existing folder to load previously generated checkpoints created on a TPU of the same size and TensorFlow version.
mode
- Set this to
trainto train the model or
evalto evaluate the model.
params_override
- A JSON string that overrides default script parameters. For more
information on script parameters, see
/usr/share/models/official/legacy/detection/main.py.
-
The model will train for 10 steps in about 5 minutes on a v3-8 TPU. When the training completes, you will see output similar to the following:
Train Step: 10/10 / loss = { 'total_loss': 2.4581615924835205, 'cls_loss': 1.4098565578460693, 'box_loss': 0.012001709081232548, 'model_loss': 2.0099422931671143, 'l2_regularization_loss': 0.44821977615356445, 'learning_rate': 0.008165999 } / training metric = { 'total_loss': 2.4581615924835205, 'cls_loss': 1.4098565578460693, 'box_loss': 0.012001709081232548, 'model_loss': 2.0099422931671143, 'l2_regularization_loss': 0.44821977615356445, 'learning_rate': 0.008165999 }
Single Cloud TPU device evaluation
The following procedure uses the COCO evaluation data. It takes about 10 minutes to run through the evaluation steps on a v3-8 TPU.
Set up the following environment variables:
(vm)$ export EVAL_SAMPLES=5000
Run the evaluation script:
(vm)$ python3 main.py \ --strategy_type=tpu \ --tpu=${TPU_NAME} \ --model_dir=${MODEL_DIR} \ --checkpoint_path=${MODEL_DIR} \ --mode=eval_once \ --params_override=""{ type: retinanet, eval: { val_json_file: ${VAL_JSON_FILE}, eval_file_pattern: ${EVAL_FILE_PATTERN}, eval_samples: ${EVAL_SAMPLES} } }""
Command flag descriptions
strategy_type
- The distribution strategy to use. Either
tpuor
multi_worker_gpu.
tpu
- The name of the Cloud TPU. This is set using the
TPU_NAMEenvironment variable.
model_dir
- The Cloud Storage bucket where checkpoints and summaries are stored during training. You can use an existing folder to load previously generated checkpoints created on a TPU of the same size and TensorFlow version.
mode
- One of
train,
eval, or
train_and_eval.
params_override
- A JSON string that overrides default script parameters. For more
information on script parameters, see
/usr/share/models/official/legacy/detection/main.py.
At the end of the evaluation, you will see messages similar to the following on the console:
Accumulating evaluation results... DONE (t=7.66s). Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.000 Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.000 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.000 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.000 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.000 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.000 Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
-
You have now completed single-device training and evaluation. Use the following steps to delete the current single-device TPU resources.
Disconnect from the Compute Engine instance:
(vm)$ exit
Your prompt should now be
username@projectname, showing you are in the Cloud Shell.
Delete the TPU resource.
TPU VM
$ gcloud compute tpus tpu-vm delete retinanet-tutorial \ --zone=europe-west4-a
Command flag descriptions
zone
- The
[zone](https://cloud.google.com/tpu/docs/types-zones)where your Cloud TPU resided.
TPU Node
$ gcloud compute tpus execution-groups delete retinanet-tutorial \ --tpu-only \ --zone=europe-west4-a
Command flag descriptions
tpu-only
- Deletes only the Cloud TPU. The VM remains available.
zone
- The
[zone](https://cloud.google.com/tpu/docs/types-zones)that contains the TPU to delete.
-
At this point, you can either conclude this tutorial and
[clean up](#cleanup),
or you can continue and explore running the model on Cloud TPU Pods.
Scale your model with Cloud TPU Pods
Training your model on Cloud TPU Pods may require some changes
to your training script. For information, see
[Training on TPU Pods](/tpu/docs/training-on-tpu-pods).
Training Retinanet on a TPU Pod
Open a Cloud Shell window.
Create a variable for your project's ID.
export PROJECT_ID=project-id
Configure Google Cloud CLI to use the project where you want to create Cloud TPU.
gcloud config set project ${PROJECT_ID}
The first time you run this command in a new Cloud Shell VM, an
Authorize Cloud Shellpage is displayed. Click
Authorizeat the bottom of the page to allow
gcloudto make Google Cloud API calls with your credentials.
Create a Service Account for the Cloud TPU project.
Service accounts allow the Cloud TPU service to access other Google Cloud services.
gcloud beta services identity create --service tpu.googleapis.com --project $PROJECT_ID
The command returns a Cloud TPU Service Account with following format:
service-PROJECT_NUMBER@cloud-tpu.iam.gserviceaccount.com
Create a Cloud Storage bucket using the following command or use a bucket you created earlier for your project.
In the following command, replace europe-west4 with the name of the
[region](/tpu/docs/types-zones-tpu-vm)you will use to run the training. Replace bucket-name with the name you want to assign to your bucket.
gsutil mb -p ${PROJECT_ID} -c standard -l europe-west4 gs://bucket-name
This Cloud Storage bucket stores the data you use to train your model and the training results. The
gcloudcommand used in this tutorial sets up default permissions for the Cloud TPU Service Account you set up in the previous step. If you want finer-grain permissions, review the
[access level permissions](/tpu/docs/storage-buckets).
The bucket location must be in the same region as your TPU resources.
If you previously prepared the COCO dataset and moved it to your storage bucket, you can use it again for Pod training. If you have not yet prepared the COCO dataset,
[prepare it now](#prepare-dataset)and return here to set up the training.
Set up and launch a Cloud TPU Pod
This tutorial specifies a v3-32 Pod. For other Pod options, see the
[available TPU types page](/tpu/docs/supported-tpu-configurations).
TPU VM
Launch a TPU VM Pod using the
gcloud compute tpus tpu-vmcommand. This tutorial specifies a v3-32 Pod. For other Pod options, see the
[available TPU types page](/tpu/docs/supported-tpu-configurations).
$ gcloud compute tpus tpu-vm create retinanet-tutorial \ --zone=europe-west4-a \ --accelerator-type=v3-32 \ --version=tpu-vm-tf-2.15.0-pod-pjrt
Command flag descriptions
zone
- The
[zone](/tpu/docs/types-zones)where you plan to create your Cloud TPU.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
version
- The Cloud TPU
[software version](/tpu/docs/supported-tpu-versions#tpu_software_versions).
TPU Node
Run the
gcloud compute tpus execution-groupscommand, using the
accelerator-typeparameter to specify the Pod slice you want to use. For example, the following command uses a v3-32 Pod slice.
$ gcloud compute tpus execution-groups create \ --zone=europe-west4-a \ --name=retinanet-tutorial \ --accelerator-type=v3-32 \ --machine-type=n1-standard-8 \ --disk-size=300 \ --tf-version=2.12.0
Command flag descriptions
zone
- The
[zone](/tpu/docs/types-zones)where you plan to create your Cloud TPU.
name
- The TPU name. If not specified, defaults to your username.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
machine-type
- The
[machine type](/compute/docs/machine-types)of the Compute Engine VM to create.
tf-version
- The version of Tensorflow
gcloudinstalls on the VM.
-
If you are not automatically logged in to the Compute Engine instance, log in by running the following
sshcommand. When you are logged into the VM, your shell prompt changes from
username@projectnameto
username@vm-name:
TPU VM
gcloud compute tpus tpu-vm ssh retinanet-tutorial --zone=europe-west4-a
TPU Node
gcloud compute ssh retinanet-tutorial --zone=europe-west4-a
Set the Cloud TPU name variable.
(vm)$ export TPU_NAME=retinanet-tutorial
Set Cloud Storage bucket variables
Set up the following environment variables, replacing bucket-name with the name of your Cloud Storage bucket:
(vm)$ export STORAGE_BUCKET=gs://bucket-name
(vm)$ export MODEL_DIR=${STORAGE_BUCKET}/retinanet-train (vm)$ export DATA_DIR=${STORAGE_BUCKET}/coco
The training application expects your training data to be accessible in Cloud Storage. The training application also uses your Cloud Storage bucket to store checkpoints during training.
Install extra packages
The RetinaNet training application requires several extra packages. Install them now:
(vm)$ sudo apt-get install -y python3-tk (vm)$ pip3 install --user Cython matplotlib opencv-python-headless pyyaml Pillow (vm)$ pip3 install --user 'git+https://github.com/cocodataset/cocoapi#egg=pycocotools&subdirectory=PythonAPI'
Install TensorFlow requirements.
TPU VM
(vm)$ pip3 install -r /usr/share/tpu/models/official/requirements.txt
TPU Node
(vm)$ pip3 install --user -r /usr/share/models/official/requirements.txt
Set some required environment variables:
(vm)$ export RESNET_PRETRAIN_DIR=gs://cloud-tpu-checkpoints/retinanet/resnet50-checkpoint-2018-02-07 (vm)$ export TRAIN_FILE_PATTERN=${DATA_DIR}/train-* (vm)$ export EVAL_FILE_PATTERN=${DATA_DIR}/val-* (vm)$ export VAL_JSON_FILE=${DATA_DIR}/instances_val2017.json
Set the
PYTHONPATHenvironment variable:
TPU VM
(vm)$ export PYTHONPATH=""${PWD}/models:${PYTHONPATH}"" (vm)$ export TPU_LOAD_LIBRARY=0
TPU Node
(vm)$ export PYTHONPATH=""${PYTHONPATH}:/usr/share/models""
Change to directory that stores the model:
TPU VM
(vm)$ cd /usr/share/tpu/models/official/legacy/detection
TPU Node
(vm)$ cd /usr/share/models/official/legacy/detection
Train the model
TPU VM
(vm)$ python3 main.py \ --strategy_type=tpu \ --tpu=${TPU_NAME} \ --model_dir=${MODEL_DIR} \ --mode=train \ --model=retinanet \ --params_override=""{architecture: {use_bfloat16: true}, eval: {batch_size: 40, eval_file_pattern: ${EVAL_FILE_PATTERN}, val_json_file: ${VAL_JSON_FILE}}, postprocess: {pre_nms_num_boxes: 1000}, predict: {batch_size: 40}, train: {batch_size: 256, checkpoint: {path: ${RESNET_PRETRAIN_DIR}, prefix: resnet50/}, iterations_per_loop: 5000, total_steps: 5625, train_file_pattern: ${TRAIN_FILE_PATTERN}, } }""
Command flag descriptions
tpu
- The name of your TPU.
model_dir
- Specifies the directory where checkpoints and summaries are stored
during model training. If the folder is missing, the program creates
one. When using a Cloud TPU, the
model_dirmust be a Cloud Storage path (
gs://...). You can reuse an existing folder to load current checkpoint data and to store additional checkpoints as long as the previous checkpoints were created using Cloud TPU of the same size and TensorFlow version.
params_override
- A JSON string that overrides default script parameters. For more
information on script parameters, see
/usr/share/tpu/models/official/legacy/detection/main.py.
This procedure trains the model on the COCO dataset for 5625 training steps. This training takes approximately 20 minutes on a v3-32 Cloud TPU. When the training completes, a message similar to the following appears:
TPU Node
The following sample training script was run on a Cloud TPU v3-32 Pod. It trains for only 10 steps and takes less than 5 minutes to run. To train to convergence requires 2109 steps and takes approximately 50 minutes on a v3-32 TPU Pod.
(vm)$ python3 main.py \ --strategy_type=tpu \ --tpu=${TPU_NAME} \ --model_dir=${MODEL_DIR} \ --mode=""train"" \ --params_override=""{ type: retinanet, train: { total_steps: 10, batch_size: 256, checkpoint: { path: ${RESNET_CHECKPOINT}, prefix: resnet50/ }, train_file_pattern: ${TRAIN_FILE_PATTERN} }, eval: { val_json_file: ${VAL_JSON_FILE}, eval_file_pattern: ${EVAL_FILE_PATTERN}, eval_samples: 5000 } }""
Command flag descriptions
strategy_type
- The distribution strategy to use. Either
tpuor
multi_worker_gpu.
tpu
- Specifies the name of the Cloud TPU. This is set
using the
TPU_NAMEenvironment variable.
model_dir
- The Cloud Storage bucket where checkpoints and summaries are stored during training. You can use an existing folder to load previously generated checkpoints created on a TPU of the same size and TensorFlow version.
mode
- One of
train,
eval, or
train_and_eval.
params_override
- A JSON string that overrides default script parameters. For more
information on script parameters, see
/usr/share/models/official/legacy/detection/main.py.
-
When the training completes, a message similar to the following appears:
TPU VM
Train Step: 5625/5625 / loss = {'total_loss': 0.730501651763916, 'cls_loss': 0.3229793608188629, 'box_loss': 0.003082591574639082, 'model_loss': 0.4771089553833008, 'l2_regularization_loss': 0.2533927261829376, 'learning_rate': 0.08} / training metric = {'total_loss': 0.730501651763916, 'cls_loss': 0.3229793608188629, 'box_loss': 0.003082591574639082, 'model_loss': 0.4771089553833008, 'l2_regularization_loss': 0.2533927261829376, 'learning_rate': 0.08}
TPU Node
Train Step: 10/10 / loss = {'total_loss': 3.5455241203308105, 'cls_loss': 1.458828330039978, 'box_loss': 0.01220895815640688, 'model_loss': 2.0692763328552246, 'l2_regularization_loss': 1.4762479066848755, 'learning_rate': 0.008165999} / training metric = {'total_loss': 3.5455241203308105, 'cls_loss': 1.458828330039978, 'box_loss': 0.01220895815640688, 'model_loss': 2.0692763328552246, 'l2_regularization_loss': 1.4762479066848755, 'learning_rate': 0.008165999}
Clean up
To avoid incurring charges to your Google Cloud account for the resources used in this tutorial, either delete the project that contains the resources, or keep the project and delete the individual resources.
Disconnect from the Compute Engine VM:
(vm)$ exit
Your prompt should now be
username@projectname, showing you are in the Cloud Shell.
Delete your Cloud TPU and Compute Engine resources. The command you use to delete your resources depends upon whether you are using TPU VMs or TPU Nodes. For more information, see
[System Architecture](/tpu/docs/system-architecture-tpu-vm).
TPU VM
$ gcloud compute tpus tpu-vm delete retinanet-tutorial \ --zone=europe-west4-a
TPU Node
$ gcloud compute tpus execution-groups delete retinanet-tutorial \ --zone=europe-west4-a
Verify the resources have been deleted by running
gcloud compute tpus execution-groups list. The deletion might take several minutes. A response like the one below indicates your instances have been successfully deleted.
$ gcloud compute tpus execution-groups list --zone=europe-west4-a
Listed 0 items.
Delete your Cloud Storage bucket using
gsutilas shown below. Replace bucket-name with the name of your Cloud Storage bucket.
$ gsutil rm -r gs://bucket-name
What's next
The TensorFlow Cloud TPU tutorials generally train the model using a
sample dataset. The results of this training are not usable
for inference. To use a model for inference, you can train the data on a
publicly available dataset or your own data set. TensorFlow models
trained on Cloud TPUs generally require datasets to be in
[TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) format.
You can use the
[dataset conversion tool sample](https://cloud.google.com/tpu/docs/classification-data-conversion) to convert an image classification dataset into TFRecord format. If you are not using an image classification model, you will have to convert your dataset to [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) format yourself. For more information, see [TFRecord and tf.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord).
Hyperparameter tuning
To improve the model's performance with your dataset, you can tune
the model's hyperparameters. You can find information about hyperparameters
common to all TPU supported models on
[GitHub](https://github.com/tensorflow/tpu/tree/master/models/hyperparameters).
Information about model-specific hyperparameters can be found in the
[source code](https://github.com/tensorflow/tpu/tree/master/models/official)
for each model. For more information on hyperparameter tuning, see
[Overview of hyperparameter tuning](https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview),
[Using the Hyperparameter tuning service](https://cloud.google.com/ai-platform/training/docs/using-hyperparameter-tuning),
and [Tune hyperparameters](https://developers.google.com/machine-learning/guides/text-classification/step-5).
Inference
Once you have trained your model you can use it for inference
(also called prediction).
[AI Platform](https://cloud.google.com/ai-platform/docs/technical-overview)
is a cloud-based solution for developing,
[training](https://cloud.google.com/ai-platform/training/docs),
and [deploying](https://cloud.google.com/ai-platform/prediction/docs/deploying-models)
machine learning models. Once a model is deployed, you can use the
[AI Platform Prediction service](https://cloud.google.com/ai-platform/prediction/docs).
Train with different image sizes
You can explore using a larger backbone network (for example, ResNet-101 instead of ResNet-50). A larger input image and a more powerful backbone will yield a slower but more precise model.
Use a different basis
Alternatively, you can explore pre-training a ResNet model on your own dataset and using it as a basis for your RetinaNet model. With some more work, you can also swap in an alternative backbone network in place of ResNet. Finally, if you are interested in implementing your own object detection models, this network may be a good basis for further experimentation.",Training RetinaNet on Cloud TPU (TF 2.x) | Google Cloud,
id,url,body,title,description
95,https://cloud.google.com/tpu/docs/tensorflow-performance-guide,"TensorFlow performance guide
Overview
To profile a TensorFlow model on Cloud TPUs, you use TensorBoard and the TPU
TensorBoard plug in. TensorBoard is preinstalled on TPU VMs. For information on
how to install the TPU TensorBoard plug in and capture a performance profile,
see
[Profile your model with TPU tools](/tpu/docs/cloud-tpu-tools). For general
TPU performance information, see [Cloud TPU performance guide](/tpu/docs/performance-guide).
For more information, see
[TensorBoard callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard).
TensorFlow function performance notes
See the full list of
[TensorFlow operations](https://cloud.google.com/tpu/docs/tensorflow-ops)
available on Cloud TPU.
tf.matmul
- Transposing the result of either of the operands is effectively free.
- Note that
tf.matmulsupports fusing into its input and output. This means that activation functions or biases applied directly to the output of
tf.matmulhave low overhead.
tf.nn.conv_n_d,
tf.nn.depthwise_conv2d,
tf.nn.separable_conv2d
- For the activations, the batch and feature dimensions are padded to a
multiple of either 8 or 128.
- First XLA tracks the most common size of batch dimensions for convolutions in the module. This helps distinguish between forward convolutions, activation gradient convolutions, and kernel gradient convolutions.
- If the most common batch size is greater than or equal to 64:
- Batch is padded to a multiple of 128 and feature padded to a multiple of 8 for forwards and backwards convolutions.
- Batch is padded to a multiple of 8 and feature padded to a multiple of 128 for gradient update convolutions.
- If the most common batch size is less than 64:
- Batch is padded to a multiple of 8 and feature padded to a multiple of 128 for forwards and backwards convolutions.
- Batch is padded to a multiple of 128 and feature padded to a multiple of 8 for gradient update convolutions.
- Transposing the activations right before sending it to a convolution is free if the transpose only swaps the input feature and batch dimensions.
- For the kernel, the input feature and output feature dimensions are padded
to a multiple of either 8 or 128. The exact determination is influenced by
the producers and other consumers of the kernel.
- Transposing a kernel right before sending it to a convolution is free if the transpose only swaps the input and output feature dimensions.
- For the result, the batch and feature dimensions are padded to a multiple of
either 8 or 128.
- Transposing the result of a convolution is free if the transpose only swaps the batch and output feature dimensions.
- Note that
tf.nn.conv_n_dsupports fusing into its result, the activations and/or the kernel. This means that activation functions or biases applied directly to the output have low overhead.
tf.nn.avg_pool,
tf.nn.max_pool
- The padding rules apply: spatial dimensions are more major than batch and feature. Each of batch and feature may be padded to a multiple of either 8 or 128.
- Typically, the layout of a pool operation matches the convolutions that flow in or out of it.
- The gradient calculation for
tf.nn.max_pool may be slower than their
tf.nn.avg_poolequivalent. Consider switching from max-pooling to average-pooling when possible.
tf.concat,
tf.slice,
tf.strided_slice
- Avoid unnecessary slices and concatenations. Slices and concatenations in a
dimension that has been padded is considerably more expensive.
- Data movement is minimized if the slice dimension has no padding overhead.
tf.transpose
- Transposing any of the operands of a
tf.matmulor its result are free.
- Transposing the activations of a
tf.conv_n_dis free if it swaps the batch and input feature dimensions.
- Transposing the kernel of a
tf.conv_n_dis free if it swaps the input and output feature dimensions.
- Transposing the result of a
tf.conv_n_dis free if it swaps the batch and output feature dimensions.
tf.batch_to_space,
tf.space_to_batch,
tf.space_to_depth,
tf.depth_to_space
- These are costly because they involve moving data from padded to unpadded dimensions and vice-versa.
tf.reshape
- Reshaping may be costly on Cloud TPU when moving around data in a padded dimension.
- It can be beneficial to reshape data to R1 on the host and reshape it back
to some higher dimension shape on the device if there is substantial
padding. This can make transfers between host and device more efficient.
- It can also help with peak memory utilization because the packed parameter can be unpacked on-demand.
tf.random_uniform,
tf.distributions.Bernoulli,
tf.random_normal,
tf.multinomial
- Pseudo random-number generation for uniform or Bernoulli distributions is very fast.
- Normal distributions are slightly more expensive than uniform or Bernoulli distributions.
- Pseudo random-number generation for Categorical/Multinomial distributions is considerably more expensive.
tf.reduce_all,
tf.reduce_any,
tf.reduce_logsumexp,
tf.reduce_max,
tf.reduce_min,
tf.reduce_prod,
tf.reduce_sum
- Multiple reductions with the same input and output shape can be performed in
parallel via fusion.
- Try to rewrite sequential chains of reductions into parallel ones, if possible.
Reductions support fusing elementwise operations into their input but not their output. When possible, rewrite expressions to promote fusion. For example:
tf.multiply(tf.reduce_sum(x), y)
Into:
tf.reduce_sum(tf.multiply(x, y))
tf.nn.batch_normalization,
tf.nn.fused_batch_norm,
tf.layers.batch_normalization
- The XLA compiler can efficiently lower TensorFlow's fused
variants of batch normalization. Using them can be considerably more
efficient than the alternative.
- Prefer
tf.nn.fused_batch_normover
tf.nn.batch_normalization.
- For
tf.layers.batch_normalization, set the ""fused"" argument to true.
- Prefer",TensorFlow performance guide | Cloud TPU | Google Cloud,
id,url,body,title,description
80,https://cloud.google.com/tpu/docs/reference/rest/v1alpha1/projects.locations.operations/delete,"Send feedback
Method: projects.locations.operations.delete
Stay organized with collections
Save and categorize content based on your preferences.
Deletes a long-running operation. This method indicates that the client is no longer interested in the operation result. It does not cancel the operation. If the server doesn't support this method, it returns
google.rpc.Code.UNIMPLEMENTED.
HTTP request
DELETE https://tpu.googleapis.com/v1alpha1/{name=projects/*/locations/*/operations/*}
The URL uses
gRPC Transcoding syntax.
Path parameters
Parameters
name
string
The name of the operation resource to be deleted.
Request body
The request body must be empty.
Response body
If successful, the response body is empty.
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
Authentication Overview.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]
Need to tell us more?",Method: projects.locations.operations.delete | Cloud TPU | Google Cloud,
id,url,body,title,description
189,https://cloud.google.com/tpu/docs/managing-tpus-tpu-vm,"Manage TPU resources
This page describes how to manage Cloud TPU resources using:
- The
[Google Cloud CLI](https://cloud.google.com/sdk/gcloud/reference/compute/tpus/), which provides the primary CLI to Google Cloud.
- The
[Google Cloud console](https://console.cloud.google.com/), which provides an integrated management console for your Google Cloud resources.
Cloud TPU has two VM architectures, TPU Node and TPU VM. The two VM
architectures are described in
[System Architecture](https://cloud.google.com/tpu/docs/system-architecture).
You can use the
gcloud commands described in this document with
both TPU configurations. The
gcloud commands you
use depend on the TPU configuration you are using. Each
gcloud command is
shown in a tabbed section. Choose the tab for the TPU configuration you want to
use and the web page shows the appropriate
gcloud command. Unless you know you
need to use TPU Nodes, we recommend using TPU VMs. For Cloud TPU v4, only the
TPU VM architecture is supported.
Prerequisites
Before you run these procedures, you must install the Google Cloud CLI,
create a Google Cloud project, and enable the Cloud TPU API.
For instructions, see
[Set up a project and enable the Cloud TPU API](/tpu/docs/setup-gcp-account).
If you are using the Google Cloud CLI, you can use the Google Cloud Shell,
a Compute Engine VM, or install the Google Cloud CLI locally. The Google
Cloud Shell allows you to interact with Cloud TPUs without having to install
any software. The Google Cloud Shell may disconnect after a period of
inactivity. If you're running long-running commands, we recommend installing the
Google Cloud CLI on your local machine. For more information on the Google Cloud CLI,
see the
[. gcloud Reference](/sdk/gcloud/reference)
TPUs are not available in all regions. For more information, see
[Cloud TPU regions and zones](/tpu/docs/regions-zones).
Creating a Cloud TPU
You can create a Cloud TPU using
gcloud or the Google Cloud console. You can also
make a request to the Cloud TPU API using
curl.
Creating a Cloud TPU using
gcloud
The commands you use depend on whether you are using the TPU VM or TPU Node
architecture. For more information, see
[System Architecture](/tpu/docs/system-architecture-tpu-vm).
TPU VMs
You can specify TPU configurations in terms of TensorCores or TPU chips.
For more information, see the section for the TPU version you are using in
[System architecture](/tpu/docs/system-architecture-tpu-vm#tpu-v4-config).
The following command uses a TensorCore-based configuration:
$ gcloud compute tpus tpu-vm create tpu-name \
--zone=us-central2-b \
--accelerator-type=v4-8 \
--version=tpu-software-version
Command flag descriptions
zone
- The
[zone](/tpu/docs/types-zones-tpu-vm)where you plan to create your Cloud TPU.
accelerator-type
- The
[type](/tpu/docs/types-topologies#tpu-types)of the Cloud TPU to create.
version
- The TPU software
[version.](/tpu/docs/supported-tpu-versions#tpu_software_versions)
shielded-secure-boot(optional)
- Specifies that the TPU instances are created with secure boot enabled.
This implicitly makes them Shielded VM instances. See
[What is shielded VM?](/compute/shielded-vm/docs/shielded-vm)for more details.
The following command creates a TPU using a chip-based configuration:
$ gcloud compute tpus tpu-vm create tpu-name \
--zone=us-central2-b \
--type=v4 \
--topology=2x2x1 \
--version=tpu-software-version
Required flags
tpu-name
- The name of the TPU VM you are creating.
zone
- The
[zone](/tpu/docs/types-zones-tpu-vm)where you are creating your Cloud TPU.
type
- For more information on supported TPU types, see
[TPU types](/tpu/docs/supported-tpu-configurations).
topology
- See the
[topology](/tpu/docs/types-topologies#v4-topologies)section for the supported topologies.
version
- The TPU software version you want to use. For more information, see
[TPU software versions](/tpu/docs/supported-tpu-configurations#tpu_software_versions)
For more information on supported TPU types and topologies, see
[Types and topologies](/tpu/docs/types-topologies).
TPU Nodes
$ gcloud compute tpus execution-groups create --name=tpu-name \
--zone=us-central1-a \
--tf-version=2.12.0 \
--machine-type=n1-standard-1 \
--accelerator-type=v3-8
Command flag descriptions
zone
- The
[zone](/tpu/docs/tpu/docs/types-zones-tpu-vm)where you plan to create your Cloud TPU.
tf-version
- The version of Tensorflow the
gcloudcommand installs on your VM.
machine-type
- The
[machine type](/compute/docs/machine-types)of the Compute Engine VM to create.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
Creating a Cloud TPU queued resource using
gcloud
Using
gcloud you can also create a Cloud TPU as a queued resource. When you
make a request for a queued resource, your request is added to a queue managed
by Cloud TPU. When a resource becomes available, the resource is allocated and
is available for your exclusive use. For more information see,
[Cloud TPU queued resources](/tpu/docs/queued-resources).
Run a startup script
You can run a startup script on each TPU VM by specifying the
--metadata startup-script parameter when creating the TPU VM. The following
command creates a TPU VM using a startup script.
$ gcloud compute tpus tpu-vm create tpu-name \
--zone=us-central2-b \
--accelerator-type=tpu-type \
--version=tpu-vm-tf-2.15.0-pjrt \
--metadata startup-script='#! /bin/bash
pip3 install numpy
EOF'
After the TPU VM is created, you can view the logs from the startup script by
[connecting to the TPU VM](#tpu-connect) using
SSH and running:
$ cat /var/log/syslog | grep startup-script
Creating a Cloud TPU in the Google Cloud console
- Navigate to the
[Google Cloud console](https://console.cloud.google.com/).
- From the navigation menu, select Compute Engine > TPUs.
- Click CREATE TPU NODE.
- In the Name box, type a TPU instance name.
- In the Zone box, select the zone in which to create the TPU.
- Under TPU settings, select either TPU VM architecture or TPU node architecture.
The TPU configuration determines whether you create the TPU as a TPU VM or a
TPU Node. For more information, see
[System Architecture](/tpu/docs/system-architecture-tpu-vm).
- For TPU type, select the
[TPU type](/tpu/docs/types-zones)you want to create.
- For TPU software version, select the software version. When creating a
Cloud TPU VM, the TPU software version specifies the version of the TPU
runtime to install. When creating a Cloud TPU Node, the TPU software
version allows you to choose the ML framework installed on the node's VM.
No other settings are required. For more information, see
[Supported Models](/tpu/docs/tutorials/supported-models).
- Click CREATE to create your resources.
Creating a Cloud TPU VM using
curl
The following command uses
curl to create a TPU.
$ curl -X POST -H ""Authorization: Bearer $(gcloud auth print-access-token)"" -H ""Content-Type: application/json"" -d ""{accelerator_type: 'v4-8', \
runtime_version:'tpu-vm-tf-2.15.0-pjrt', \
network_config: {enable_external_ips: true}, \
shielded_instance_config: { enable_secure_boot: true }}"" \
https://tpu.googleapis.com/v2/projects/project-id/locations/us-central2-b/nodes?node_id=node_name
Required fields
runtime_version
- The Cloud TPU runtime version that you want to use.
project
- The name of your enrolled Google Cloud project.
zone
- The
[zone](/tpu/docs/types-zones-tpu-vm)where you're creating your Cloud TPU.
node_name
- The name of the TPU VM you're creating.
Connecting to a Cloud TPU
You can connect to a TPU using SSH.
TPU VMs
When using TPU VMs, you must explicitly SSH into your TPU using
SSH-in-browser (
[Preview](/products#product-launch-stages)) or the
Google Cloud CLI:
Use SSH-in-browser by doing the following:
In the Google Cloud console, go to the TPUs page:
In the list of TPU VMs, click SSH in the row of the TPU VM that you want to connect to.
-
Use the gcloud CLI by running the
[:](/sdk/gcloud/reference/compute/tpus/tpu-vm/ssh)
gcloud compute tpus tpu-vm sshcommand
$ gcloud compute tpus tpu-vm ssh tpu-name --zone=zone
To connect to other TPU VMs associated with the TPU Pod, append
--worker worker-numberto the command, where
worker-numberis a 0-based index.
TPU Nodes
By default, the
gcloud command you use to create TPU Nodes automatically
attempts to SSH into your TPU node. If you are using TPU Nodes and are not
connected to the Compute Engine instance by the
gcloud command, you can
connect by running the following:
$ gcloud compute ssh tpu-name \
--zone=zone
Listing your Cloud TPU resources
You can list all of your Cloud TPU in a specified zone.
Listing your Cloud TPU resources using
gcloud
The commands you use depend on whether you are using TPU VMs or TPU Nodes. For
more information, see
[System Architecture](/tpu/docs/system-architecture-tpu-vm).
TPU VMs
$ gcloud compute tpus tpu-vm list --zone=zone
TPU Nodes
$ gcloud compute tpus execution-groups list --zone=zone
This command lists the Cloud TPU resources in the specified zone. If no resources are currently set up, the output will just show dashes for the VM and TPU. If one resource is active and the other is not, you will see a message saying the status is unhealthy. You need to start or restart whichever resource is not running.
Listing your Cloud TPU resources in the Google Cloud console
Navigate to the
[Google Cloud console](https://console.cloud.google.com/).
From the navigation menu, select Compute Engine > TPUs. The console displays the TPUs page.
Retrieving information about your Cloud TPU
You can retrieve information about a specified Cloud TPU.
Retrieve information about a Cloud TPU using
gcloud
The commands you use depend on whether you are using TPU VMs or TPU Nodes. For
more information, see
[System Architecture](/tpu/docs/system-architecture-tpu-vm).
TPU VMs
$ gcloud compute tpus tpu-vm describe tpu-name \
--zone=zone
TPU Nodes
$ gcloud compute tpus execution-groups describe tpu-name \
--zone=zone
Retrieve information about a Cloud TPU in the Google Cloud console
- Navigate to the
[Google Cloud console](https://console.cloud.google.com/).
- From the navigation menu, select Compute Engine > TPUs. The console displays the TPUs page.
- Click the name of your Cloud TPU. The Cloud TPU detail page is displayed.
Stopping your Cloud TPU resources
You can stop a single Cloud TPU to stop incurring charges without losing your VM's
configuration and software. Stopping TPU Pods or TPUs allocated through the
[queued resources](/tpu/docs/queued-resources) API is not supported. To stop
incurring charges for TPUs allocated through the queued resources API, you must
[delete](#tpu-delete) the TPU.
Stopping a Cloud TPU using
gcloud
The commands you use for stopping a Cloud TPU depend on whether you are using
TPU VMs or TPU Nodes. For more information, see
[System Architecture](/tpu/docs/system-architecture-tpu-vm).
TPU VMs
$ gcloud compute tpus tpu-vm stop tpu-name \
--zone=zone
TPU Nodes
$ gcloud compute tpus stop tpu-name \
--zone=zone
Stopping a Cloud TPU in the Google Cloud console
Navigate to the
[Google Cloud console](https://console.cloud.google.com/).
From the navigation menu, select Compute Engine > TPUs. The console displays the TPUs page.
Select the checkbox next to your Cloud TPU and click Stop.
Starting your Cloud TPU resources
You can start a Cloud TPU when it is stopped.
Starting a Cloud TPU using
gcloud
You can start a stopped Cloud TPU to resume using it.
The command you use for starting a stopped Cloud TPU depend on whether you are using
TPU VMs or TPU Nodes. For more information, see
[System Architecture](/tpu/docs/system-architecture-tpu-vm).
TPU VMs
$ gcloud compute tpus tpu-vm start tpu-name --zone=zone
TPU Nodes
$ gcloud compute tpus start tpu-name --zone=zone
Starting a Cloud TPU in the Google Cloud console
Navigate to the
[Google Cloud console](https://console.cloud.google.com/).
From the navigation menu, select Compute Engine > TPUs. The console displays the TPUs page.
Select the checkbox next to your Cloud TPU and click Start.
Deleting your Compute Engine VM and Cloud TPU resources
You can delete your Cloud TPU when you are done using it.
Deleting a Cloud TPU using
gcloud
The command you use depends on whether you are using TPU VMs or TPU Nodes.
For more information, see
[System Architecture](/tpu/docs/system-architecture-tpu-vm).
TPU VMs
$ gcloud compute tpus tpu-vm delete tpu-name \
--zone=zone
Command flag descriptions
zone
- The
[zone](https://cloud.google.com/tpu/docs/types-zones-tpu-vm)where you plan to delete your Cloud TPU.
TPU Nodes
$ gcloud compute tpus execution-groups delete tpu-name \
--zone=zone
Command flag descriptions
zone
- The
[zone](https://cloud.google.com/tpu/docs/types-zones)where you plan to delete your Cloud TPU.
Deleting a Cloud TPU in the Google Cloud console
Navigate to the
[Google Cloud console](https://console.cloud.google.com/).
From the navigation menu, select Compute Engine > TPUs. The console displays the TPUs page.
Select the checkbox next to your Cloud TPU and click Delete.
Advanced Configurations
Custom Network Resources
When you create the TPU, you can choose to specify the network and/or a
subnetwork. You can do this either by submitting a
gcloud command or a
curl
call.
To specify the network or subnetwork in the
gcloud CLI, use:
--network [NETWORK] --subnetwork [SUBNETWORK]
To specify the network or subnetwork in a
curl call, use:
network_config: {network: '[NETWORK]', subnet: '[SUBNETWORK]', enable_external_ips: true}
Network
You can optionally specify the network to use for the TPU. If not specified,
the
default network is used.
Valid network formats:
https://www.googleapis.com/compute/{version}/projects/{proj-id}/global/networks/{network} compute/{version}/projects/{proj-id}/global/networks/{network} compute/{version}/projects/{proj-##}/global/networks/{network} projects/{proj-id}/global/networks/{network} projects/{proj-##}/global/networks/{network} global/networks/{network} {network}
Subnetwork
You can specify the subnetwork to use a specific subnetwork. The specified subnetwork needs to be in the same region as the zone where the TPU runs.
Valid Formats:
https://www.googleapis.com/compute/{version}/projects/{proj-id}/regions/{region}/subnetworks/{subnetwork} compute/{version}/projects/{proj-id}/regions/{region}/subnetworks/{subnetwork} compute/{version}/projects/{proj-##}/regions/{region}/subnetworks/{subnetwork} projects/{proj-id}/regions/{region}/subnetworks/{subnetwork} projects/{proj-##}/regions/{region}/subnetworks/{subnetwork} regions/{region}/subnetworks/{subnetwork} {subnetwork}
Private Google Access
In order to SSH into the TPU VMs, you need to either add access configs for the
TPU VMs, or turn on the
[Private Google Access](https://cloud.google.com/vpc/docs/configure-private-google-access)
for the subnetwork to which the TPU VMs are connected.
To add access configs,
enable_external_ips must be set. When you
[create a TPU](#tpu-setup),
enable_external_ips is set by default. If you want to opt out, specify the
following command:
--internal-ips
Or use a
curl call:
network_config: {enable_external_ips: true}
After you have configured Private Google Access,
[connect to the VM via SSH](#tpu-connect).
Custom Service Account
Each TPU VM has an associated service account it uses to make API requests
on your behalf. TPU VMs use this service account to call Cloud TPU
APIs, access Cloud Storage and other services. By default, your TPU VM uses the
[default Compute Engine service account](/compute/docs/access/service-accounts#default_service_account).
You can specify a custom service account when creating a TPU VM using the
--service-account flag. The service account must be defined in the same
Google Cloud project where you create your TPU VM. Custom service accounts used
for TPU VMs must have the
[TPU Viewer](/iam/docs/understanding-roles#tpu.viewer)
role to call the Cloud TPU API. If the code running in your TPU VM calls other
Google Cloud services, it must have the roles necessary to access those services.
When you create a TPU, you can choose to specify a custom service account
using the
--service-account flag. For more information about service accounts,
see
[Service Accounts](/compute/docs/access/service-accounts#serviceaccount).
Use the following commands to specify a custom service account.
Create a TPU VM using the
gcloud CLI
$ gcloud compute tpus tpu-vm create tpu-name \
--zone=us-central2-b \
--accelerator-type=tpu-type \
--version=tpu-vm-tf-2.15.0-pjrt \
--service-account=your-service-account
Create a TPU VM using
curl
$ curl -X POST -H ""Authorization: Bearer $(gcloud auth print-access-token)"" -H ""Content-Type: application/json"" -d ""{accelerator_type: 'v4-8', \
runtime_version:'tpu-vm-tf-2.15.0-pjrt', \
network_config: {enable_external_ips: true}, \
shielded_instance_config: { enable_secure_boot: true }}"" \
service_account: {email: 'your-service-account'} \
https://tpu.googleapis.com/v2/projects/project-id/locations/us-central2-b/nodes?node_id=node_name
To use a custom service account, you need to authorize the service account for
your Google Cloud Storage buckets. For more information, see
[Connecting to Cloud Storage buckets](/tpu/docs/storage-buckets#authorize_the_service_account).
Custom VM SSH methods
Set up a firewall for SSH.
The default network comes preconfigured to allow SSH access to all VMs. If you don't use the default network, or the default network settings were edited, you may need to explicitly enable SSH access by adding a firewall-rule:
$ gcloud CLI compute firewall-rules create \ --network=network allow-ssh \ --allow=tcp:22
SSH into the TPU VMs.
$ gcloud compute tpus tpu-vm ssh tpu-name \ --zone=us-central2-b \ --project=project-id
Required fields
tpu-name: Name of the TPU node.
zone: The location of the TPU node. Currently, only
us-central2-bis supported.
project-id: The project you created above.
For a list of optional fields, see the
[.](/sdk/gcloud/reference/compute/tpus/tpu-vm/ssh)
gcloudAPI documentation
-",Manage TPU resources | Google Cloud,
id,url,body,title,description
38,https://cloud.google.com/tpu/docs/reference/rest/v2alpha1/projects.locations/get,"Method: projects.locations.get
Stay organized with collections
Save and categorize content based on your preferences.
Gets information about a location.
HTTP request
GET https://tpu.googleapis.com/v2alpha1/{name=projects/*/locations/*}
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
string
Resource name for the location.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
.
[Location](/tpu/docs/reference/rest/Shared.Types/ListLocationsResponse#Location)
Authorization scopes
Requires one of the following OAuth scopes:
-
https://www.googleapis.com/auth/cloud.tpu
-
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2023-08-30 UTC.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",Method: projects.locations.get | Cloud TPU | Google Cloud,
id,url,body,title,description
194,https://cloud.google.com/tpu/docs/reference/rest/v2/projects.locations.nodes/getGuestAttributes,"[HTTP request](#body.HTTP_TEMPLATE) [Path parameters](#body.PATH_PARAMETERS) [Request body](#body.request_body) [Response body](#body.response_body) [Authorization scopes](#body.aspect) [GuestAttributes](#GuestAttributes) [GuestAttributesValue](#GuestAttributesValue) [GuestAttributesEntry](#GuestAttributesEntry) [Try it!](#try-it)
Retrieves the guest attributes for the node.
HTTP request
POST https://tpu.googleapis.com/v2/{name=projects/*/locations/*/nodes/*}:getGuestAttributes
The URL uses
[gRPC Transcoding](https://google.aip.dev/127) syntax.
Path parameters
|Parameters
|
name
|
Required. The resource name.
Request body
The request body contains data with the following structure:
|JSON representation
|
{ ""queryPath"": string, ""workerIds"": [ string ] }
|Fields
|
queryPath
|
The guest attributes path to be queried.
|
workerIds[]
|
The 0-based worker ID. If it is empty, all workers' GuestAttributes will be returned.
Response body
Response for
.
[nodes.getGuestAttributes](/tpu/docs/reference/rest/v2/projects.locations.nodes/getGuestAttributes#google.cloud.tpu.v2.Tpu.GetGuestAttributes)
If successful, the response body contains data with the following structure:
|JSON representation
|
{
""guestAttributes"": [
{
object (
|Fields
|
guestAttributes[]
|
The guest attributes for the TPU workers.
Authorization scopes
Requires one of the following OAuth scopes:
https://www.googleapis.com/auth/cloud.tpu
https://www.googleapis.com/auth/cloud-platform
For more information, see the
[Authentication Overview](https://cloud.google.com/docs/authentication/).
GuestAttributes
A guest attributes.
|JSON representation
|
{
""queryPath"": string,
""queryValue"": {
object (
|Fields
|
queryPath
|
The path to be queried. This can be the default namespace ('/') or a nested namespace ('/<namespace>/') or a specified key ('/<namespace>/<key>')
|
queryValue
|
The value of the requested queried path.
GuestAttributesValue
Array of guest attribute namespace/key/value tuples.
|JSON representation
|
{
""items"": [
{
object (
|Fields
|
items[]
|
The list of guest attributes entries.
GuestAttributesEntry
A guest attributes namespace/key/value entry.
|JSON representation
|
{ ""namespace"": string, ""key"": string, ""value"": string }
|Fields
|
namespace
|
Namespace for the guest attribute entry.
|
key
|
Key for the guest attribute entry.
|
value
|
Value for the guest attribute entry.",Method: projects.locations.nodes.getGuestAttributes | Cloud TPU | Google Cloud,
id,url,body,title,description
125,https://cloud.google.com/tpu/docs/bfloat16,"The bfloat16 numerical format
Using reduced-precision floating point numbers is a common method used to
decrease time to convergence without losing accuracy. TPUs use the
bfloat16
number format when performing matrix operations. Matrix multiplication operations
are performed on
bfloat16 values and accumulations are performed on IEEE
float32 values.
bfloat16 is a custom 16-bit floating point format for machine learning that is
composed of one sign bit, eight exponent bits, and seven mantissa bits. The
following diagram shows the internals of three floating point formats:
float32: IEEE single-precision,
float16: IEEE half-precision, and
bfloat16.
The dynamic range of
bfloat16 and
float32 are equivalent. However,
bfloat16
takes up half the memory space. For more information about
bfloat16 performance,
see
[A Study of BFLOAT16 for Deep Learning Training](https://arxiv.org/abs/1905.12322).
Choosing bfloat16
The Google hardware team chose
bfloat16 for Cloud TPUs to improve hardware
efficiency while maintaining the ability to train deep learning models accurately,
all with minimal switching costs from
float32. The physical size of a hardware
multiplier scales with the square of the mantissa width. With fewer mantissa
bits than
FP16, the
bfloat16 multipliers are about half the size in silicon of a
typical
FP16 multiplier, and they are eight times smaller than an
float32 multiplier.
Neural networks are more sensitive to the size of the exponent than the size of
the mantissa. To ensure identical behavior for underflows, overflows, and NaNs,
bfloat16 has the same exponent size as
float32.
bfloat16 handles denormals
differently from
float32, it flushes them to zero. Unlike
float16, which typically
requires special handling like loss scaling,
bfloat16 is a drop-in replacement
for
float32 when training and running deep neural networks.
Mixed-precision training
Most computations within a deep neural network can accomplish a task with the same accuracy using a lower-precision values. Some models can even reach a higher accuracy with lower-precision values.
When programming Cloud TPUs, the XLA compiler automatically converts values between
float32 and
bfloat16.
Details about Format Conversion
The format conversion from float32 to bfloat16 is automatically inserted by the
XLA compiler. On TPU, the rounding scheme in the conversion is
[round to nearest even](https://en.wikipedia.org/wiki/IEEE_754#Roundings_to_nearest)
and overflow to inf. Also, the bfloat16 on Cloud TPU does not support
subnormals, so all subnormals are flushed to zero during the conversion.
Special values, such as NaN and inf, are preserved in the conversion.
The format conversion from bfloat16 to float32 is also automatically inserted by the XLA compiler. Since float32 can represent all exact values in bfloat16, the conversion simply pads 16 zeros in the mantissa bits. Special values are preserved in the conversion.
Model portability
The values of parameters and activations in a model can be stored in 32-bit
format because the TPU hardware can automatically cast these values to
bfloat16.
Checkpoints obtained from a model trained on Cloud TPUs can be deployed on other
hardware platforms (for example, inference or fine-tuning on CPUs or GPUs)
without extensive manual conversions.
Improving performance with bfloat16
While automatic format conversion in TPUs lets you avoid thinking about
numerical precision, further performance improvements can be achieved by
explicitly casting values to
bfloat16. There are two reasons for explicitly
casting values to
bfloat16:
Storing values in
bfloat16format saves on-chip memory, enabling Cloud TPUs to train larger models or use larger batch sizes.
Some operations are memory-bandwidth-bound, which means the amount of time it takes to load data from memory can slow down the overall time spent performing the computation. Storing operands and outputs of those ops in
bfloat16format reduces the amount of data that must be transferred, improving overall speed.
To get started, we recommend getting some hands-on experience with one of the
[
that have been optimized for Cloud TPUs. After that,
our performance guide, bfloat16-enabled reference models](https://github.com/tensorflow/tpu/tree/master/models/official)",The bfloat16 numerical format | Cloud TPU | Google Cloud,
id,url,body,title,description
97,https://cloud.google.com/tpu/docs/reference,"All APIs and reference documents
Stay organized with collections
Save and categorize content based on your preferences.
Frameworks (TensorFlow, PyTorch, JAX)
Except as otherwise noted, the content of this page is licensed under the
[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.
[{
""type"": ""thumb-down"",
""id"": ""hardToUnderstand"",
""label"":""Hard to understand""
},{
""type"": ""thumb-down"",
""id"": ""incorrectInformationOrSampleCode"",
""label"":""Incorrect information or sample code""
},{
""type"": ""thumb-down"",
""id"": ""missingTheInformationSamplesINeed"",
""label"":""Missing the information/samples I need""
},{
""type"": ""thumb-down"",
""id"": ""otherDown"",
""label"":""Other""
}]
[{
""type"": ""thumb-up"",
""id"": ""easyToUnderstand"",
""label"":""Easy to understand""
},{
""type"": ""thumb-up"",
""id"": ""solvedMyProblem"",
""label"":""Solved my problem""
},{
""type"": ""thumb-up"",
""id"": ""otherUp"",
""label"":""Other""
}]",All APIs and reference documents | Cloud TPU | Google Cloud,
id,url,body,title,description
41,https://cloud.google.com/tpu/docs/run-calculation-pytorch,"Run a calculation on a Cloud TPU VM using PyTorch
This quickstart shows you how to create a Cloud TPU, install PyTorch and run
a simple calculation on a Cloud TPU. For a more in depth tutorial showing you
how to train a model on a Cloud TPU see one of the
[Cloud TPU PyTorch Tutorials](/tpu/docs/tutorials/pytorch-pod).
Before you begin
Before you follow this quickstart, you must create a Google Cloud Platform
account, install the Google Cloud CLI. and configure the
gcloud command.
For more information, see
[Set up an account and a Cloud TPU project](/tpu/docs/setup-gcp-account).
Create a Cloud TPU with
gcloud
To create a TPU VM in the default user project, network and compute/zone run:
$ gcloud compute tpus tpu-vm create tpu-name \
--zone=us-central1-b \
--accelerator-type=v3-8 \
--version=tpu-ubuntu2204-base
Command flag descriptions
While creating your TPU, you can pass the additional
--network and
--subnetwork flags if
you want to specify the default network and subnetwork.
If you do not want to use the default network, you must pass the
--network flag. The
--subnetwork flag is optional and can be used to
specify a default subnetwork for whatever network you are using (default or
user-specified). See the
gcloud
[
API reference page](/sdk/gcloud/reference/compute/tpus/tpu-vm/create) for details on these flags.
Connect to your Cloud TPU VM
$ gcloud compute tpus tpu-vm ssh tpu-name --zone=us-central1-b
Install PyTorch/XLA on your TPU VM
(vm)$ pip install torch~=2.2.0 torch_xla[tpu]~=2.2.0 torchvision -f https://storage.googleapis.com/libtpu-releases/index.html
Set TPU runtime configuration
Ensure that the PyTorch/XLA runtime uses the TPU.
(vm) $ export PJRT_DEVICE=TPU
Perform a simple calculation:
Create a file named
tpu-test.pyin the current directory and copy and paste the following script into it.
import torch import torch_xla.core.xla_model as xm dev = xm.xla_device() t1 = torch.randn(3,3,device=dev) t2 = torch.randn(3,3,device=dev) print(t1 + t2)
Run the script:
(vm)$ python3 tpu-test.py
Output from the script shows the result of the computation:
tensor([[-0.2121, 1.5589, -0.6951], [-0.7886, -0.2022, 0.9242], [ 0.8555, -1.8698, 1.4333]], device='xla:1')
Clean up
To avoid incurring charges to your Google Cloud account for the resources used on this page, follow these steps.
Disconnect from the Compute Engine instance, if you have not already done so:
(vm)$ exit
Your prompt should now be
username@projectname, showing you are in the Cloud Shell.
Delete your Cloud TPU.
$ gcloud compute tpus tpu-vm delete tpu-name \ --zone=us-central1-b
The output of this command should confirm that your TPU has been deleted.
What's next
Read more about Cloud TPU VMs:",Run a calculation on a Cloud TPU VM using PyTorch | Google Cloud,"Learn how to create a Cloud TPU, install PyTorch and run a simple calculation on a Cloud TPU."
id,url,body,title,description
22,https://cloud.google.com/tpu/docs/supported-tpu-configurations,"TPU configurations
TPU v5e configurations
Cloud TPU v5e is a combined training and inference (serving) product. To
differentiate between a training and an inference environment, use the
AcceleratorType or
AcceleratorConfig flags with the TPU API or the
--machine-type flag
[when creating a GKE node
pool](/kubernetes-engine/docs/how-to/tpus#create-node-pool).
Training jobs are optimized for throughput and availability while serving jobs are optimized for latency. So, a training job on TPUs provisioned for serving could have lower availability and similarly, a serving job executed on TPUs provisioned for training could have higher latency.
You use
AcceleratorType to specify the number of TensorCores you want to use.
You specify the
AcceleratorType when creating a TPU using the
gcloud CLI or the
[Google Cloud console](https://console.cloud.google.com/). The value you
specify for
AcceleratorType is a string with the format:
v$VERSION_NUMBER-$CHIP_COUNT.
You can also use
AcceleratorConfig to specify the number of TensorCores you
want to use. However, because there are no custom 2D topology variants for TPU
v5e, there is no difference between using
AcceleratorConfig and
AcceleratorType.
To configure a TPU v5e using
AcceleratorConfig, use the
--version and the
--topology flags. Set
--version to the TPU version you want to use and
--topology to the physical arrangement of the TPU chips in the slice. The
value you specify for
AcceleratorConfig is a string with the format
AxB,
where
A and
B are the chip counts in each direction.
The following 2D slice shapes are supported for v5e:
|Topology
|Number of TPU chips
|Number of Hosts
|1x1
|1
|1/8
|2x2
|4
|1/2
|2x4
|8
|1
|4x4
|16
|2
|4x8
|32
|4
|8x8
|64
|8
|8x16
|128
|16
|16x16
|256
|32
Each TPU VM in a v5e TPU slice contains 1, 4 or 8 chips. In 4-chip and smaller slices, all TPU chips share the same Non Uniform Memory Access (NUMA) node.
For 8-chip v5e TPU VMs, CPU-TPU communication will be more efficient within NUMA
partitions. For example, in the following figure,
CPU0-Chip0 communication will
be faster than
CPU0-Chip4 communication.
Cloud TPU v5e types for serving
Single-host serving is supported for up to 8 v5e chips. The following configurations are supported: 1x1, 2x2 and 2x4 slices. Each slice has 1, 4 and 8 chips respectively.
To provision TPUs for a serving job, use one of the following accelerator types in your CLI or API TPU creation request:
|AcceleratorType (TPU API)
|Machine type (GKE API)
|
v5litepod-1
|
ct5lp-hightpu-1t
|
v5litepod-4
|
ct5lp-hightpu-4t
|
v5litepod-8
|
ct5lp-hightpu-8t
Serving on more than 8 v5e chips, also called multi-host serving, is supported
using
[Sax](https://github.com/google/saxml). For more information, see
[Large Language Model Serving](/tpu/docs/v5e-inference#large_language_model_serving).
Cloud TPU v5e types for training
Training is supported for up to 256 chips.
To provision TPUs for a v5e training job, use one of the following accelerator types in your CLI or API TPU creation request:
|AcceleratorType (TPU API)
|Machine type (GKE API)
|Topology
|
v5litepod-16
|
ct5lp-hightpu-4t
|4x4
|
v5litepod-32
|
ct5lp-hightpu-4t
|4x8
|
v5litepod-64
|
ct5lp-hightpu-4t
|8x8
|
v5litepod-128
|
ct5lp-hightpu-4t
|8x16
|
v5litepod-256
|
ct5lp-hightpu-4t
|16x16
v5e TPU VM type comparison:
|VM Type
|n2d-48-24-v5lite-tpu
|n2d-192-112-v5lite-tpu
|n2d-384-224-v5lite-tpu
|# of v5e chips
|1
|4
|8
|# of vCPUs
|24
|112
|224
|RAM (GB)
|48
|192
|384
|# of NUMA Nodes
|1
|1
|2
|Applies to
|v5litepod-1
|v5litepod-4
|v5litepod-8
|Disruption
|High
|Medium
|Low
To make space for workloads that require more chips, schedulers may preempt VMs with fewer chips. So 8-chip VMs are likely to preempt 1 and 4-chip VMs.
TPU v4 configurations
A TPU v4 Pod is composed of 4096 chips interconnected with reconfigurable
high-speed links. TPU v4's flexible networking lets you connect the chips in a
same-sized Pod slice in multiple ways. When you create a TPU Pod slice, you
specify the TPU version and the number of TPU resources you require. When you
create a TPU v4 Pod slice, you can specify its type and size in one of two ways:
AcceleratorType and
AccleratorConfig.
Using
AcceleratorType
Use AcceleratorType when you are not specifying a topology. To configure v4 TPUs
using
AcceleratorType, use the
--accelerator-type flag when creating your
TPU Pod slice. Set
--accelerator-type to a string that contains the TPU
version and the number of TensorCores you want to use. For example, to create a
v4 Pod slice with 32 TensorCores, you would use
--accelerator-type=v4-32.
The following command creates a v4 TPU Pod slice with 512 TensorCores using
the
--accelerator-type flag:
$ gcloud compute tpus tpu-vm create tpu-name
--zone=zone
--accelerator-type=v4-512
--version=tpu-vm-tf-2.15.0-pod-pjrt
The number after the TPU version (
v4) specifies the number of TensorCores.
There are two TensorCores in a v4 TPU, so the number of TPU chips
would be 512/2 = 256.
Using
AcceleratorConfig
Use
AcceleratorConfig when you want to customize the
[physical topology](/tpu/docs/types-topologies#v4-variants)
of your TPU slice. This is generally required for performance tuning with Pod
slices greater than 256 chips.
To configure v4 TPUs using
AcceleratorConfig, use the
--version and the
--topology flags. Set
--version to the TPU version you want to use and
--topology to the physical arrangement of the TPU chips in the Pod slice.
You specify a TPU topology using a 3-tuple, AxBxC where A<=B<=C and A, B, C are
either all <= 4 or are all integer multiples of 4. The values A, B, and C are
the chip counts in each of the three dimensions. For example, to create a v4 Pod
slice with 16 chips, you would set
--version=v4 and
--topology=2x2x4.
The following command creates a v4 TPU Pod slice with 128 TPU chips arranged in a 4x4x8 array:
$ gcloud compute tpus tpu-vm create tpu-name
--zone=zone
--type=v4
--topology=4x4x8
--version=tpu-vm-tf-2.15.0-pod-pjrt
Topologies where 2A=B=C or 2A=2B=C also have
[topology variants](#v4-variants)
optimized for all-to-all communication, for example, 448, 8816, and 121224.
These are known as twisted tori topologies.
The following illustrations show some common TPU v4 topologies.
Larger Pod slices can be built from one or more 4x4x4 ""cubes"" of chips.
Twisted Tori topologies
Some v4 3D torus slice shapes have the option to use what is known as a
twisted torus topology. For example two v4 cubes can be arranged as a 4x4x8
slice or 4x4x8_twisted. Twisted topologies offer significantly higher bisection
bandwidth. Increased bisection bandwidth is useful for workloads that use global
communication patterns. Twisted topologies can improve performance for most
models, with large
[TPU embedding workloads](https://www.tensorflow.org/recommenders/api_docs/python/tfrs/layers/embedding/TPUEmbedding)
benefiting the most.
For workloads that use data parallelism as the only parallelism strategy, twisted
topologies might perform slightly better. For LLMs, performance using a twisted
topology can vary depending on the type of parallelism (DP, MP, etc.). Best
practice is to train your LLM with and without a twisted topology to determine
which provides the best performance for your model. Some experiments on the
[FSDP MaxText model](https://github.com/google/maxtext#getting-started) have
seen 1-2 MFU improvements using a twisted topology.
The primary benefit of twisted topologies is that it transforms an asymmetric torus topology (for example, 448) into a closely related symmetric topology. The symmetric topology has many benefits:
- Improved load balancing
- Higher bisection bandwidth
- Shorter packet routes
These benefits ultimately translate into improved performance for many global communication patterns.
The TPU software supports twisted tori on slices where the size of each dimension is either equal to or twice the size of the smallest dimension. For example, 4x4x8, 488, or 12x12x24.
As an example, consider this 42 torus topology with TPUs labeled with their (X,Y) coordinates in the slice:
The edges in this topology graph are shown as undirected edges for clarity. In practice, each edge is a bidirectional connection between TPUs. We refer to the edges between one side of this grid and the opposite side as wrap-around edges, as noted in the diagram.
By twisting this topology, we end up a completely symmetric 42 twisted torus topology:
All that has changed between this diagram and the previous one is the Y wrap-around edges. Instead of connecting to another TPU with the same X coordinate, they have been shifted to connect to the TPU with coordinate X+2 mod 4.
The same idea generalizes to different dimension sizes and different numbers of dimensions. The resulting network is symmetric, as long as each dimension is equal to or twice the size of the smallest dimension.
See
[using AcceleratorConfig](#using-accelerator-config) for details about how
to specify a twisted tori configuration when creating a Cloud TPU.
The following table shows the supported twisted topologies and a theoretical increase in bisection bandwidth with them versus untwisted topologies.
|Twisted Topology
|Theoretical increase in bisection
bandwidth versus a non-twisted torus
|448_twisted
|~70%
|8x8x16_twisted
|121224_twisted
|488_twisted
|~40%
|81616_twisted
TPU v4 Topology variants
Some topologies containing the same number of chips can be arranged in different ways. For example, a TPU Pod slice with 512 chips (1024 TensorCores) can be configured using the following topologies: 4x4x32, 4x8x16, or 8x8x8. A TPU Pod slice with 2048 chips (4096 TensorCores) offers even more topology options: 4x4x128, 4x8x64, 4x16x32, and 8x16x16. A TPU Pod slice with 2048 chips (4096 TensorCores) offers even more topology options: 4x4x128, 4x8x64, 4x16x32, and 8x16x16.
The default topology associated with a given chip count is the one that's most
similar to a cube (see
[v4 Topology](/tpu/docs/system-architecture-tpu-vm#twisted-tori)).
This shape is likely the best choice for data-parallel ML training. Other
topologies can be useful for workloads with multiple kinds of parallelism (for
example, model and data parallelism, or spatial partitioning of a simulation).
These workloads perform best if the topology is matched to the parallelism used.
For example, placing 4-way model parallelism on the X dimension and 256-way data
parallelism on the Y and Z dimensions matches a 4x16x16 topology.
Models with multiple dimensions of parallelism perform best with their parallelism dimensions mapped to TPU topology dimensions. These are usually data+model parallel Large Language Models (LLMs). For example, for a TPU v4 Pod slice with topology 8x16x16, the TPU topology dimensions are 8, 16 and 16. It is more performant to use 8-way or 16-way model parallelism (mapped to one of the physical TPU topology dimensions). A 4-way model parallelism would be sub-optimal with this topology, since it's not aligned with any of the TPU topology dimensions, but it would be optimal with a 4x16x32 topology on the same number of chips.
TPU v4 configurations consist of two groups, those with topologies smaller than 64 chips (small topologies), and those with topologies greater than 64 chips (large topologies).
Small v4 topologies
Cloud TPU supports the following TPU v4 Pod slices smaller than 64 chips, a 4x4x4 cube. You can create these small v4 topologies using either their TensorCore-based name (for example, v4-32), or their topology (for example, 2x2x4):
|Name (based on TensorCore count)
|Number of chips
|Topology
|v4-8
|4
|2x2x1
|v4-16
|8
|2x2x2
|v4-32
|16
|2x2x4
|v4-64
|32
|2x4x4
Large v4 topologies
TPU v4 Pod slices are available in increments of 64 chips, with shapes that are
multiples of 4 on all three dimensions. The dimensions must also be in
increasing order. Several examples are shown in the following table. A few of
these topologies are ""custom"" topologies that can only be created using the
--type and
--topology flags because there is more than one way to arrange
the chips.
The following command creates a v4 TPU Pod slice with 512 TPU chips arranged in a 8x8x8 array:
$ gcloud compute tpus tpu-vm create tpu-name
--zone=zone
--type=v4
--topology=8x8x8
--version=tpu-vm-tf-2.15.0-pod-pjrt
You can create a v4 TPU Pod slice with the same number of TensorCores using
--accelerator-type:
$ gcloud compute tpus tpu-vm create tpu-name
--zone=zone
--accelerator-type=v4-1024
--version=tpu-vm-tf-2.15.0-pod-pjrt
|Name (based on TensorCore count)
|Number of chips
|Topology
|v4-128
|64
|4x4x4
|v4-256
|128
|4x4x8
|v4-512
|256
|4x8x8
|N/A - must use the
--type and
--topology flags
|256
|4x4x16
|v4-1024
|512
|8x8x8
|v4-1536
|768
|8x8x12
|v4-2048
|1024
|8x8x16
|N/A - must use
--type and
--topology flags
|1024
|4x16x16
|v4-4096
|2048
|8x16x16
|
|
|
TPU v3 configurations
A TPU v3 Pod is composed of 1024 chips interconnected with high-speed links. To
create a TPU v3 device or Pod slice, use the
--accelerator-type flag for the
gcloud compute tpus tpu-vm command. You specify the accelerator type by specifying the
TPU version and the number of TPU cores. For a single v3 TPU, use
--accelerator-type=v3-8. For a v3 Pod slice with 128 TensorCores, use
--accelerator-type=v3-128.
The following command shows how to create a v3 TPU Pod slice with 128 TensorCores:
$ gcloud compute tpus tpu-vm create tpu-name
--zone=zone
--accelerator-type=v3-128
--version=tpu-vm-tf-2.15.0-pjrt
The following table lists the supported v3 TPU types:
|TPU version
|Support ends
|v3-8
|(End date not yet set)
|v3-32
|(End date not yet set)
|v3-128
|(End date not yet set)
|v3-256
|(End date not yet set)
|v3-512
|(End date not yet set)
|v3-1024
|(End date not yet set)
|v3-2048
|(End date not yet set)
For more information about managing TPUs, see
[Manage TPUs](/tpu/docs/managing-tpus-tpu-vm).
For more information about the different versions of Cloud TPU, see
[System architecture](/tpu/docs/system-architecture).
TPU v2 configurations
A TPU v2 Pod is composed of 512 chips interconnected with reconfigurable
high-speed links. To create a TPU v2 Pod slice, use the
--accelerator-type flag
for the gcloud compute tpus tpu-vm command. You specify the accelerator type by
specifying the TPU version and the number of TPU cores. For a single v2 TPU, use
--accelerator-type=v2-8. For a v2 Pod slice with 128 TensorCores, use
--accelerator-type=v2-128.
The following command shows how to create a v2 TPU Pod slice with 128 TensorCores:
$ gcloud compute tpus tpu-vm create tpu-name
--zone=zone
--accelerator-type=v2-128
--version=tpu-vm-tf-2.15.0-pjrt
For more information about managing TPUs, see
[Manage TPUs](/tpu/docs/managing-tpus-tpu-vm).
For more information about the different versions of Cloud TPU, see
[System architecture](/tpu/docs/system-architecture).
The following table lists the supported v2 TPU types
|TPU version
|Support ends
|v2-8
|(End date not yet set)
|v2-32
|(End date not yet set)
|v2-128
|(End date not yet set)
|v2-256
|(End date not yet set)
|v2-512
|(End date not yet set)
TPU type compatibility
You can change the TPU type to another TPU type that has the same number of
TensorCores or chips (for example,
v3-128 and
v4-128)
and run your training script without code changes. However, if you change to a
TPU type with a larger or smaller number TensorCores, you will need to perform
significant tuning and optimization. For more information,
see
[Training on TPU Pods](/tpu/docs/training-on-tpu-pods).
TPU VM software versions
This section describes the TPU software versions you should use for a TPU with
the
[TPU VM](/tpu/docs/system-architecture-tpu-vm#tpu-vm-arch) architecture. For the [TPU
Node](/tpu/docs/system-architecture-tpu-vm#tpu-node-arch) architecture,
see [TPU Node software versions](#tpu-node-versions).
TPU software versions are available for TensorFlow, PyTorch, and JAX frameworks.
TensorFlow
For TensorFlow version 2.15.0, use the TPU software
version that matches the version of TensorFlow
with which your model was written. You must also specify either
the stream executor (SE) runtime or the PJRT runtime. For example, if you are
using TensorFlow 2.15.0 with the PJRT runtime, use
the
tpu-vm-tf-2.15.0-pjrt TPU software version.
PJRT features automatic device memory defragmentation and simplifies the
integration of hardware with frameworks. For more information about PJRT, see
[PJRT: Simplifying ML Hardware and Framework Integration](https://opensource.googleblog.com/2023/05/pjrt-simplifying-ml-hardware-and-framework-integration.html)
on the Google Open Source Blog.
We are working to migrate all features of TPU v2, v3, and v4 to the PJRT runtime. The following table describes which features are currently supported on PJRT or steam executor.
|Accelerator
|Feature
|Supported on PJRT
|Supported on stream executor
|TPU v2-v4
|Dense compute (no TPU embedding API)
|Yes
|Yes
|TPU v2-v4
|Dense compute API + TPU embedding API
|No
|Yes
|TPU v2-v4
|
tf.summary/
tf.print with soft device placement
|No
|Yes
|TPU v5e
|Dense compute (no TPU embedding API)
|Yes
|No
|TPU v5e
|TPU embedding API
|N/A - TPU v5e doesn't support TPU embedding API
|N/A
TensorFlow versions 2.14.0 and earlier only support stream executor.
Use the TPU software version that matches the version of TensorFlow
with which your model was written. For example, if you are using
TensorFlow 2.14.0, use the
tpu-vm-tf-2.14.0 TPU software version.
The current supported TensorFlow TPU VM software versions for TPUs are:
- tpu-vm-tf-2.15.0-pjrt
- tpu-vm-tf-2.15.0-se
- tpu-vm-tf-2.14.1
- tpu-vm-tf-2.14.0
- tpu-vm-tf-2.13.1
- tpu-vm-tf-2.13.0
- tpu-vm-tf-2.12.1
- tpu-vm-tf-2.12.0
- tpu-vm-tf-2.11.1
- tpu-vm-tf-2.11.0
- tpu-vm-tf-2.10.1
- tpu-vm-tf-2.10.0
- tpu-vm-tf-2.9.3
- tpu-vm-tf-2.9.1
- tpu-vm-tf-2.8.4
- tpu-vm-tf-2.8.3
- tpu-vm-tf-2.8.0
- tpu-vm-tf-2.7.4
- tpu-vm-tf-2.7.3
If you are using a Pod slice, append
-pod after the TensorFlow version
number. For example,
tpu-vm-tf-2.15.0-pod-pjrt.
For more information on TensorFlow patch versions, see
[Supported
TensorFlow patch versions](/tpu/docs/supported-versions).
TPU v4 with TensorFlow versions 2.10.0 and earlier
If you are training a model on TPU v4 with TensorFlow,
TensorFlow versions 2.10.0 and earlier use
v4-specific versions shown
in the following table. If the TensorFlow version you're using is not shown
in the table, follow the guidance in the
[TensorFlow](#tensorflow-vm) section.
|TensorFlow version
|TPU software version
|2.10.0
|tpu-vm-tf-2.10.0-v4, tpu-vm-tf-2.10.0-pod-v4
|2.9.3
|tpu-vm-tf-2.9.3-v4, tpu-vm-tf-2.9.3-pod-v4
|2.9.2
|tpu-vm-tf-2.9.2-v4, tpu-vm-tf-2.9.2-pod-v4
|2.9.1
|tpu-vm-tf-2.9.1-v4, tpu-vm-tf-2.9.1-pod-v4
Libtpu versions
TPU VMs are created with TensorFlow and the corresponding Libtpu
library preinstalled. If you are creating your own VM image, specify the
following TensorFlow TPU software versions and corresponding
libtpu
versions:
|TensorFlow version
|libtpu.so version
|
[1.9.0](https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/libtpu/1.9.0/libtpu.so) [2.14.1](https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/tensorflow/tf-2.14.1/tensorflow-2.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl) [1.8.1](https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/libtpu/1.8.1/libtpu.so) [2.14.0](https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/tensorflow/tf-2.14.0/tensorflow-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl) [1.8.0](https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/libtpu/1.8.0/libtpu.so) [2.13.1](https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/tensorflow/tf-2.13.1/tensorflow-2.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl) [1.7.1](https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/libtpu/1.7.1/libtpu.so) [2.13.0](https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/tensorflow/tf-2.13.0/tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl) [1.7.0](https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/libtpu/1.7.0/libtpu.so) [2.12.1](https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/tensorflow/tf-2.12.1/tensorflow-2.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl) [1.6.1](https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/libtpu/1.6.1/libtpu.so) [2.12.0](https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/tensorflow/tf-2.12.0/tensorflow-2.12.0-cp38-cp38-linux_x86_64.whl) [1.6.0](https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/libtpu/1.6.0/libtpu.so) [2.11.1](https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/tensorflow/tf-2.11.1/tensorflow-2.11.1-cp38-cp38-linux_x86_64.whl) [1.5.1](https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/libtpu/1.5.1/libtpu.so) [2.11.0](https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/tensorflow/tf-2.11.0/tensorflow-2.11.0-cp38-cp38-linux_x86_64.whl) [1.5.0](https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/libtpu/1.5.0/libtpu.so) [2.10.1](https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/tensorflow/tf-2.10.1/tensorflow-2.10.1-cp38-cp38-linux_x86_64.whl) [1.4.1](https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/libtpu/1.4.1/libtpu.so) [2.10.0](https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/tensorflow/tf-2.10.0/tensorflow-2.10.0-cp38-cp38-linux_x86_64.whl) [1.4.0](https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/libtpu/1.4.0/libtpu.so) [2.9.3](https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/tensorflow/tf-2.9.3/tensorflow-2.9.3-cp38-cp38-linux_x86_64.whl) [1.3.2](https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/libtpu/1.3.2/libtpu.so) [2.9.1](https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/tensorflow/tf-2.9.1/tensorflow-2.9.1-cp38-cp38-linux_x86_64.whl) [1.3.0](https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/libtpu/1.3.0/libtpu.so) [2.8.3](https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/tensorflow/tf-2.8.3/tensorflow-2.8.3-cp38-cp38-linux_x86_64.whl) [1.2.3](https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/libtpu/1.2.3/libtpu.so) [2.8.*](https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/tensorflow/tf-2.8.0/tensorflow-2.8.0-cp38-cp38-linux_x86_64.whl) [1.2.0](https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/libtpu/1.2.0/libtpu.so) [2.7.3](https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/tensorflow/tf-2-7-3/tensorflow-2.7.3-cp38-cp38-linux_x86_64.whl) [1.1.2](https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/libtpu/1.1.2/libtpu.so)
PyTorch
Use the TPU software version that matches the version of PyTorch with which
your model was written. For example, if you are using PyTorch 1.13 and TPU v2 or
v3, use the
tpu-vm-pt-1.13 TPU software version. If you are using TPU v4 use
the
tpu-vm-v4-pt-1.13 TPU software version. The same TPU software version
is used for TPU Pods (for example,
v2-32,
v3-128,
v4-32). The current
supported TPU software versions are:
TPU v2/v3:
- tpu-vm-pt-2.0 (pytorch-2.0)
- tpu-vm-pt-1.13 (pytorch-1.13)
- tpu-vm-pt-1.12 (pytorch-1.12)
- tpu-vm-pt-1.11 (pytorch-1.11)
- tpu-vm-pt-1.10 (pytorch-1.10)
- v2-alpha (pytorch-1.8.1)
TPU v4:
- tpu-vm-v4-pt-2.0 (pytorch-2.0)
- tpu-vm-v4-pt-1.13 (pytorch-1.13)
When you create a TPU VM, the latest version of PyTorch is preinstalled on the TPU VM. The correct version of libtpu.so is automatically installed when you install PyTorch.
To change the current PyTorch software version, see
[Changing PyTorch version](/tpu/docs/pytorch-xla-ug-tpu-vm#changing_pytorch_version).
JAX
You must manually install JAX on your TPU VM, because there is no JAX-specific TPU software version. For all TPU versions, use tpu-ubuntu2204-base. The correct version of libtpu.so is automatically installed when you install JAX.
TPU Node software versions
This section describes the TPU software versions you should use for a TPU with
the
[TPU Node](/tpu/docs/system-architecture-tpu-vm#tpu-node-arch) architecture. For the [TPU
VM](/tpu/docs/system-architecture-tpu-vm#tpu-vm-arch) architecture, see [TPU VM software
versions](#tpu-vm-versions).
TPU software versions are available for TensorFlow, PyTorch, and JAX frameworks.
TensorFlow
Use the TPU software version that matches the version of TensorFlow
with which your model was written. For example, if you are using
TensorFlow 2.12.0, use the
2.12.0
TPU software version. The TensorFlow specific TPU software versions
are:
- 2.12.1
- 2.12.0
- 2.11.1
- 2.11.0
- 2.10.1
- 2.10.0
- 2.9.3
- 2.9.1
- 2.8.4
- 2.8.2
- 2.7.3
For more information on TensorFlow patch versions, see
[Supported
TensorFlow patch versions](/tpu/docs/supported-patches).
When you create a TPU Node, the latest version of TensorFlow is preinstalled on the TPU Node.
PyTorch
Use the TPU software version that matches the version of PyTorch with which your
model was written. For example, if you are using PyTorch 1.9, use the
pytorch-1.9 software version.
The PyTorch specific TPU software versions are:
- pytorch-2.0
- pytorch-1.13
- pytorch-1.12
- pytorch-1.11
- pytorch-1.10
- pytorch-1.9
- pytorch-1.8
- pytorch-1.7
pytorch-1.6
pytorch-nightly
When you create a TPU Node, the latest version of PyTorch is preinstalled on the TPU Node.
JAX
You must manually install JAX on your TPU VM, so there is no pre-installed JAX-specific TPU software version. You can use any of the software versions listed for TensorFlow.
What's next
- Learn more about TPU architecture in the
[System Architecture](/tpu/docs/system-architecture-tpu-vm)page.
- See
[When to use TPUs](/tpu/docs/tpus#when_to_use_tpus)to learn about the types of models that are well suited to Cloud TPU.",TPU configurations | Google Cloud,
id,url,body,title,description
81,https://cloud.google.com/tpu/docs/tutorials/ncf-2.x,"Overview
This is an implementation of the Neural Collaborative Filtering (NCF)
framework using a Neural Matrix Factorization (NeuMF) model as described in
the
[Neural Collaborative Filtering paper](https://arxiv.org/abs/1708.05031).
The current implementation is based
on the code from the authors' NCF code and the Stanford implementation in
the MLPerf Repo.
NCF is a general framework for collaborative filtering of recommendations in which a neural network architecture is used to model user-item interactions. Unlike traditional models, NCF does not resort to Matrix Factorization (MF) with an inner product on latent features of users and items. It replaces the inner product with a multi-layer perceptron that can learn an arbitrary function from data.
Two implementations of NCF are Generalized Matrix Factorization (GMF) and
Multi-Layer Perceptron (MLP). GMF applies a linear kernel to model the latent
feature interactions, and MLP uses a nonlinear kernel to learn the
interaction function from data. NeuMF is a fused model of GMF and MLP to
better model complex user-item interactions, and unifies the strengths
of linearity of MF and non-linearality of MLP for modeling the user-item latent
structures. NeuMF allows GMF and MLP to learn separate embeddings, and
combines the two models by concatenating their last hidden layer.
[neumf_model.py](https://github.com/tensorflow/models/blob/master/official/recommendation/neumf_model.py)
defines the architecture details.
The instructions below assume you are already familiar with training a model on
Cloud TPU. If you are new to Cloud TPU,
refer to the
[Quickstart](/tpu/docs/quickstart) for a basic introduction.
Dataset
The MovieLens datasets are used for model training and evaluation. Specifically, we use two datasets: ml-1m (short for MovieLens 1 million) and ml-20m (short for MovieLens 20 million).
ml-1m
ml-1m dataset contains 1,000,209 anonymous ratings of approximately 3,706 movies made by 6,040 users who joined MovieLens in 2000. All ratings are contained in the file ""ratings.dat"" without a header row, and are in the following format:
UserID::MovieID::Rating::Timestamp
- UserIDs range between 1 and 6040.
- MovieIDs range between 1 and 3952.
- Ratings are made on a 5-star scale (whole-star ratings only).
ml-20m
ml-20m dataset contains 20,000,263 ratings of 26,744 movies by 138493 users. All ratings are contained in the file ""ratings.csv"". Each line of this file after the header row represents a single user's rating of a movie, and has the following format:
userId,movieId,rating,timestamp
The lines within this file are ordered first by userId, then, within user, by movieId. Ratings are made on a 5-star scale, with half-star increments (0.5 stars - 5.0 stars). In both datasets, the timestamp is represented in seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970. Each user has at least 20 ratings.
Objectives
- Create a Cloud Storage bucket to hold your dataset and model output
- Prepare the MovieLens dataset
- Set up a Compute Engine VM and Cloud TPU node for training and evaluation
- Run training and evaluation
Costs
In this document, you use the following billable components of Google Cloud:
- Compute Engine
- Cloud TPU
- Cloud Storage
To generate a cost estimate based on your projected usage,
use the
[pricing calculator](/products/calculator).
[free trial](/free-trial).
Before you begin
Before starting this tutorial, check that your Google Cloud project is correctly set up.
-
Sign in to your Google Cloud account. If you're new to
Google Cloud,
[create an account](https://console.cloud.google.com/freetrial)to evaluate how our products perform in real-world scenarios. New customers also get $300 in free credits to run, test, and deploy workloads.
-
In the Google Cloud console, on the project selector page, select or
[create a Google Cloud project](/resource-manager/docs/creating-managing-projects).
-
[Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console).
-
In the Google Cloud console, on the project selector page, select or
[create a Google Cloud project](/resource-manager/docs/creating-managing-projects).
-
[Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console).
This walkthrough uses billable components of Google Cloud. Check the
[Cloud TPU pricing page](/tpu/docs/pricing) to
estimate your costs. Be sure to
[clean up](#clean_up)
resources you create when you've finished with them to avoid unnecessary
charges.
Set up your resources
This section provides information on setting up Cloud Storage, VM, and Cloud TPU resources for this tutorial.
Open a Cloud Shell window.
Create an environment variable for your project's ID.
export PROJECT_ID=project-id
Configure Google Cloud CLI to use the project where you want to create the Cloud TPU.
gcloud config set project ${PROJECT_ID}
The first time you run this command in a new Cloud Shell VM, an
Authorize Cloud Shellpage is displayed. Click
Authorizeat the bottom of the page to allow
gcloudto make API calls with your credentials.
Create a Service Account for the Cloud TPU project.
gcloud beta services identity create --service tpu.googleapis.com --project $PROJECT_ID
The command returns a Cloud TPU Service Account with following format:
service-PROJECT_NUMBER@cloud-tpu.iam.gserviceaccount.com
Create a Cloud Storage bucket using the following command:
gsutil mb -p ${PROJECT_ID} -c standard -l europe-west4 gs://bucket-name
This Cloud Storage bucket stores the data you use to train your model and the training results. The
gcloudcommand used in this tutorial to set up the TPU also sets up default permissions for the Cloud TPU Service Account you set up in the previous step. If you want finer-grain permissions, review the
[access level permissions](/tpu/docs/storage-buckets).
The bucket location must be in the same region as your virtual machine (VM) and your TPU node. VMs and TPU nodes are located in
[specific zones](/tpu/docs/types-zones#types), which are subdivisions within a region.
Launch a Compute Engine VM and Cloud TPU using the
gcloudcommand. The command you use depends on whether you are using TPU VMs or TPU nodes. For more information on the two VM architecture, see
[System Architecture](/tpu/docs/system-architecture-tpu-vm).
TPU VM
$ gcloud compute tpus tpu-vm create ncf-tutorial \ --zone=europe-west4-a \ --accelerator-type=v3-8 \ --version=tpu-vm-tf-2.15.0-pjrt
Command flag descriptions
zone
- The
[zone](/tpu/docs/types-zones)where you plan to create your Cloud TPU.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
version
- The Cloud TPU
[software version](/tpu/docs/supported-tpu-versions#tpu_software_versions).
TPU Node
$ gcloud compute tpus execution-groups create \ --zone=europe-west4-a \ --name=ncf-tutorial \ --accelerator-type=v3-8 \ --machine-type=n1-standard-8 \ --disk-size=300 \ --tf-version=2.12.0
Command flag descriptions
zone
- The
[zone](/tpu/docs/types-zones)where you plan to create your Cloud TPU.
name
- The TPU name. If not specified, defaults to your username.
accelerator-type
- The
[type](/tpu/docs/supported-tpu-configurations)of the Cloud TPU to create.
machine-type
- The
[machine type](/compute/docs/machine-types)of the Compute Engine VM to create.
disk-size
- The root volume size of your Compute Engine VM (in GB).
tf-version
- The version of Tensorflow
gcloudinstalls on the VM.
For more information on the
gcloudcommand, see the
[gcloud Reference](/sdk/gcloud/reference).
-
If you are not automatically logged in to the Compute Engine instance, log in by running the following
sshcommand. When you are logged into the VM, your shell prompt changes from
username@projectnameto
username@vm-name:
TPU VM
gcloud compute tpus tpu-vm ssh ncf-tutorial --zone=europe-west4-a
TPU Node
gcloud compute ssh ncf-tutorial --zone=europe-west4-a
Prepare the data
Add an environment variable for your storage bucket. Replace bucket-name with your bucket name.
(vm)$ export STORAGE_BUCKET=gs://bucket-name
Add an environment variable for the data directory.
(vm)$ export DATA_DIR=${STORAGE_BUCKET}/ncf_data
Set up the model location and set the
PYTHONPATHenvironment variable.
TPU VM
(vm)$ git clone https://github.com/tensorflow/models.git (vm)$ pip3 install -r models/official/requirements.txt
(vm)$ export PYTHONPATH=""${PWD}/models:${PYTHONPATH}""
TPU Node
(vm)$ export PYTHONPATH=""${PYTHONPATH}:/usr/share/models"" (vm)$ pip3 install -r /usr/share/models/official/requirements.txt
Change to directory that stores the model processing files:
TPU VM
(vm)$ cd ~/models/official/recommendation
TPU Node
(vm)$ cd /usr/share/models/official/recommendation
Generate training and evaluation data for the ml-20m dataset in DATA_DIR:
(vm)$ python3 create_ncf_data.py \ --dataset ml-20m \ --num_train_epochs 4 \ --meta_data_file_path ${DATA_DIR}/metadata \ --eval_prebatch_size 160000 \ --data_dir ${DATA_DIR}
This script generates and preprocesses the dataset on your VM. Preprocessing converts the data into TFRecord format required by the model. The download and pre-processing takes approximately 25 minutes and generates output similar to the following:
I0804 23:03:02.370002 139664166737728 movielens.py:124] Successfully downloaded /tmp/tmpicajrlfc/ml-20m.zip 198702078 bytes I0804 23:04:42.665195 139664166737728 data_preprocessing.py:223] Beginning data preprocessing. I0804 23:04:59.084554 139664166737728 data_preprocessing.py:84] Generating user_map and item_map... I0804 23:05:20.934210 139664166737728 data_preprocessing.py:103] Sorting by user, timestamp... I0804 23:06:39.859857 139664166737728 data_preprocessing.py:194] Writing raw data cache. I0804 23:06:42.375952 139664166737728 data_preprocessing.py:262] Data preprocessing complete. Time: 119.7 sec. %lt;BisectionDataConstructor(Thread-1, initial daemon)> General: Num users: 138493 Num items: 26744 Training: Positive count: 19861770 Batch size: 99000 Batch count per epoch: 1004 Eval: Positive count: 138493 Batch size: 160000 Batch count per epoch: 866 I0804 23:07:14.137242 139664166737728 data_pipeline.py:887] Negative total vector built. Time: 31.8 seconds I0804 23:11:25.013135 139664166737728 data_pipeline.py:588] Epoch construction complete. Time: 250.9 seconds I0804 23:15:46.391308 139664166737728 data_pipeline.py:674] Eval construction complete. Time: 261.4 seconds I0804 23:19:54.345858 139664166737728 data_pipeline.py:588] Epoch construction complete. Time: 248.0 seconds I0804 23:24:09.182484 139664166737728 data_pipeline.py:588] Epoch construction complete. Time: 254.8 seconds I0804 23:28:26.224653 139664166737728 data_pipeline.py:588] Epoch construction complete. Time: 257.0 seconds
Set up and start training the Cloud TPU
Set the Cloud TPU name variable.
TPU VM
(vm)$ export TPU_NAME=local
TPU Node
(vm)$ export TPU_NAME=ncf-tutorial
Run the training and evaluation
The following script runs a sample training for 3 epochs,
Add an environment variable for the Model directory to save checkpoints and TensorBoard summaries:
(vm)$ export MODEL_DIR=${STORAGE_BUCKET}/ncf
- If you set
--version=tpu-vm-tf-2.15.0-pjrtwhen creating your TPU, set the following environment variables to enable the PJRT runtime:
(vm)$ export NEXT_PLUGGABLE_DEVICE_USE_C_API=true (vm)$ export TF_PLUGGABLE_DEVICE_LIBRARY_PATH=/lib/libtpu.so
Run the following command to train the NCF model:
(vm)$ python3 ncf_keras_main.py \ --model_dir=${MODEL_DIR} \ --data_dir=${DATA_DIR} \ --train_dataset_path=${DATA_DIR}/training_cycle_*/* \ --eval_dataset_path=${DATA_DIR}/eval_data/* \ --input_meta_data_path=${DATA_DIR}/metadata \ --learning_rate=3e-5 \ --train_epochs=3 \ --dataset=ml-20m \ --eval_batch_size=160000 \ --learning_rate=0.00382059 \ --beta1=0.783529 \ --beta2=0.909003 \ --epsilon=1.45439e-07 \ --dataset=ml-20m \ --num_factors=64 \ --hr_threshold=0.635 \ --keras_use_ctl=true \ --layers=256,256,128,64 \ --use_synthetic_data=false \ --distribution_strategy=tpu \ --download_if_missing=false
The training and evaluation takes about 2 minutes and generates final output similar to:
Result is {'loss': <tf.Tensor: shape=(), dtype=float32, numpy=0.10950611>, 'train_finish_time': 1618016422.1377568, 'avg_exp_per_second': 3062557.5070816963}
Clean up
To avoid incurring charges to your Google Cloud account for the resources used in this tutorial, either delete the project that contains the resources, or keep the project and delete the individual resources.
Disconnect from the Compute Engine instance, if you have not already done so:
(vm)$ exit
Your prompt should now be
username@projectname, showing you are in the Cloud Shell.
Delete your Cloud TPU and Compute Engine resources. The command you use to delete your resources depends upon whether you are using TPU VMs or TPU Nodes. For more information, see
[System Architecture](/tpu/docs/system-architecture-tpu-vm).
TPU VM
$ gcloud compute tpus tpu-vm delete ncf-tutorial \ --zone=europe-west4-a
TPU Node
$ gcloud compute tpus execution-groups delete ncf-tutorial \ --zone=europe-west4-a
Verify the resources have been deleted by running
gcloud compute tpus execution-groups list. The deletion might take several minutes. A response like the one below indicates your instances have been successfully deleted.
TPU VM
$ gcloud compute tpus tpu-vm list \ --zone=europe-west4-a
TPU Node
$ gcloud compute tpus execution-groups list --zone=europe-west4-a
Listed 0 items.
Run
gsutilas shown, replacing bucket-name with the name of the Cloud Storage bucket you created for this tutorial:
$ gsutil rm -r gs://bucket-name
What's next
The TensorFlow Cloud TPU tutorials generally train the model using a
sample dataset. The results of this training are not usable
for inference. To use a model for inference, you can train the data on a
publicly available dataset or your own data set. TensorFlow models
trained on Cloud TPUs generally require datasets to be in
[TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) format.
You can use the
[dataset conversion tool sample](https://cloud.google.com/tpu/docs/classification-data-conversion) to convert an image classification dataset into TFRecord format. If you are not using an image classification model, you will have to convert your dataset to [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) format yourself. For more information, see [TFRecord and tf.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord).
Hyperparameter tuning
To improve the model's performance with your dataset, you can tune
the model's hyperparameters. You can find information about hyperparameters
common to all TPU supported models on
[GitHub](https://github.com/tensorflow/tpu/tree/master/models/hyperparameters).
Information about model-specific hyperparameters can be found in the
[source code](https://github.com/tensorflow/tpu/tree/master/models/official)
for each model. For more information on hyperparameter tuning, see
[Overview of hyperparameter tuning](https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview),
[Using the Hyperparameter tuning service](https://cloud.google.com/ai-platform/training/docs/using-hyperparameter-tuning),
and [Tune hyperparameters](https://developers.google.com/machine-learning/guides/text-classification/step-5).
Inference
Once you have trained your model you can use it for inference
(also called prediction).
[AI Platform](https://cloud.google.com/ai-platform/docs/technical-overview)
is a cloud-based solution for developing,
[training](https://cloud.google.com/ai-platform/training/docs),
and [deploying](https://cloud.google.com/ai-platform/prediction/docs/deploying-models)
machine learning models. Once a model is deployed, you can use the
[AI Platform Prediction service](https://cloud.google.com/ai-platform/prediction/docs).
- Learn more about
[, including how to install it on a local machine.](https://github.com/tensorflow/tpu/tree/master/tools/ctpu)
ctpu
- Explore the
[TPU tools in TensorBoard](/tpu/docs/cloud-tpu-tools).",Training NCF on Cloud TPU (TF 2.x) | Google Cloud,
